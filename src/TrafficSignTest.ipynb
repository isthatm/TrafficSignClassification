{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVWiOsBJkLyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe695897-5e9b-465d-c254-9fdb30153c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.101-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.8-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.2.101-py3-none-any.whl (874 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m874.1/874.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.8-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.2.101 ultralytics-thop-2.0.8\n"
          ]
        }
      ],
      "source": [
        "#456!apt-get update\n",
        "#!apt-get install -y libgl1-mesa-glx libsm6 libxext6 libxrender-dev\n",
        "!pip install opencv-python\n",
        "!pip install torch\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import sys\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "kydzecyXpox7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b49c78-ae92-4ac9-8842-81dc7da68203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pool of square window of size=3, stride=2\n",
        "import torch\n",
        "m = torch.nn.MaxPool2d(2, stride=2)\n",
        "# pool of non-square window\n",
        "#m = torch.nn.MaxPool2d((3, 2), stride=(2, 1))\n",
        "input = torch.randn(10, 3, 32, 32)\n",
        "conv2d_1 = torch.nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "conv2d_2 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "\n",
        "conv2d_3 = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "conv2d_4 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "\n",
        "conv2d_5 = torch.nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "conv2d_6 = torch.nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "conv2d_7 = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "conv2d_8 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "\n",
        "conv2d_9 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "conv2d_10 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "\n",
        "block1 = [ conv2d_1, torch.nn.ReLU(inplace=True), conv2d_2, torch.nn.ReLU(inplace=True), m ]\n",
        "block2 = [ conv2d_3, torch.nn.ReLU(inplace=True), conv2d_4, torch.nn.ReLU(inplace=True), m ]\n",
        "block3 = [ conv2d_5, torch.nn.ReLU(inplace=True), conv2d_6, torch.nn.ReLU(inplace=True), m ]\n",
        "block4 = [ conv2d_7, torch.nn.ReLU(inplace=True), conv2d_8, torch.nn.ReLU(inplace=True), m ]\n",
        "block5 = [ conv2d_9, torch.nn.ReLU(inplace=True), conv2d_10, torch.nn.ReLU(inplace=True), m ]\n",
        "layer1 = torch.nn.Sequential(*block1)\n",
        "layer2 = torch.nn.Sequential(*block2)\n",
        "layer3 = torch.nn.Sequential(*block3)\n",
        "layer4 = torch.nn.Sequential(*block4)\n",
        "layer5 = torch.nn.Sequential(*block5)\n",
        "\n",
        "layers = [layer1, layer2, layer3, layer4, layer5]\n",
        "output = []\n",
        "tmpInput = input\n",
        "for layerIdx, layer in enumerate(layers):\n",
        "  output.append(layer(tmpInput))\n",
        "  tmpInput = output[layerIdx]\n",
        "\n",
        "output_m1 = torch.cat((m(output[1]), output[2]), 1)\n",
        "output_m2 = torch.cat((m(output_m1), output[3]), 1)\n",
        "output_m3 = torch.cat((m(output_m2), output[4]), 1)\n",
        "#ouput_pre_classification = output_m2.view(output_m2.size(0), -1)\n",
        "ouput_pre_classification = output_m3.view(output_m3.size(0), -1)\n",
        "\n",
        "#output1 = layer1(input)\n",
        "#output2 = layer2(output1)\n",
        "#output3 = layer3(output2)\n",
        "#output4 = layer4(output3)\n",
        "\n",
        "# output_m1 = torch.cat((m(output2), output3), 1)\n",
        "# output_m2 = torch.cat((m(output_m1), output4), 1)\n",
        "\n",
        "print(input.size())\n",
        "print(ouput_pre_classification.size())\n",
        "print(output[-1].size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rCk72dkPW3D",
        "outputId": "dcd19f8f-3980-4f50-a3af-fa3d57968fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 32, 32])\n",
            "torch.Size([10, 1408])\n",
            "torch.Size([10, 512, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Utils**"
      ],
      "metadata": {
        "id": "M5adTFD1oifs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_name_mapping = {\n",
        "    0: \"3-junction\",\n",
        "    1: \"One-way road\",\n",
        "    2: \"Side to obe followed\",\n",
        "    3: \"Cross road\",\n",
        "    4: \"Intersection with Uncontrolled Road\",\n",
        "    5: \"Dangerous turn\",\n",
        "    6: \"No Left turn\",\n",
        "    7: \"Bus stop\",\n",
        "    8: \"Roundabout\",\n",
        "    9: \"No parking and stopping\",\n",
        "    10: \"U-turn\",\n",
        "    11: \"Lane-allocation\",\n",
        "    12: \"No left turn for motorcycles\",\n",
        "    13: \"Slow Down\",\n",
        "    14: \"No Trucks Allowed\",\n",
        "    15: \"Narrow Road on the Right\",\n",
        "    16: \"No Passenger Cars and Trucks\",\n",
        "    17: \"Height Limit\",\n",
        "    18: \"No U-Turn\",\n",
        "    19: \"No U-Turn and No Right Turn\",\n",
        "    20: \"No Cars Allowed\",\n",
        "    21: \"Narrow Road on the Left\",\n",
        "    22: \"Uneven Road\",\n",
        "    23: \"No Two or Three-wheeled Vehicles\",\n",
        "    24: \"Customs Checkpoint\",\n",
        "    25: \"Motorcycles Only\",\n",
        "    26: \"Obstacle on the Road\",\n",
        "    27: \"Children Presence\",\n",
        "    28: \"Trucks and Containers\",\n",
        "    29: \"No Motorcycles Allowed\",\n",
        "    30: \"Trucks Only\",\n",
        "    31: \"Road with Surveillance Camera\",\n",
        "    32: \"No Right Turn\",\n",
        "    33: \"Series of Dangerous Turns\",\n",
        "    34: \"No Containers Allowed\",\n",
        "    35: \"No Left or Right Turn\",\n",
        "    36: \"No Straight and Right Turn\",\n",
        "    37: \"Intersection with T-Junction\",\n",
        "    38: \"Speed limit (50km/h)\",\n",
        "    39: \"Speed limit (60km/h)\",\n",
        "    40: \"Speed limit (80km/h)\",\n",
        "    41: \"Speed limit (40km/h)\",\n",
        "    42: \"Left Turn\",\n",
        "    43: \"Low Clearance\",\n",
        "    44: \"Other Danger\",\n",
        "    45: \"Go Straight\",\n",
        "    46: \"No Parking\",\n",
        "    47: \"No Left or U-turn\",\n",
        "    48: \"No U-Turn for Cars\",\n",
        "    49: \"Level Crossing with Barriers\"\n",
        "}\n",
        "\n",
        "def display_batch(images, labels):\n",
        "    num_images = len(images)\n",
        "    plt.figure(figsize=(5, 5 * num_images))  # Adjust the figure size to have more space for each image\n",
        "\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(num_images, 1, i + 1)\n",
        "        plt.imshow(images[i].permute(1, 2, 0))  # Convert from CxHxW to HxWxC for plt.imshow\n",
        "        plt.title(label_name_mapping[labels[0][i].item()])\n",
        "        plt.axis('off')  # Hide axes for better readability\n",
        "\n",
        "    plt.tight_layout()  # Adjust subplots to fit into the figure area.\n",
        "    plt.show()\n",
        "\n",
        "def pickle_data(file, writeColumns=None):\n",
        "    \"\"\"\n",
        "    Read/Write pickle training/testing data, models to avoid\n",
        "    loading data again (time consuming)\n",
        "\n",
        "    :param file: path to pickle file\n",
        "    :param writeColumns (torch.Tensor or np.ndarray): variables to be saved to pickle file\n",
        "\n",
        "    :returns :\n",
        "    If writeColumns = None -> tuple(torch.Tensor)\n",
        "    tuple()\n",
        "    \"\"\"\n",
        "    if writeColumns is None:\n",
        "        with open(file, mode=\"rb\") as f:\n",
        "            dataset = pickle.load(f)\n",
        "            return tuple( # Convert the pickled data into tensor if it is of any other types\n",
        "                map(lambda col: torch.tensor(dataset[col])\n",
        "                    if not type(dataset[col]) == torch.Tensor else dataset[col],\n",
        "                    ['images', 'labels'])\n",
        "                )\n",
        "                # lambda(col) where columns are the inputs\n",
        "    else:\n",
        "        with open(file, mode=\"wb\") as f:\n",
        "            dataset = pickle.dump({\"images\": writeColumns[0], \"labels\": writeColumns[1]}, f)\n",
        "            print(\"Data is saved in\", file)\n",
        "\n",
        "    def preprocess_batch(batch_tensor):\n",
        "      preprocessed_image = []\n",
        "      idx = 0\n",
        "      for img_tensor in batch_tensor:\n",
        "         # Convert tensor to numpy array\n",
        "        #print(\"{}: {}\".format(idx, img_tensor.size()))\n",
        "        img_np = img_tensor.permute(1, 2, 0).numpy()  # [C, H, W] -> [H, W, C]\n",
        "\n",
        "         # Convert from [0, 1] or [0, 255] to [0, 255] and ensure uint8 type\n",
        "        if img_np.max() <= 1.0:\n",
        "            img_np = (img_np * 255).astype(np.uint8)\n",
        "        else:\n",
        "            img_np = img_np.astype(np.uint8)\n",
        "\n",
        "        img_res = preprocess(img_np)\n",
        "        img_tensor = torch.from_numpy(img_res).unsqueeze(0)\n",
        "        preprocessed_image.append(img_tensor)\n",
        "        idx += 1\n",
        "\n",
        "      preprocessed_tensor = torch.stack(preprocessed_image)\n",
        "      return preprocessed_tensor\n",
        "\n",
        "def preprocess(image):\n",
        "    if image.shape[2] > 1:\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "      # Noise reduction\n",
        "      image = cv2.GaussianBlur(image, (3, 3), 1,5)\n",
        "      # Histogram equalization\n",
        "      image = cv2.equalizeHist(image)\n",
        "      # Image eroding\n",
        "      image = cv2.erode(image, (3, 3))\n",
        "      # Resize image\n",
        "      image = cv2.resize(image, (32, 32))\n",
        "    return image\n",
        "\n",
        "class CustomDataset(TensorDataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "# Germany\n",
        "# train_images, train_labels  = pickle_data(file = '/content/drive/MyDrive/TrafficSignData/German/gray32_train_dataset')\n",
        "# val_images, val_labels      = pickle_data(file = '/content/drive/MyDrive/TrafficSignData/German/gray32_val_dataset')\n",
        "\n",
        "# Vietnam\n",
        "vietnam_imgs, vietnam_labels = pickle_data(file = '/content/drive/MyDrive/TrafficSignData/Vietnam/gray_processed_dataset')\n",
        "portion    = 0.8\n",
        "train_size = int(portion * len(vietnam_imgs))\n",
        "val_size   = len(vietnam_imgs) - train_size\n",
        "train_dataset, val_dataset = random_split(list(zip(vietnam_imgs, vietnam_labels)), [train_size, val_size])\n",
        "print(len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-ZWONxLra2t",
        "outputId": "7e84e8e7-9963-41e6-b4d7-f84ea192bd1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Object detection**"
      ],
      "metadata": {
        "id": "O5gy5OMHePul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = '/content/drive/MyDrive/TrafficSignData/Vietnam/Object Detection'\n",
        "model = YOLO('yolov5s.yaml')\n",
        "results = model.train(data=os.path.join(ROOT_DIR, 'obj_detection.yaml'), epochs=30)"
      ],
      "metadata": {
        "id": "_VVEIo2DeYJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y locales\n",
        "!locale-gen en_US.UTF-8\n",
        "!update-locale LANG=en_US.UTF-8"
      ],
      "metadata": {
        "id": "yW1pXqOUti_6",
        "outputId": "2cc9a05c-67ee-437b-c1c1-ba6742ed49b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4216a52eb034>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'apt-get install -y locales'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'locale-gen en_US.UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'update-locale LANG=en_US.UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/runs /content/drive/MyDrive/TrafficSignData/Vietnam/Object Detection/Model_240824"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "0BfxI_pfYIHG",
        "outputId": "c8449f76-382f-43de-fc65-c82989779e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-54d26b1ec9d1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mv /content/runs /content/drive/MyDrive/TrafficSignData/Vietnam/Object Detection/Model_240824'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Network**\n"
      ],
      "metadata": {
        "id": "HI0dFA-Zwkiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile my_mainScript.py\n",
        "\n",
        "# Ensure that the input tensor size is [batch_size, channel, width, heigh]\n",
        "# Ensure that the labels are int64\n",
        "# Ensure that the ouput of the classfier matches the number of classes\n",
        "\n",
        "supportedArch = [\n",
        "    'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
        "    'vgg19_bn', 'vgg19',\n",
        "]\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    '''\n",
        "    VGG model\n",
        "    '''\n",
        "    def __init__(self, vgg_blocks):\n",
        "        super(VGG, self).__init__()\n",
        "        self.layers = nn.ModuleList(vgg_blocks)\n",
        "        self.maxPool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1408, 512), # nn.Linear(1408, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 50),\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "      output = []\n",
        "      tmpInput = x\n",
        "      for layerIdx, layer in enumerate(self.layers):\n",
        "        current_output = layer(tmpInput)\n",
        "        output.append(current_output)\n",
        "        tmpInput = current_output\n",
        "\n",
        "      output_m1 = torch.cat((self.maxPool (output[1]), output[2]), 1)\n",
        "      output_m2 = torch.cat((self.maxPool (output_m1), output[3]), 1)\n",
        "      output_m3 = torch.cat((self.maxPool (output_m2), output[4]), 1)\n",
        "\n",
        "      ouput_pre_classification = output_m3.view(output_m3.size(0), -1)\n",
        "      #ouput_pre_classification = output[-1].view(output[-1].size(0), -1)\n",
        "      #print(ouput_pre_classification.size())\n",
        "      final_ouput = self.classifier(ouput_pre_classification)\n",
        "      return final_ouput\n",
        "\n",
        "    def vgg11():\n",
        "        \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
        "        return VGG(make_layers(cfg['A']))\n",
        "\n",
        "\n",
        "    def vgg11_bn():\n",
        "        \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
        "        return VGG(make_layers(cfg['A'], batch_norm=True))\n",
        "\n",
        "\n",
        "    def vgg13():\n",
        "        \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
        "        return VGG(make_layers(cfg['B']))\n",
        "\n",
        "\n",
        "    def vgg13_bn():\n",
        "        \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
        "        return VGG(make_layers(cfg['B'], batch_norm=True))\n",
        "\n",
        "\n",
        "    def vgg16():\n",
        "        \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
        "        return VGG(make_layers(cfg['D']))\n",
        "\n",
        "\n",
        "    def vgg16_bn():\n",
        "        \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
        "        return VGG(make_layers(cfg['D'], batch_norm=True))\n",
        "\n",
        "\n",
        "    def vgg19():\n",
        "        \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
        "        return VGG(make_layers(cfg['E']))\n",
        "\n",
        "\n",
        "    def vgg19_bn():\n",
        "        \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
        "        return VGG(make_layers(cfg['E'], batch_norm=True))\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "    vgg_blocks = []\n",
        "    in_channels = 1\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            vgg_sequential = nn.Sequential(*layers)\n",
        "            vgg_blocks.append(vgg_sequential)\n",
        "            layers = [] # empty the current block\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return vgg_blocks\n",
        "\n",
        "cfg = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',\n",
        "          512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "model_names = sorted(name for name in supportedArch\n",
        "    if name.islower() and not name.startswith(\"__\")\n",
        "                     and name.startswith(\"vgg\")\n",
        "                    )\n",
        "\n",
        "best_prec1     = 0\n",
        "ARCH           = \"vgg16_bn\"\n",
        "WORKERS        = 4\n",
        "EPOCHS         = 100\n",
        "START_EPOCH    = 0\n",
        "BATCH_SIZE     = 32\n",
        "LEARNING_RATE  = 0.05\n",
        "MOMENTUM       = 0.9\n",
        "WEIGHT_DECAY   = 5E-4\n",
        "PRINT_FREQ     = 20\n",
        "RESUME         = \"/content/drive/MyDrive/TrafficSignData/Vietnam/24.08.18_Take1/checkpoint_99.tar\" # Path to model file\n",
        "EVALUATE       = True\n",
        "HALF_PRECISION = False\n",
        "CPU            = True\n",
        "SAVE_DIR       = \"save_temp\"\n",
        "\n",
        "def main():\n",
        "    global best_prec1, START_EPOCH\n",
        "    print (VGG.__dict__)\n",
        "\n",
        "    # Check the save_dir exists or not\n",
        "    if not os.path.exists(SAVE_DIR):\n",
        "        os.makedirs(SAVE_DIR)\n",
        "\n",
        "    # model = VGG.__dict__[args.arch]()\n",
        "    model_method = getattr(VGG, ARCH)\n",
        "    model = model_method()\n",
        "    #model.features = torch.nn.DataParallel(model.features)\n",
        "    if CPU:\n",
        "        model.cpu()\n",
        "    else:\n",
        "        model.cuda()\n",
        "\n",
        "    # optionally resume from a checkpoint\n",
        "    if RESUME:\n",
        "        if os.path.isfile(RESUME):\n",
        "            print(\"=> loading checkpoint '{}'\".format(RESUME))\n",
        "            if not CPU:\n",
        "              checkpoint = torch.load(RESUME)\n",
        "            else:\n",
        "              checkpoint = torch.load(RESUME, map_location=torch.device('cpu'))\n",
        "            START_EPOCH = checkpoint['epoch']\n",
        "            best_prec1 = checkpoint['best_prec1']\n",
        "            model.load_state_dict(checkpoint['state_dict'])\n",
        "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(EVALUATE, checkpoint['epoch']))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(RESUME))\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # define loss function (criterion) and pptimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if CPU:\n",
        "        criterion = criterion.cpu()\n",
        "    else:\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    if HALF_PRECISION:\n",
        "        model.half()\n",
        "        criterion.half()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), LEARNING_RATE,\n",
        "                                momentum=MOMENTUM,\n",
        "                                weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    if EVALUATE:\n",
        "        validate(val_loader, model, criterion)\n",
        "        return\n",
        "\n",
        "    for epoch in range(START_EPOCH, EPOCHS):\n",
        "        adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "        # train for one epoch\n",
        "        train(train_loader, model, criterion, optimizer, epoch)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        prec1 = validate(val_loader, model, criterion)\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = prec1 > best_prec1\n",
        "        best_prec1 = max(prec1, best_prec1)\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'best_prec1': best_prec1,\n",
        "        }, is_best, filename=os.path.join(SAVE_DIR, 'checkpoint_{}.tar'.format(epoch)))\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    \"\"\"\n",
        "        Run one train epoch\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    data_time  = AverageMeter()\n",
        "    losses     = AverageMeter()\n",
        "    top1       = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        target = target.type(torch.int64)\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        if CPU == False:\n",
        "            input = input.cuda(non_blocking=True)\n",
        "            target = target.cuda(non_blocking=True)\n",
        "        if HALF_PRECISION:\n",
        "            input = input.half()\n",
        "\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        output = output.float()\n",
        "        loss = loss.float()\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target)[0]\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % PRINT_FREQ == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                      data_time=data_time, loss=losses, top1=top1))\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion):\n",
        "    \"\"\"\n",
        "    Run evaluation\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses     = AverageMeter()\n",
        "    top1       = AverageMeter()\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "      # TODO: preprocess here\n",
        "        target = target.type(torch.int64)\n",
        "        if CPU == False:\n",
        "            input = input.cuda(non_blocking = True)\n",
        "            target = target.cuda(non_blocking = True)\n",
        "\n",
        "        if HALF_PRECISION:\n",
        "            input = input.half()\n",
        "\n",
        "        # compute output\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "        output = output.float()\n",
        "        #print(target.size())\n",
        "        loss = loss.float()\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target)[0]\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # if (i == 0):\n",
        "        #   maxk = max((1,))\n",
        "        #   batch_size = target.size(0)\n",
        "\n",
        "        #   _, output_lbl = output.topk(maxk, 1, True, True)\n",
        "        #   output_lbl = output_lbl.t()\n",
        "        #   display_batch(input, output_lbl)\n",
        "        if i % PRINT_FREQ == 0:\n",
        "            print('Test: [{0}/{1}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "                      top1=top1))\n",
        "\n",
        "    print(' * Prec@1 {top1.avg:.3f}'\n",
        "          .format(top1=top1))\n",
        "\n",
        "    return top1.avg\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    \"\"\"\n",
        "    Save the training model\n",
        "    \"\"\"\n",
        "    torch.save(state, filename)\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every 30 epochs\"\"\"\n",
        "    lr = LEARNING_RATE * (0.5 ** (epoch // 30))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    # Print predictions and targets for debugging\n",
        "    # print(\"Predictions (top-k indices):\")\n",
        "    # print(pred)\n",
        "    # print(\"True Labels:\")\n",
        "    # print(target)\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGYuOWI_wrjg",
        "outputId": "c3c21060-90ff-4a18-b076-5a284499fe52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'__module__': '__main__', '__doc__': '\\n    VGG model\\n    ', '__init__': <function VGG.__init__ at 0x7f7b4d4d2560>, 'forward': <function VGG.forward at 0x7f7b4d4d25f0>, 'vgg11': <function VGG.vgg11 at 0x7f7b4d4d2680>, 'vgg11_bn': <function VGG.vgg11_bn at 0x7f7b4d4d2710>, 'vgg13': <function VGG.vgg13 at 0x7f7b4d4d27a0>, 'vgg13_bn': <function VGG.vgg13_bn at 0x7f7b4d4d2830>, 'vgg16': <function VGG.vgg16 at 0x7f7b4d4d28c0>, 'vgg16_bn': <function VGG.vgg16_bn at 0x7f7b4d4d2950>, 'vgg19': <function VGG.vgg19 at 0x7f7b4d4d29e0>, 'vgg19_bn': <function VGG.vgg19_bn at 0x7f7b4d4d2a70>}\n",
            "=> loading checkpoint '/content/drive/MyDrive/TrafficSignData/Vietnam/24.08.18_Take1/checkpoint_99.tar'\n",
            "=> loaded checkpoint 'True' (epoch 100)\n",
            "Test: [0/92]\tTime 0.249 (0.249)\tLoss 0.0244 (0.0244)\tPrec@1 100.000 (100.000)\n",
            "Test: [20/92]\tTime 0.245 (0.249)\tLoss 0.0001 (0.0286)\tPrec@1 100.000 (99.851)\n",
            "Test: [40/92]\tTime 0.350 (0.258)\tLoss 0.0557 (0.0369)\tPrec@1 96.875 (99.314)\n",
            "Test: [60/92]\tTime 0.246 (0.273)\tLoss 0.0002 (0.0341)\tPrec@1 100.000 (99.334)\n",
            "Test: [80/92]\tTime 0.256 (0.267)\tLoss 0.0003 (0.0273)\tPrec@1 100.000 (99.383)\n",
            " * Prec@1 99.452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python my_mainScript.py --arch vgg16_bn --batch-size 64"
      ],
      "metadata": {
        "id": "r6ooUnKx97yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "IMAGE_EXTENSIONS = {'.png', '.jpg', '.jpeg', '.bmp'}\n",
        "IMG_WIDTH  = 32\n",
        "IMG_HEIGHT = 32\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 folder_path=None, # input: image folder to be converted to dataset\n",
        "                 images=None,      # input: image tensor [num_imgs, C, W, H]\n",
        "                 labels=None,      # input: labels tensor [num_labels]\n",
        "                 transform=None    # input: transformation method applied to images\n",
        "                 ):\n",
        "        self.transform = transform\n",
        "\n",
        "        if folder_path:\n",
        "            # Loading images from folder\n",
        "            self.image_files = [f for f in os.listdir(folder_path) if os.path.splitext(f)[1].lower() in IMAGE_EXTENSIONS]\n",
        "            self.folder_path = folder_path\n",
        "            self.images = None\n",
        "            self.labels = None\n",
        "        elif images is not None and labels is not None:\n",
        "            # Using pre-loaded tensors\n",
        "            self.images = images\n",
        "            self.labels = labels\n",
        "            self.image_files = None\n",
        "            self.folder_path = None\n",
        "        else:\n",
        "            raise ValueError(\"Either folder_path or (images, labels) must be provided\")\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.image_files:\n",
        "            return len(self.image_files)\n",
        "        elif self.images is not None:\n",
        "            return len(self.images)\n",
        "        else:\n",
        "            raise RuntimeError(\"Dataset is not properly initialized\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.image_files:\n",
        "            # Load image from disk\n",
        "            image_file = self.image_files[idx]\n",
        "            image_path = os.path.join(self.folder_path, image_file)\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "        else:\n",
        "            # Get image from pre-loaded tensors\n",
        "            image = self.images[idx]\n",
        "\n",
        "        # Apply transformation to image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Label tensor is available, add it to the dataset also\n",
        "        if self.labels is not None:\n",
        "            label = self.labels[idx]\n",
        "            return image, label\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "class PreProcessing:\n",
        "    LOWER_THRESH = 50\n",
        "    UPPER_THRESH = 200\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    \"\"\"\n",
        "        Input images are expected to be colored\n",
        "    \"\"\"\n",
        "    def augment_data(self,\n",
        "                     input_folder_path,\n",
        "                     output_folder_path,\n",
        "                     target_num_data):\n",
        "\n",
        "        # Define transformations for augmentation\n",
        "        augmentation_transforms = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.RandomRotation(20),\n",
        "            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "        ])\n",
        "\n",
        "        # Load images into a tensor\n",
        "        dataset    = ImageDataset(input_folder_path, transform=transforms.ToTensor())\n",
        "        dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "        # Ensure output folder exists\n",
        "        os.makedirs(output_folder_path, exist_ok=True)\n",
        "\n",
        "        images = []\n",
        "\n",
        "        # Collect all images in a list\n",
        "        for images_batch in dataloader:\n",
        "            images.append(images_batch[0])\n",
        "\n",
        "        images = torch.stack(images)\n",
        "\n",
        "        # Shuffle the images\n",
        "        images = images[torch.randperm(images.size(0))]\n",
        "\n",
        "        # Apply augmentations until target number of images is reached\n",
        "        org_img_idx  = 0\n",
        "        num_org_imgs = len(dataset)\n",
        "        augmented_count = target_num_data - len(dataset)\n",
        "        for augIdx in range(augmented_count):\n",
        "            img = images[org_img_idx]\n",
        "            img = transforms.ToPILImage()(img)\n",
        "            augmented_image = augmentation_transforms(img)\n",
        "            augmented_image_path = os.path.join(output_folder_path, f'augmented_{augIdx + 1}.png')\n",
        "            augmented_image.save(augmented_image_path)\n",
        "\n",
        "            org_img_idx = (org_img_idx + 1) % num_org_imgs\n",
        "\n",
        "    def apply_preprocessing(self,\n",
        "                            input_folder_path,\n",
        "                            output_folder_path):\n",
        "        self.create_folder(self, output_folder_path)\n",
        "\n",
        "        for filename in os.listdir(input_folder_path):\n",
        "            # Get the file extension\n",
        "            _, ext = os.path.splitext(filename)\n",
        "\n",
        "            # Check if the file is an image\n",
        "            if ext.lower() in IMAGE_EXTENSIONS:\n",
        "                file_path = os.path.join(input_folder_path, filename)\n",
        "                image     = cv2.imread(file_path)\n",
        "\n",
        "                # Convert to grayscale\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                # Noise reduction\n",
        "                image = cv2.GaussianBlur(image, (3, 3), 1.5)\n",
        "\n",
        "                # Otsu threshold\n",
        "                _, image = cv2.threshold(\n",
        "                    image,\n",
        "                    0,\n",
        "                    255, # images at exceeding the threshold is set to 255\n",
        "                    cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "                # Histogram equalization\n",
        "                image = cv2.equalizeHist(image)\n",
        "\n",
        "                # Image eroding\n",
        "                image = cv2.erode(image, (3, 3))\n",
        "\n",
        "                # Resize image\n",
        "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
        "\n",
        "                output_filepath = os.path.join(output_folder_path, filename)\n",
        "                cv2.imwrite(output_filepath, image)\n",
        "\n",
        "    @staticmethod\n",
        "    def create_folder(self, folder_path):\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def save_images(self, dataset: Dataset, output_folder_path: str):\n",
        "        os.makedirs(output_folder_path, exist_ok=True)\n",
        "\n",
        "        dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "        for idx, (image_batch, _) in enumerate(dataloader):\n",
        "            img = image_batch[0]\n",
        "            img_pil = transforms.ToPILImage()(img)\n",
        "            original_image_path = os.path.join(output_folder_path, f'original_{idx + 1}.png')\n",
        "            img_pil.save(original_image_path)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    INPUT_DIR                = r\"C:\\MSPACE\\02_DAT\\TestImg\"\n",
        "    OUTPUT_DIR_PREPROCESSING = r\"C:\\MSPACE\\02_DAT\\output_process\"\n",
        "    OUTPUT_DIR_AUGMENTATION  = r\"C:\\MSPACE\\02_DAT\\output_augmentation\"\n",
        "    preprocessor = PreProcessing()\n",
        "\n",
        "    #preprocessor.apply_preprocessing(INPUT_DIR, OUTPUT_DIR_PREPROCESSING)\n",
        "\n",
        "    \"\"\"\n",
        "        ImageDataSet usage:\n",
        "\n",
        "        1. Image folder:\n",
        "            transform = transform.ToTensor()\n",
        "            dataset_from_folder    = ImageDataset(folder_path='path_to_images', transform=transform)\n",
        "            dataloader_from_folder = DataLoader(dataset_from_folder, batch_size=10, shuffle=True)\n",
        "\n",
        "        2. Image and Label tensors:\n",
        "            images = torch.randn(100, 3, 128, 128)  # Example tensor with shape (num_samples, channels, height, width)\n",
        "            labels = torch.randint(0, 10, (100,))   # Example tensor with class labels\n",
        "            dataset_from_tensors    = ImageDataset(images=images, labels=labels, transform=transform)\n",
        "            dataloader_from_tensors = DataLoader(dataset_from_tensors, batch_size=10, shuffle=True)\n",
        "    \"\"\"\n",
        "\n",
        "    preprocessor.augment_data(INPUT_DIR, OUTPUT_DIR_AUGMENTATION, 5)\n",
        "    # image_index = 0\n",
        "\n",
        "    # for i in range(20):\n",
        "    #     # Move to the next image, wrap around if needed\n",
        "    #     print(image_index)\n",
        "    #     image_index = (image_index + 1) % 3"
      ],
      "metadata": {
        "id": "D6ROMLQUCnih",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "0ca02cea-9218-44ad-ba2e-7e1228526547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\MSPACE\\\\02_DAT\\\\TestImg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9a0e0cfe9e2e>\u001b[0m in \u001b[0;36m<cell line: 175>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m     \"\"\"\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIR_AUGMENTATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0;31m# image_index = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-9a0e0cfe9e2e>\u001b[0m in \u001b[0;36maugment_data\u001b[0;34m(self, input_folder_path, output_folder_path, target_num_data)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load images into a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mdataset\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-9a0e0cfe9e2e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, folder_path, images, labels, transform)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfolder_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Loading images from folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mIMAGE_EXTENSIONS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolder_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\MSPACE\\\\02_DAT\\\\TestImg'"
          ]
        }
      ]
    }
  ]
}