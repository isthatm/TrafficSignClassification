{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil, pickle, os, random, matplotlib\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "\n",
    "# data logging\n",
    "import configparser\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Main</strong>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "The data is converted into RGB tensors with skimage.io.imread()\n",
    "\n",
    "<strong>Notice:</strong> pip install scikit-image "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingReporter:\n",
    "    def __init__(self, file_name) -> None:\n",
    "        self.file_name = file_name\n",
    "        self.cols = [\"Batch ID\",  \"Accuracy\", \"Margin Loss\", \"Reconstruction Loss\", \"Total Loss\"]\n",
    "        self.epoch, self.accuracy, self.marg_loss, self.recons_loss, self.total_loss  = [], [], [], [], []\n",
    "        \n",
    "        self.__write_rp()\n",
    "\n",
    "    def __write_rp(self):\n",
    "        df = pd.DataFrame(list(\n",
    "            zip(self.epoch, self.accuracy, self.marg_loss, self.recons_loss,self.total_loss)), \n",
    "            columns=self.cols) \n",
    "\n",
    "    def record(self, data: dict) -> None: \n",
    "        self.epoch.append(data['Batch ID'])\n",
    "        self.accuracy.append(data['Accuracy'])\n",
    "        self.marg_loss.append(data['Loss']['Margin'])\n",
    "        self.recons_loss.append(data['Loss']['Recon'])\n",
    "        self.total_loss.append(data['Loss']['Total'])\n",
    "\n",
    "    def test_dict(self):\n",
    "        train_dict = {}\n",
    "        a = [1,2,3,4,5]\n",
    "        for entry in self.cols:\n",
    "            print(entry)\n",
    "            train_dict.update({entry: a})\n",
    "        print(train_dict)\n",
    "a = TrainingReporter(\"hello.csv\")\n",
    "a.test_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],\n",
    "                   index=['row 1', 'row 2'],\n",
    "                   columns=['col 1', 'col 2'])\n",
    "df1.to_excel(\"output.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Pickling data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data(file, writeColumns=None):\n",
    "    \"\"\"\n",
    "    Read/Write pickle training/testing data, models to avoid\n",
    "    loading data again (time consuming)\n",
    "    \n",
    "    ---Params---\n",
    "\n",
    "    file: path to pickle file\n",
    "\n",
    "    writeColumns (array): variables to be saved to pickle file\n",
    "\n",
    "    \"\"\"\n",
    "    if writeColumns is None:\n",
    "        with open(file, mode=\"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "            return tuple(map(lambda col: dataset[col], ['images', 'labels'])) # lambda(col) where columns are the inputs\n",
    "    else:\n",
    "        with open(file, mode=\"wb\") as f:\n",
    "            dataset = pickle.dump({\"images\": writeColumns[0], \"labels\": writeColumns[1]}, f)\n",
    "            print(\"Data is saved in\", file)\n",
    "# lambda function: https://www.youtube.com/watch?v=BcbVe1r2CYc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Label the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_test(src, csv_file, labeled_test_dir, NoOfCategories):\n",
    "    \"\"\"\n",
    "    This function creates named folders corresponding to 43 categories\n",
    "    and move the test images to these folders\n",
    "\n",
    "    `csv_file` and `labeled_test_dir` should have already been in src directly \n",
    "    (create a blank folder to store labeled images)\n",
    "\n",
    "    \"\"\"\n",
    "    # Remove the existing folders in the labeled test directory if there is any\n",
    "    for filename in os.listdir(labeled_test_dir):\n",
    "        file_path = os.path.join(labeled_test_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "    csv_dir = os.path.join(src, csv_file)\n",
    "    SubTestDir = [os.path.join(labeled_test_dir, str(d)) for d in range(NoOfCategories)]\n",
    "    \n",
    "    # Create label folders\n",
    "    [os.mkdir(test_d) for test_d in SubTestDir]\n",
    "\n",
    "    testImageDir = pd.read_csv(csv_dir)['Path']\n",
    "    testImageLabel = pd.read_csv(csv_dir)['ClassId']\n",
    "    for idx in range(len(testImageLabel)):\n",
    "        label = testImageLabel[idx]\n",
    "        shutil.copy(os.path.join(src, testImageDir[idx]), SubTestDir[label])\n",
    "\n",
    "labeled_test_dir = \"D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\German\\LabeledTest\"\n",
    "# labeled_test_dir = r\"C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\German\\LabeleTest\" # BH   \n",
    "src = \"D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\German\" \n",
    "# src = r\"C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\German\" # BH\n",
    "csv_file = \"Test.csv\"\n",
    "NoOfCats = 43\n",
    "\n",
    "label_test(src, csv_file, labeled_test_dir, NoOfCats)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\"Loads a data set and returns two lists:\n",
    "    \n",
    "    images: a list of Numpy arrays, each representing an image.\n",
    "    labels: a list of numbers that represent the images labels.\n",
    "    \"\"\"\n",
    "    # Get all subdirectories of data_dir. Each represents a label.\n",
    "    directories = [d for d in os.listdir(data_dir) \n",
    "                   if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    # Loop through the label directories and collect the data in\n",
    "    # two lists, labels and images.\n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in directories:\n",
    "        # label_dir contains 61 catefories paths\n",
    "        label_dir = os.path.join(data_dir, d)\n",
    "\n",
    "        # list subdirectories within each of the 61 categories\n",
    "        file_names = [os.path.join(label_dir, f) \n",
    "                      for f in os.listdir(label_dir) if f.endswith(\".png\")]\n",
    "        # For each label, load it's images and add them to the images list.\n",
    "        # And add the label number (i.e. directory name) to the labels list.\n",
    "        for f in file_names:\n",
    "            images.append(skimage.io.imread(f))\n",
    "            labels.append(int(d))\n",
    "    return images, labels\n",
    "\n",
    "# ROOT_PATH = \"D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\German\"\n",
    "ROOT_PATH = r\"C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\German\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last execution 10:01PM 20/5/23\n",
    "!!!: resized images are already normalized to [0,1] format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset.\n",
    "train_data_dir = os.path.join(ROOT_PATH, \"Train\")\n",
    "first_train_images, first_train_labels = load_data(train_data_dir)\n",
    "pickle_data(file = \"primary_train_dataset\", writeColumns = [first_train_images, first_train_labels] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply constant resolution among train images and pickle them \n",
    "train_images, train_labels  = pickle_data(file = 'primary_train_dataset')\n",
    "\n",
    "train_images = [ skimage.transform.resize(train_image, (32, 32), mode = \"constant\") \n",
    "                            for train_image in train_images ]\n",
    "\n",
    "train_images = np.stack(train_images, axis = 0)\n",
    "\n",
    "pickle_data(file = \"primary32_train_dataset\", writeColumns = [train_images, train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_data_dir = os.path.join(ROOT_PATH, \"LabeledTest\")\n",
    "first_test_images, first_test_labels = load_data(test_data_dir)\n",
    "test_images, val_images, test_labels, val_labels = train_test_split(first_test_images, first_test_labels, \n",
    "                                                                        test_size=0.36, random_state=0)\n",
    "pickle_data(file = \"primary_test_dataset\", writeColumns = [test_images, test_labels])\n",
    "pickle_data(file = \"primary_val_dataset\", writeColumns = [val_images, val_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply constant resolution among test,val images and pickle them \n",
    "test_images, test_labels  = pickle_data(file = 'primary_test_dataset')\n",
    "val_images, val_labels  = pickle_data(file = 'primary_val_dataset')\n",
    "\n",
    "test_images = [ skimage.transform.resize(test_image, (32, 32), mode = \"constant\") \n",
    "                            for test_image in test_images ]\n",
    "val_images = [ skimage.transform.resize(val_image, (32, 32), mode = \"constant\") \n",
    "                            for val_image in val_images ]\n",
    "\n",
    "test_images = np.stack(test_images, axis = 0)\n",
    "val_images = np.stack(val_images, axis = 0)\n",
    "\n",
    "pickle_data(file = \"primary32_test_dataset\", writeColumns = [test_images, test_labels])\n",
    "pickle_data(file = \"primary32_val_dataset\", writeColumns = [val_images, val_labels])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program starts here if pickle folders are not updated\n",
    "train_images, train_labels  = pickle_data(file = './Data/primary32_train_dataset')\n",
    "test_images, test_labels  = pickle_data(file = './Data/primary32_test_dataset')\n",
    "val_images, val_labels  = pickle_data(file = './Data/primary32_val_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Display images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images, labels, category=False, greyScale=False):\n",
    "    \"\"\"\n",
    "    Display the first image of each label.\n",
    "    \n",
    "    category: set to True when only images within a category are displayed\n",
    "    greyScale: set to True to display images in grey scale\n",
    "    \"\"\"\n",
    "\n",
    "    if category:\n",
    "        i = 1\n",
    "        startIndex = labels.index(category)\n",
    "        catImages = images[startIndex:(startIndex + labels.count(category))] #catImage = categoryImage\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        for catImage in catImages[:24]:\n",
    "            plt.subplot(8, 8, i)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.imshow(catImage)\n",
    "            i += 1 \n",
    "    else:\n",
    "        unique_labels = set(labels) # Create a list contains only the labels (non-iterative)\n",
    "    \n",
    "        #Example: a = [1, 1, 1, 2, 2, 3]\n",
    "        #set(a) >> {1,2,3}\n",
    "\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        i = 1\n",
    "        for label in unique_labels:\n",
    "            image = images[labels.index(label)] # Pick the first image for each label.\n",
    "\n",
    "        # object.index(element) returns the index of the element specified when it's first encountered\n",
    "        # Example: a = [1, 1, 1, 3, 2, 2, 3]\n",
    "        # a.index(3) = 3\n",
    "\n",
    "            plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.title(\"Label {0} ({1})\".format(label, labels.count(label))) # sign category and the # of its samples\n",
    "            if greyScale is False:\n",
    "                plt.imshow(image)\n",
    "            else:\n",
    "                plt.imshow(image, cmap=plt.cm.binary)\n",
    "            i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Convert to grey scale, improve contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "\n",
    "def preprocess_images(images):\n",
    "    \"\"\"\n",
    "        - Convert RGB images to grey scale \n",
    "        - Normalize pixels to 0-1,\n",
    "        - Improve the contrast with adaptive histogram localization\n",
    "    \"\"\"\n",
    "     \n",
    "    # Conver RGB -> grey scale\n",
    "    images = 0.299 * images[:, :, :, 0] + 0.587 * images[:, :, :, 1] + 0.114 * images[:, :, :, 2]\n",
    "    # Improve the contrast\n",
    "    #images = exposure.equalize_adapthist(images)\n",
    "\n",
    "    # Add ONE 3-D channel for grey scale\n",
    "    images = images.reshape(images.shape + (1,)) \n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import torchvision.transforms as T\n",
    "\n",
    "a = torch.Tensor(train_images[0]) # RGB images\n",
    "b = torch.Tensor(train_images[1])\n",
    "c = torch.stack((a,b), dim=0).permute(0, 3, 1, 2) # create a 2 batchs with 1 image in each batch\n",
    "\n",
    "class ImgAug:\n",
    "    def __init__(self, batch_img, value):\n",
    "        \"\"\"\n",
    "        Augment images by batch\n",
    "        :param batch_img: [batch, C, H, W]\n",
    "        :param value: the degree at which the imgs are transformed \n",
    "                      (recommended 0.3 -> 9 pixels)\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_img = batch_img\n",
    "        self.value = value\n",
    "        aug_method = random.randint(0, 2)\n",
    "        aug_dict = {\n",
    "            '0': self._horizontal_shift(),\n",
    "            '1': self._vertical_shift(),\n",
    "            '2': self._rotate()\n",
    "        }\n",
    "        print(aug_method)\n",
    "        augmented_batch = aug_dict[str(aug_method)]\n",
    "        resized_img = self._fill(augmented_batch).permute(0, 2, 3, 1)\n",
    "       \n",
    "    @staticmethod\n",
    "    def _fill(img):\n",
    "        return T.Resize(size=(32,32))(img)\n",
    "\n",
    "    def _horizontal_shift(self):\n",
    "        ratio = random.uniform(-self.value, self.value)\n",
    "        shift_by = int(ratio*(self.batch_img.size()[-1]))\n",
    "        if ratio < 0: # shift to the right\n",
    "            shifted_batch_img = self.batch_img[:, :, :, :shift_by]\n",
    "        elif ratio > 0: # shift to the left\n",
    "            shifted_batch_img = self.batch_img[:, :, :, shift_by:]\n",
    "        else:\n",
    "            shifted_batch_img = self.batch_img\n",
    "        \n",
    "        return shifted_batch_img\n",
    "\n",
    "    def _vertical_shift(self):\n",
    "        ratio = random.uniform(-self.value, self.value)\n",
    "        shift_by = int(ratio*(self.batch_img.size()[-1]))\n",
    "        if ratio < 0: # shift to the right\n",
    "            shifted_batch_img = self.batch_img[:, :, :shift_by, :]\n",
    "        elif ratio > 0: # shift to the left\n",
    "            shifted_batch_img = self.batch_img[:, :, shift_by:, :]\n",
    "        else:\n",
    "            shifted_batch_img = self.batch_img\n",
    "\n",
    "        return shifted_batch_img\n",
    "    \n",
    "    def _rotate(self):\n",
    "        return T.RandomRotation(degrees=90)(self.batch_img)\n",
    "\n",
    "shifted_batch = ImgAug(c, 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_images[0]\n",
    "# print(a.shape)\n",
    "a = 0.299 * a[:, :, 0] + 0.587 * a[:, :, 1] + 0.114 * a[:, :, 2]\n",
    "print(a.shape)\n",
    "a = exposure.equalize_adapthist(a)\n",
    "a = a.reshape(a.shape + (1,)) \n",
    "plt.imshow(a,cmap=plt.cm.binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all images \n",
    "display_images(preprocess_images(train_images, improveCon=True), train_labels, greyScale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all images \n",
    "display_images(preprocess_images(train_images), train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "n_classes = len(set(train_labels))\n",
    "values, bins, patches = ax.hist(train_labels, n_classes)\n",
    "ax.set_xlabel(\"Classes\")\n",
    "ax.set_ylabel(\"Number of images\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "X_train = train_images\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "inference_datagen = ImageDataGenerator()\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "train_datagen.fit(X_train)\n",
    "train_datagen_augmented.fit(X_train)\n",
    "fig = plt.figure()\n",
    "n = 0\n",
    "graph_size = 3\n",
    "\n",
    "for x_batch, y_batch in train_datagen_augmented.flow(X_train, train_labels, batch_size=4):\n",
    "    a=fig.add_subplot(graph_size, graph_size, n+1)\n",
    "    greyBatch = preprocess_images(x_batch)\n",
    "    print(greyBatch[0].shape)\n",
    "    imgplot = plt.imshow(greyBatch[0])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Label:{}\".format(y_batch[0]))\n",
    "    n = n + 1\n",
    "    if n > 8:\n",
    "        break\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('test_img.png')\n",
    "def rotation(img, angle):\n",
    "    angle = int(random.uniform(-angle, angle))\n",
    "    h, w = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((int(w/2), int(h/2)), angle, 1)\n",
    "    img = cv2.warpAffine(img, M, (w, h))\n",
    "    return img\n",
    "img = rotation(img, 30)    \n",
    "cv2.imshow('Result', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capsule layer attributes\n",
    "TODO:\n",
    "- build a simple conv layer: DONE (first conv layer with relu)\n",
    "- build capsule's functions (squash, routing)\n",
    "- build primary and secondary capsnet\n",
    "- fully connected layer for reconstruction\n",
    "- loss functions\n",
    "- add visualizations to track the changes in prior logits, training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu will be used\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Currently this network only supports grey scale imgage due to fully connected reconstruction layer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "logging.basicConfig(filename='caps_net.log', filemode='w', format='%(asctime)s: %(message)s', level=logging.DEBUG)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\n",
    "    r'D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\src\\capsnet_config.ini'\n",
    "    )\n",
    "\n",
    "# General info\n",
    "num_class = int(config['network']['num_class'])\n",
    "img_channels = int(config['network']['image_channels'])\n",
    "batch_size = int(config['network']['batch_size'])\n",
    "learning_rate = float(config['network']['lr'])\n",
    "epochs = int(config['network']['epochs'])\n",
    "\n",
    "train_num = torch.tensor(train_images).size()[0]\n",
    "valid_num = torch.tensor(val_images).size()[0]\n",
    "test_num = torch.tensor(test_images).size()[0]\n",
    "\n",
    "# Primary caps\n",
    "primary_num_caps = int(config['primary_caps']['num_caps'])\n",
    "primary_channels = int(config['primary_caps']['channels'])\n",
    "\n",
    "# Digit caps\n",
    "digit_num_caps = num_class\n",
    "digit_channels = int(config['digit_caps']['channels'])\n",
    "num_iterations = int(config['network']['num_routing_iter'])\n",
    "\n",
    "# Loss hyper params\n",
    "m_plus = float(config['loss']['m_plus'])\n",
    "m_minus = float(config['loss']['m_minus'])\n",
    "lmbd = float(config['loss']['lambda'])\n",
    "regularization_factor = float(config['loss']['regularization_factor'])\n",
    "\n",
    "\n",
    "shuffeled_train_images, shuffeled_train_labels = shuffle(train_images, train_labels, random_state=0)\n",
    "demo_train_images = shuffeled_train_images[0:100]\n",
    "demo_train_labels = shuffeled_train_labels[0:100]\n",
    "X_train = demo_train_images\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "inference_datagen = ImageDataGenerator()\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "def squash(vector, axis=-1 ,epsilon=1e-7, squash=True):\n",
    "        \"\"\"\n",
    "        normalize the length of the vector by 1\n",
    "        `vector`: the muliplication of coupling coefs and prediction vectors sum [ c(ij)u^(j|i) ]\n",
    "        `axis`: the axis that would not be reduced\n",
    "        'epsilon`: a workaround to prevent devision by zero\n",
    "        \"\"\"\n",
    "        squared_norm = torch.sum(torch.square(vector), dim=axis, \n",
    "                                keepdim=True)\n",
    "        safe_norm = torch.sqrt(squared_norm + epsilon)\n",
    "\n",
    "        if squash:\n",
    "            squash_factor = squared_norm / (1. + squared_norm)\n",
    "            unit_vector = vector / safe_norm\n",
    "            return squash_factor * unit_vector\n",
    "        else:\n",
    "            return safe_norm\n",
    "\n",
    "\n",
    "class CapsLayers(nn.Module):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "    :param num_conv: number of filters/convolutional unit per capsule (dimension of a capsule)\n",
    "    :param num_capsules: number of primary/digit caps\n",
    "    :param num_routing_nodes: number of possible u(i), \n",
    "                            set to -1 if it's not a secondary capsule layer\n",
    "    :param in_channels: output convolutional layers of the prev layer\n",
    "    :param out_channels: output convolutional layers of the current layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsules: int, in_channels: int, out_channels: int, \n",
    "                 kernel_size=None, stride=None, num_routing_nodes=None ,num_iterations=None):\n",
    "        super(CapsLayers, self).__init__()\n",
    "        self.num_capsules = num_capsules\n",
    "        \n",
    "        self.num_iterations = num_iterations\n",
    "        self.num_routing_nodes = num_routing_nodes\n",
    "        if num_routing_nodes is not None:\n",
    "            self.weights = nn.Parameter(torch.randn(\n",
    "                                        self.num_routing_nodes, num_capsules, out_channels, in_channels))\n",
    "            self.b = nn.Parameter(torch.zeros(\n",
    "                                    self.num_routing_nodes, num_capsules, 1, 1))\n",
    "        else:\n",
    "            self.primary_caps = nn.ModuleList(nn.Conv2d(in_channels, out_channels, kernel_size, stride) \n",
    "                                                    for _ in range(num_capsules))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Feed foward function for non-reconstruction layer\n",
    "        :param inputs: \n",
    "            for the primary caps, the inputs are convolutional layer pixels\n",
    "            for digit caps, the inputs are n-D vectors from a primary cap\n",
    "                where n is the # of filters for one capsule  \n",
    "            Required Paramteters:\n",
    "            prior_logits(b) \n",
    "            primary layer prediction (requires u-layer 1 ouput, Weights)\n",
    "        \"\"\"\n",
    "        if self.num_routing_nodes is not None:\n",
    "            weights = self.weights[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "            b_ij = self.b[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "            inputs = inputs.tile(1, 1, self.num_capsules, 1, 1)\n",
    "            \n",
    "            # u_hat = [batch, num_routing_nodes, # digit_caps, digit_caps_dims, 1]\n",
    "            u_hat = weights @ inputs \n",
    "\n",
    "            for i in range(self.num_iterations):\n",
    "                c_ij = F.softmax(b_ij, dim=2)\n",
    "                outputs = squash((c_ij*u_hat).sum(dim=1, keepdim=True))\n",
    "\n",
    "                if i < self.num_iterations - 1 :\n",
    "                    # v_j OR outputs = [batch, 1 -> num_routing_nodes, num_digit_caps, digit_caps_dims, 1 )]\n",
    "                    b_ij +=  (b_ij + torch.transpose(u_hat, 3, 4) @ outputs.tile(1, self.num_routing_nodes, 1, 1, 1))\n",
    "\n",
    "        else:\n",
    "            outputs = [\n",
    "                capsule(inputs)[:, None, :, :, :].permute(0, 1, 3, 4, 2) for capsule in self.primary_caps]\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "            # u(i) = [batch, num_prim_caps*prim_caps_2D_size, prim_caps_output_dimension]\n",
    "            outputs = outputs.view(outputs.size(0), -1, outputs.size(4)) \n",
    "            outputs = squash(outputs)[:, :, None, :, None]\n",
    "        \n",
    "        # outputs = [batch, 1, num_digit_caps/num_class, digit_caps_dims, 1 )]\n",
    "        return outputs            \n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    \"\"\"\n",
    "        This class contains the full CapsNet architecture:\n",
    "        Convolutional -> primary capsules -> digit capsules -> (3) fully connected\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Params: \n",
    "        `inputs`: a 4D tensor (grey scale or RGB)\n",
    "        \"\"\"\n",
    "        super(CapsNet, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(img_channels, 256, \n",
    "                                kernel_size=9, stride=1)\n",
    "        self.primary_caps = CapsLayers(primary_num_caps, 256, primary_channels, \n",
    "                                        kernel_size=5, stride=2)\n",
    "        self.digit_caps = CapsLayers(digit_num_caps, primary_channels, digit_channels, \n",
    "                                     num_routing_nodes=10*10*primary_num_caps, num_iterations=num_iterations)\n",
    "        self.grey_scale_decoder = nn.Sequential(\n",
    "                nn.Linear(digit_channels*digit_num_caps, 576),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(576, 1600),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1600, 1024),\n",
    "                nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.RGB_decoder = nn.Sequential(\n",
    "                nn.Upsample(size=(8, 8)),\n",
    "                nn.Conv2d(16, 4, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(4, 8, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(8, 16, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv2d(16, 3, 3, padding='same'),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, images, labels=None): # labels should be applied `one_hot` function\n",
    "        conv_1_ouputs = F.relu(self.conv_1(images))\n",
    "        primary_caps_outputs =  self.primary_caps(conv_1_ouputs) #TODO: check again the size of conv1ouputs to \n",
    "        # verify capsules takes the correct size (since conv outputs has size [b, v, h, w])\n",
    "        digit_caps_outputs = self.digit_caps(primary_caps_outputs).squeeze(1)\n",
    "        \n",
    "        assert list(digit_caps_outputs.size()) == [images.size()[0], num_class, digit_channels, 1]\n",
    "               \n",
    "        v_norm = squash(digit_caps_outputs, axis=-2, squash=False)\n",
    "        v_prob = F.softmax(v_norm, dim=1)\n",
    "\n",
    "        self.img = images\n",
    "        self.v_norm = v_norm\n",
    "\n",
    "        idx = torch.zeros(images.size()[0], 1, 1)\n",
    "        # Masking\n",
    "        if labels is None: # Testing mode\n",
    "            _, idx = torch.max(v_prob, dim=1)\n",
    "            labels = torch.eye(num_class).index_select(dim=0, index = idx.squeeze())\n",
    "\n",
    "        #masked_v = [batch_size, digit_channels*classes])\n",
    "        masked_v = (labels[:, :, None, None] * digit_caps_outputs).view(images.size(0), -1)\n",
    "        \n",
    "        # Reconstruction\n",
    "        if images.size()[1] == 1:\n",
    "            reconstructed_img = self.grey_scale_decoder(masked_v) # [batch, 32x32]\n",
    "\n",
    "        else:\n",
    "            linear_trans = nn.Linear(digit_channels*digit_num_caps, 400)\n",
    "            fc_out = linear_trans(masked_v)\n",
    "            reconstructed_img = self.RGB_decoder(fc_out) # [batch, channel, height, width]\n",
    "\n",
    "        logging.debug(f'Capsule layer pred / batch: {idx}')\n",
    "\n",
    "        return int(idx), reconstructed_img \n",
    "        \n",
    "\n",
    "    def loss_fn(self, reconstructed_img, labels):\n",
    "        # Margin loss\n",
    "        max_1 = F.relu(m_plus - self.v_norm)\n",
    "        max_2 = F.relu(self.v_norm - m_minus)\n",
    "        T_k = labels[:, :, None, None]\n",
    "        \n",
    "        L_k = T_k * torch.square(max_1) + lmbd * (1 - T_k) * torch.square(max_2)\n",
    "\n",
    "        assert L_k.size() == self.v_norm.size()\n",
    "        margin_loss = L_k.sum(dim=1).mean()\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss_obj = nn.MSELoss()\n",
    "\n",
    "        # original_img = [batch size, flatten image (pixels are flatten into arrays)]\n",
    "        original_img = self.img.view(batch_size, -1)\n",
    "        reconstruction_loss = reconstruction_loss_obj(reconstructed_img, original_img)\n",
    "        \n",
    "        total_loss = margin_loss + regularization_factor * reconstruction_loss\n",
    "\n",
    "        return margin_loss, reconstruction_loss, total_loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    # plot_batch = plt.figure()\n",
    "    n = 0\n",
    "    model.train() # set the model to training mode\n",
    "    for batch_idx in range(train_num//batch_size):\n",
    "        # plot_batch.add_subplot(10, 10, n+1)\n",
    "        batch = next(train_loader)\n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "        # plt.imshow(x_batch[0])\n",
    "        y_batch = torch.Tensor(y_batch)\n",
    "        y_batch = torch.nn.functional.one_hot(y_batch.to(torch.int64), num_classes=43)\n",
    "\n",
    "        x_batch = torch.Tensor(preprocess_images(x_batch))\n",
    "        x_batch = x_batch.permute(0, 3, 1, 2)# [batch_size, channels, height, width]\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            x_batch, y_batch = x_batch.cuda(), y_batch.cuda() \n",
    "\n",
    "        _, recons_img = model(x_batch, y_batch)\n",
    "        margin_loss, recons_loss, total_loss = model.loss_fn(recons_img, y_batch)\n",
    "        total_loss.backward()\n",
    "        optimizer.step() # update the params to be optimized (weights, biases, routing weights)\n",
    "     \n",
    "        if batch_idx % (batch_size * 2) == 0:\n",
    "            print(f'Batch {batch_idx+1}/{train_num//batch_size}')\n",
    "        \n",
    "        if n > 100:\n",
    "            break\n",
    "        n += 1 \n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Can be used for both test and validation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    for idx, (batch_images, batch_labels) in enumerate(test_loader):\n",
    "        if device == 'cuda':\n",
    "            batch_images = batch_images.cuda()\n",
    "            batch_labels = batch_labels.cuda()\n",
    "        \n",
    "        image = torch.Tensor(\n",
    "            preprocess_images(image[None, :, :, :]).permute(0, 3, 1, 2),\n",
    "            dtype=torch.float32)\n",
    "        \n",
    "        pred_idx, recons_img = model(image)\n",
    "        \n",
    "def data_loader(images: np.array, \n",
    "                labels: np.array, \n",
    "                batch_size: int,\n",
    "                shuffle: bool) -> object:\n",
    "    \"\"\"\n",
    "    :param images: 3D array [H, W, Channels]\n",
    "    :param labels: 1D array\n",
    "    :param batch_size: number of images per batch\n",
    "\n",
    "    :return: an iterable object\n",
    "    \"\"\"\n",
    "    images_tensor = torch.Tensor(images)\n",
    "    labels_tensor = torch.Tensor(labels)\n",
    "\n",
    "    dataset = TensorDataset(images_tensor, labels_tensor)\n",
    "    data_loader = DataLoader(dataset, batch_size, shuffle=shuffle)\n",
    "\n",
    "    return data_loader\n",
    "if __name__ == '__main__':\n",
    "    device_as_str = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device_as_str, \"will be used\")\n",
    "\n",
    "    model = CapsNet() #.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, min_lr=1e-5)\n",
    "\n",
    "    train_loader = train_datagen_augmented.flow(X_train, demo_train_labels, batch_size=batch_size)\n",
    "\n",
    "    # Validation dataset\n",
    "    val_loader = data_loader(val_images, val_labels, batch_size, shuffle=False)\n",
    "    train_loader = data_loader(train_images, train_labels, batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        # train(model, train_loader, optimizer, device_as_str) \n",
    "        val_loss = test(model, val_loader, device_as_str)\n",
    "        # scheduler.step(val_loss)\n",
    "\n",
    "    # should put next_batch into train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset ,random_split\n",
    "\n",
    "img = torch.Tensor(val_images)\n",
    "labels = torch.Tensor(val_labels)\n",
    "\n",
    "my_dataset = TensorDataset(img, labels)\n",
    "test_loader = DataLoader(\n",
    "        my_dataset,\n",
    "        batch_size=10, shuffle=False)\n",
    "# print(len(test_loader))\n",
    "for data, target in test_loader:\n",
    "    print(data.size(), target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "#Declare transform to convert raw data to tensor\n",
    "transforms = transforms.Compose([\n",
    "\t\t\t\t\t\t\t\ttransforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Loading Data and splitting it into train and validation data\n",
    "train = datasets.MNIST('', train = True, transform = transforms, download = True)\n",
    "train, valid = random_split(train,[50000,10000])\n",
    "\n",
    "# Create Dataloader of the above tensor with batch size = 32\n",
    "trainloader = DataLoader(train, batch_size=32)\n",
    "validloader = DataLoader(valid, batch_size=32)\n",
    "print(\"!!!!!\", len(trainloader))\n",
    "# Building Our Mode\n",
    "class Network(nn.Module):\n",
    "\t# Declaring the Architecture\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Network,self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(28*28, 256)\n",
    "\t\tself.fc2 = nn.Linear(256, 128)\n",
    "\t\tself.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "\t# Forward Pass\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x.view(x.shape[0],-1) # Flatten the images\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\tx = self.fc3(x)\n",
    "\t\treturn x\n",
    "\n",
    "model = Network()\n",
    "if torch.cuda.is_available():\n",
    "\tmodel = model.cuda()\n",
    "\n",
    "# Declaring Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "# Training with Validation\n",
    "epochs = 5\n",
    "min_valid_loss = np.inf\n",
    "\n",
    "for e in range(epochs):\n",
    "\ttrain_loss = 0.0\n",
    "\tfor data, labels in trainloader:\n",
    "\t\t# Transfer Data to GPU if available\n",
    "\t\tif torch.cuda.is_available():\n",
    "\t\t\tdata, labels = data.cuda(), labels.cuda()\n",
    "\t\t\n",
    "\t\t# Clear the gradients\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\t# Forward Pass\n",
    "\t\ttarget = model(data)\n",
    "\t\t# Find the Loss\n",
    "\t\tloss = criterion(target,labels)\n",
    "\t\t# Calculate gradients\n",
    "\t\tloss.backward()\n",
    "\t\t# Update Weights\n",
    "\t\toptimizer.step()\n",
    "\t\t# Calculate Loss\n",
    "\t\ttrain_loss += loss.item()\n",
    "\t\n",
    "\tvalid_loss = 0.0\n",
    "\tmodel.eval()\t # Optional when not using Model Specific layer\n",
    "\tfor data, labels in validloader:\n",
    "\t\t# Transfer Data to GPU if available\n",
    "\t\tif torch.cuda.is_available():\n",
    "\t\t\tdata, labels = data.cuda(), labels.cuda()\n",
    "\t\t\n",
    "\t\t# Forward Pass\n",
    "\t\ttarget = model(data)\n",
    "\t\t# Find the Loss\n",
    "\t\tloss = criterion(target,labels)\n",
    "\t\t# Calculate Loss\n",
    "\t\tvalid_loss += loss.item()\n",
    "\n",
    "\tprint(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(trainloader)} \\t\\t Validation Loss: {valid_loss / len(validloader)}')\n",
    "\tprint('@@@@@@', len(validloader))\n",
    "\tif min_valid_loss > valid_loss:\n",
    "\t\tprint(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "\t\tmin_valid_loss = valid_loss\n",
    "\t\t\n",
    "\t\t# Saving State Dict\n",
    "\t\ttorch.save(model.state_dict(), 'saved_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_channels = 16\n",
    "digit_num_caps = 43\n",
    "\n",
    "grey_scale_decoder = nn.Sequential(\n",
    "                nn.Linear(digit_channels*digit_num_caps, 576, device='cuda'),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(576, 1600, device='cuda'),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1600, 1024, device='cuda'),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "RGB_decoder = nn.Sequential(\n",
    "            nn.Upsample(size=(8, 8)),\n",
    "            nn.Conv2d(16, 4, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(4, 8, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(8, 16, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16, 3, 3, padding='same'),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "inputs = torch.ones(2, 43, 16, 1).view(2, -1)\n",
    "decoder = nn.Linear(digit_channels*digit_num_caps, 400)\n",
    "decoded_v = decoder(inputs).view(2, digit_channels, 5, 5) #torch.Size([2, 16, 8, 8])\n",
    "\n",
    "inputs2 = torch.ones(2, 43, 16, 1, device = 'cuda').view(2, -1)\n",
    "\n",
    "output1 = RGB_decoder(decoded_v)\n",
    "output2 = grey_scale_decoder(inputs2)\n",
    "print(output1.size())\n",
    "plt.imshow(output1[0].permute(1, 2, 0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "capsule_vector = tf.ones(shape=[2, 43, 16, 1])\n",
    "capsule_vector = tf.reshape(capsule_vector, shape=[2, -1])\n",
    "fc1 = tf.keras.layers.Dense(400)(capsule_vector)\n",
    "fc1 = tf.reshape(fc1, shape=(batch_size, 5, 5, 16))\n",
    "upsample1 = tf.image.resize(fc1, [8, 8])\n",
    "\n",
    "print('11111',tf.shape(upsample1))\n",
    "conv1 = tf.keras.layers.Conv2D(4, kernel_size=(3,3), activation=tf.nn.relu)(upsample1)\n",
    "print('22222',tf.shape(conv1))\n",
    "\n",
    "upsample2 = tf.image.resize(conv1, (16, 16))\n",
    "conv2 = tf.keras.layers.Conv2D(8, (3,3), padding='same', activation=tf.nn.relu)(upsample2)\n",
    "\n",
    "upsample3 = tf.image.resize(conv2, (32, 32))\n",
    "conv6 = tf.keras.layers.Conv2D(16, (3,3), padding='same', activation=tf.nn.relu)(upsample3)\n",
    "\n",
    "# 3 channel for RGG\n",
    "logits = tf.keras.layers.Conv2D( 3, (3,3), padding='same', activation=None)(conv6)\n",
    "decoded = tf.nn.sigmoid(logits, name='decoded')\n",
    "print(tf.shape(decoded))\n",
    "\n",
    "plt.imshow(decoded[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo (based on German dataset but matrix sizes are chosen on purpose to match MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_conv = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=13, stride=1) #kernel = 9, stride = 1 for 10x10\n",
    "primary_caps =  nn.ModuleList(nn.Conv2d(256, 8, 9, 2) for _ in range(32))#kernel = 5 or 6, stride = 2 for 10x10\n",
    "\n",
    "def squash(vector, axis=-1, epsilon=1e-7, squash=True):\n",
    "        \"\"\"\n",
    "        normalize the length of the vector by 1\n",
    "        `vector`: the muliplication of coupling coefs and prediction vectors sum [ c(ij)u^(j|i) ]\n",
    "        `axis`: the axis that would not be reduced\n",
    "        'epsilon`: a workaround to prevent devision by zero\n",
    "        \"\"\"\n",
    "        squaredNorm = torch.sum(torch.square(vector), dim=axis, \n",
    "                                keepdim=True)\n",
    "        safeNorm = torch.sqrt(squaredNorm + epsilon)\n",
    "        \n",
    "        if squash:\n",
    "                squashFactor = squaredNorm / (1. + squaredNorm)\n",
    "                unitVector = vector / safeNorm\n",
    "                return squashFactor * unitVector\n",
    "        else:\n",
    "                return squaredNorm\n",
    "\n",
    "def test_routing(inputs, num_capsules, num_routing_nodes, num_iterations ):\n",
    "        b = nn.Parameter(torch.zeros(num_routing_nodes, num_capsules, 1, 1))\n",
    "        weights = nn.Parameter(torch.rand(num_routing_nodes, num_capsules, 16, 8))\n",
    "\n",
    "        weights = weights[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "        b_ij = b[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "        inputs = inputs.tile(1, 1, num_capsules, 1, 1)\n",
    "  \n",
    "        # u_hat = [batch, num_routing_nodes, # digit_caps, digit_caps_dims, 1]\n",
    "        u_hat = torch.matmul(weights, inputs)\n",
    "    \n",
    "        for i in range(num_iterations):\n",
    "                c_ij = F.softmax(b_ij, dim=2)\n",
    "                v_j = squash((c_ij*u_hat).sum(dim=1, keepdim=True))\n",
    "                \n",
    "                if i < num_iterations - 1 :\n",
    "                        # v_j = [batch, 1 -> num_routing_nodes, num_digit_caps, digit_caps_dims, 1 )]\n",
    "                        b_ij +=  (b_ij + torch.transpose(u_hat, 3, 4) @ v_j.tile(1, num_routing_nodes, 1, 1, 1))\n",
    "\n",
    "        return v_j \n",
    "\n",
    "def main():\n",
    "        a = torch.Tensor(train_images[0]) # RGB images\n",
    "        b = torch.Tensor(train_images[1])\n",
    "        c = torch.stack((a,b), dim=0) # create a 2 batchs with 1 image in each batch\n",
    "\n",
    "        num_categories = 10\n",
    "        conv_output = demo_conv(c.permute(0, 3, 1, 2)) # Notice: input is in form [batch, channel, height, width]\n",
    "        # ouputs -> list: len(list) = num_caps, outputs elements -> tensor: size (1,8,10,10)\n",
    "        caps_output = [\n",
    "                (cap(conv_output))[:, None, :, :, :].permute(0, 1, 3, 4, 2) for cap in primary_caps]\n",
    "        output = torch.cat(caps_output, dim=1)\n",
    "        output = output.view(output.size(0), -1, output.size(4))\n",
    "        output = squash(output)[:, :, None, :, None]\n",
    "        # print(conv_output.size(), output.size())\n",
    "\n",
    "        #routing\n",
    "        v_j = test_routing(output, 10, 1152, 3).squeeze(1)\n",
    "        v_j_norm = squash(v_j, axis=-2, squash=False)\n",
    "        v_softmax = F.softmax(v_j_norm, dim=1)\n",
    "        # if y is None:\n",
    "        v_active, idx = torch.max(v_softmax, dim=1)\n",
    "        y = torch.eye(10).index_select(dim=0, index = idx.squeeze())\n",
    "        \n",
    "        masked_v = (y[:, :, None, None] * v_j).view(batch_size, -1)\n",
    "\n",
    "        # print(masked_v.size())\n",
    "        print(idx.size())\n",
    "        assert list(v_j.size()) == [c.size()[0], 10, 16, 1]\n",
    "   \n",
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Miscellaneous</strong>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super() examples\n",
    "use super to access the characteristics of other classes\n",
    "Ex:\n",
    "super().__init__(mammalName) is equivalent to Class1.__init_(self, mammalName)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animal(object):\n",
    "  def __init__(self, Animal):\n",
    "    print(Animal, 'is an animal.')\n",
    "\n",
    "class Mammal(Animal):\n",
    "  def __init__(self, mammalName):\n",
    "    print(mammalName, 'is a warm-blooded animal.')\n",
    "    super().__init__(mammalName)\n",
    "\n",
    "class NonMarineMammal(Mammal):\n",
    "  def __init__(self, NonMarineMammal):\n",
    "    print(NonMarineMammal, \"can't swim.\")\n",
    "    super().__init__(NonMarineMammal)\n",
    "   \n",
    "class NonWingedMammal(Mammal):\n",
    "  def __init__(self, NonWingedMammal):\n",
    "    print(NonWingedMammal, \"can't fly.\")\n",
    "    super().__init__(NonWingedMammal)\n",
    "\n",
    "class Dog(NonMarineMammal, NonWingedMammal):\n",
    "  def __init__(self):\n",
    "    print('Dog has 4 legs.')\n",
    "    super().__init__('Dog')\n",
    "    \n",
    "d = Dog()\n",
    "print(d)\n",
    "# bat = NonMarineMammal('Bat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parent:\n",
    "  def __init__(self, *txt):\n",
    "    # *args is not a must have input\n",
    "    self.message = txt\n",
    "\n",
    "  def printmessage(self):\n",
    "    print(self.message)\n",
    "\n",
    "  @staticmethod\n",
    "  def printmessage2(text):\n",
    "    print(text)\n",
    "\n",
    "\n",
    "class Child(Parent):\n",
    "  def __init__(self, txt: str, num):\n",
    "    self.num = num\n",
    "    super(Child, self).__init__()\n",
    "    self.printmessage()\n",
    "  \n",
    "  def call_static(self, msg):\n",
    "    self.printmessage2(msg)\n",
    "\n",
    "x = Child(\"Hello, and welcome!\", 2)\n",
    "\n",
    "x.printmessage2(\"hi static\") # another way to call Parent's method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rectangle(object):\n",
    "    def __init__(self, length, width):\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "\n",
    "    def area(self):\n",
    "        return self.length * self.length\n",
    "\n",
    "    def perimeter(self):\n",
    "        return 2 * self.length + 2 * self.width\n",
    "\n",
    "# Here we declare that the Square class inherits from the Rectangle class\n",
    "class Square(Rectangle):\n",
    "    def __init__(self, length_sqr):\n",
    "        super().__init__(length_sqr, length_sqr)   # length_sqr = length and width of class Rectangle\n",
    "Square(5).area()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIL module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = np.matrix('250 60 143; 90 100 40; 120 150 200')\n",
    "im = Image.fromarray(a) # create a n image object as arrays\n",
    "plt.imshow(im)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Employee:\n",
    "    def __init__(self, name, base_pay):\n",
    "        self.name = name\n",
    "        self.base_pay = base_pay\n",
    "\n",
    "    def get_pay(self):\n",
    "        return self.base_pay\n",
    "\n",
    "\n",
    "class SalesEmployee(Employee):\n",
    "    def __init__(self, name, base_pay, sales_incentive):\n",
    "        self.name = name\n",
    "        self.base_pay = base_pay\n",
    "        self.sales_incentive = sales_incentive\n",
    "\n",
    "    def get_pay(self):\n",
    "        return self.base_pay + self.sales_incentive\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    john = SalesEmployee('John', 5000, 1500)\n",
    "    print(john.get_pay())\n",
    "\n",
    "    jane = Employee('Jane', 5000)\n",
    "    print(jane.get_pay())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_caps = 3\n",
    "input_dims = 3\n",
    "output_caps = 2\n",
    "output_dims = 2\n",
    "a = torch.Tensor(input_caps, input_dims , output_caps * output_dims)\n",
    "b = torch.Tensor([ [[10], [30]], [[50], [70]] ])\n",
    "print(b.shape)\n",
    "plt.imshow(b, cmap =plt.cm.binary)\n",
    "print(a, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! 4D requires diff inputs compared to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2,2,3)\n",
    "b = torch.randn(2,2,2,3)\n",
    "print(a)\n",
    "print('---------------')\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to perform 2D convolution operation on an image\n",
    "# Import the required libraries\n",
    "\n",
    "'''input of size [N,C,H, W]\n",
    "N==>batch size,\n",
    "C==> number of channels,\n",
    "H==> height of input planes in pixels,\n",
    "W==> width in pixels.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Read input image\n",
    "img = Image.open('dogncat.jpg')\n",
    "\n",
    "# convert the input image to torch tensor\n",
    "img = T.ToTensor()(img)\n",
    "print(\"Input image size:\", img.size()) # size = [3, 466, 700]\n",
    "\n",
    "# unsqueeze the image to make it 4D tensor\n",
    "img = img.unsqueeze(0) # image size = [1, 3, 466, 700]\n",
    "# define convolution layer\n",
    "# conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "conv = torch.nn.Conv2d(3, 3, 2)\n",
    "\n",
    "# apply convolution operation on image\n",
    "img = conv(img)\n",
    "print(img.size())\n",
    "plt.imshow(img[0,:,:,:].detach().numpy())\n",
    "\n",
    "\n",
    "# squeeze image to make it 3D\n",
    "img = img.squeeze(0) #now size is again [3, 466, 700]\n",
    "# convert image to PIL image\n",
    "img = T.ToPILImage()(img)\n",
    "\n",
    "# display the image after convolution\n",
    "img.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary layer unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_Tensor(size):\n",
    "    return torch.rand(size, size, size)\n",
    "a = [c for c in random_Tensor(3) ]\n",
    "print(a)\n",
    "a = torch.cat(a)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
