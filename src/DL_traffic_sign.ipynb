{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil, pickle, os, random, matplotlib\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "\n",
    "# data logging\n",
    "import configparser\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Main</strong>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "The data is converted into RGB tensors with skimage.io.imread()\n",
    "\n",
    "<strong>Notice:</strong> pip install scikit-image "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingReporter:\n",
    "    def __init__(self, file_name) -> None:\n",
    "        self.file_name = file_name\n",
    "        self.cols = [\"Batch ID\",  \"Accuracy\", \"Margin Loss\", \"Reconstruction Loss\", \"Total Loss\"]\n",
    "        self.epoch, self.accuracy, self.marg_loss, self.recons_loss, self.total_loss  = [], [], [], [], []\n",
    "        \n",
    "        self.__write_rp()\n",
    "\n",
    "    def __write_rp(self):\n",
    "        df = pd.DataFrame(list(\n",
    "            zip(self.epoch, self.accuracy, self.marg_loss, self.recons_loss,self.total_loss)), \n",
    "            columns=self.cols) \n",
    "\n",
    "    def record(self, data: dict) -> None: \n",
    "        self.epoch.append(data['Batch ID'])\n",
    "        self.accuracy.append(data['Accuracy'])\n",
    "        self.marg_loss.append(data['Loss']['Margin'])\n",
    "        self.recons_loss.append(data['Loss']['Recon'])\n",
    "        self.total_loss.append(data['Loss']['Total'])\n",
    "\n",
    "    def test_dict(self):\n",
    "        train_dict = {}\n",
    "        a = [1,2,3,4,5]\n",
    "        for entry in self.cols:\n",
    "            print(entry)\n",
    "            train_dict.update({entry: a})\n",
    "        print(train_dict)\n",
    "a = TrainingReporter(\"hello.csv\")\n",
    "a.test_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],\n",
    "                   index=['row 1', 'row 2'],\n",
    "                   columns=['col 1', 'col 2'])\n",
    "df1.to_excel(\"output.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Pickling data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data(file, writeColumns=None):\n",
    "    \"\"\"\n",
    "    Read/Write pickle training/testing data, models to avoid\n",
    "    loading data again (time consuming)\n",
    "    \n",
    "    ---Params---\n",
    "\n",
    "    file: path to pickle file\n",
    "\n",
    "    writeColumns (array): variables to be saved to pickle file\n",
    "\n",
    "    \"\"\"\n",
    "    if writeColumns is None:\n",
    "        with open(file, mode=\"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "            return tuple(map(lambda col: dataset[col], ['images', 'labels'])) # lambda(col) where columns are the inputs\n",
    "    else:\n",
    "        with open(file, mode=\"wb\") as f:\n",
    "            dataset = pickle.dump({\"images\": writeColumns[0], \"labels\": writeColumns[1]}, f)\n",
    "            print(\"Data is saved in\", file)\n",
    "# lambda function: https://www.youtube.com/watch?v=BcbVe1r2CYc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Label the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_test(src, csv_file, labeled_test_dir, NoOfCategories):\n",
    "    \"\"\"\n",
    "    This function creates named folders corresponding to 43 categories\n",
    "    and move the test images to these folders\n",
    "\n",
    "    `csv_file` and `labeled_test_dir` should have already been in src directly \n",
    "    (create a blank folder to store labeled images)\n",
    "\n",
    "    \"\"\"\n",
    "    # Remove the existing folders in the labeled test directory if there is any\n",
    "    for filename in os.listdir(labeled_test_dir):\n",
    "        file_path = os.path.join(labeled_test_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "    csv_dir = os.path.join(src, csv_file)\n",
    "    SubTestDir = [os.path.join(labeled_test_dir, str(d)) for d in range(NoOfCategories)]\n",
    "    \n",
    "    # Create label folders\n",
    "    [os.mkdir(test_d) for test_d in SubTestDir]\n",
    "\n",
    "    testImageDir = pd.read_csv(csv_dir)['Path']\n",
    "    testImageLabel = pd.read_csv(csv_dir)['ClassId']\n",
    "    for idx in range(len(testImageLabel)):\n",
    "        label = testImageLabel[idx]\n",
    "        shutil.copy(os.path.join(src, testImageDir[idx]), SubTestDir[label])\n",
    "\n",
    "labeled_test_dir = \"D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\German\\LabeledTest\"\n",
    "# labeled_test_dir = r\"C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\German\\LabeleTest\" # BH   \n",
    "src = \"D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\German\" \n",
    "# src = r\"C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\German\" # BH\n",
    "csv_file = \"Test.csv\"\n",
    "NoOfCats = 43\n",
    "\n",
    "label_test(src, csv_file, labeled_test_dir, NoOfCats)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\"Loads a data set and returns two lists:\n",
    "    \n",
    "    images: a list of Numpy arrays, each representing an image.\n",
    "    labels: a list of numbers that represent the images labels.\n",
    "    \"\"\"\n",
    "    # Get all subdirectories of data_dir. Each represents a label.\n",
    "    directories = [d for d in os.listdir(data_dir) \n",
    "                   if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    # Loop through the label directories and collect the data in\n",
    "    # two lists, labels and images.\n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in directories:\n",
    "        # label_dir contains 61 catefories paths\n",
    "        label_dir = os.path.join(data_dir, d)\n",
    "\n",
    "        # list subdirectories within each of the 61 categories\n",
    "        file_names = [os.path.join(label_dir, f) \n",
    "                      for f in os.listdir(label_dir) if f.endswith(\".png\")]\n",
    "        # For each label, load it's images and add them to the images list.\n",
    "        # And add the label number (i.e. directory name) to the labels list.\n",
    "        for f in file_names:\n",
    "            images.append(skimage.io.imread(f))\n",
    "            labels.append(int(d))\n",
    "    return images, labels\n",
    "\n",
    "# ROOT_PATH = \"D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\German\"\n",
    "ROOT_PATH = r\"C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\German\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last execution 10:01PM 20/5/23\n",
    "!!!: resized images are already normalized to [0,1] format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset.\n",
    "train_data_dir = os.path.join(ROOT_PATH, \"Train\")\n",
    "first_train_images, first_train_labels = load_data(train_data_dir)\n",
    "pickle_data(file = \"primary_train_dataset\", writeColumns = [first_train_images, first_train_labels] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply constant resolution among train images and pickle them \n",
    "train_images, train_labels  = pickle_data(file = 'primary_train_dataset')\n",
    "\n",
    "train_images = [ skimage.transform.resize(train_image, (32, 32), mode = \"constant\") \n",
    "                            for train_image in train_images ]\n",
    "\n",
    "train_images = np.stack(train_images, axis = 0)\n",
    "\n",
    "pickle_data(file = \"primary32_train_dataset\", writeColumns = [train_images, train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_data_dir = os.path.join(ROOT_PATH, \"LabeledTest\")\n",
    "first_test_images, first_test_labels = load_data(test_data_dir)\n",
    "test_images, val_images, test_labels, val_labels = train_test_split(first_test_images, first_test_labels, \n",
    "                                                                        test_size=0.36, random_state=0)\n",
    "pickle_data(file = \"primary_test_dataset\", writeColumns = [test_images, test_labels])\n",
    "pickle_data(file = \"primary_val_dataset\", writeColumns = [val_images, val_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply constant resolution among test,val images and pickle them \n",
    "test_images, test_labels  = pickle_data(file = 'primary_test_dataset')\n",
    "val_images, val_labels  = pickle_data(file = 'primary_val_dataset')\n",
    "\n",
    "test_images = [ skimage.transform.resize(test_image, (32, 32), mode = \"constant\") \n",
    "                            for test_image in test_images ]\n",
    "val_images = [ skimage.transform.resize(val_image, (32, 32), mode = \"constant\") \n",
    "                            for val_image in val_images ]\n",
    "\n",
    "test_images = np.stack(test_images, axis = 0)\n",
    "val_images = np.stack(val_images, axis = 0)\n",
    "\n",
    "pickle_data(file = \"primary32_test_dataset\", writeColumns = [test_images, test_labels])\n",
    "pickle_data(file = \"primary32_val_dataset\", writeColumns = [val_images, val_labels])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program starts here if pickle folders are not updated\n",
    "train_images, train_labels  = pickle_data(file = './Data/primary32_train_dataset')\n",
    "test_images, test_labels  = pickle_data(file = './Data/primary32_test_dataset')\n",
    "val_images, val_labels  = pickle_data(file = './Data/primary32_val_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Display images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images, labels, category=False, greyScale=False):\n",
    "    \"\"\"\n",
    "    Display the first image of each label.\n",
    "    \n",
    "    category: set to True when only images within a category are displayed\n",
    "    greyScale: set to True to display images in grey scale\n",
    "    \"\"\"\n",
    "    if category:\n",
    "        i = 1\n",
    "        startIndex = labels.index(category)\n",
    "        catImages = images[startIndex:(startIndex + labels.count(category))] #catImage = categoryImage\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        for catImage in catImages[:24]:\n",
    "            plt.subplot(8, 8, i)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.imshow(catImage)\n",
    "            i += 1 \n",
    "    else:\n",
    "        unique_labels = set(labels) # Create a list contains only the labels (non-iterative)\n",
    "    \n",
    "        #Example: a = [1, 1, 1, 2, 2, 3]\n",
    "        #set(a) >> {1,2,3}\n",
    "\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        i = 1\n",
    "        for label in unique_labels:\n",
    "            image = images[labels.index(label)] # Pick the first image for each label.\n",
    "\n",
    "        # object.index(element) returns the index of the element specified when it's first encountered\n",
    "        # Example: a = [1, 1, 1, 3, 2, 2, 3]\n",
    "        # a.index(3) = 3\n",
    "\n",
    "            plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.title(\"Label {0} ({1})\".format(label, labels.count(label))) # sign category and the # of its samples\n",
    "            if greyScale is False:\n",
    "                plt.imshow(image)\n",
    "            else:\n",
    "                plt.imshow(image, cmap=plt.cm.binary)\n",
    "            i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Data Augmentation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "n_classes = len(set(train_labels))\n",
    "values, bins, patches = ax.hist(train_labels, n_classes)\n",
    "ax.set_xlabel(\"Classes\")\n",
    "ax.set_ylabel(\"Number of images\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "X_train = train_images\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "inference_datagen = ImageDataGenerator()\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "train_datagen.fit(X_train)\n",
    "train_datagen_augmented.fit(X_train)\n",
    "fig = plt.figure()\n",
    "n = 0\n",
    "graph_size = 3\n",
    "\n",
    "for x_batch, y_batch in train_datagen_augmented.flow(X_train, train_labels, batch_size=4):\n",
    "    a=fig.add_subplot(graph_size, graph_size, n+1)\n",
    "    greyBatch = preprocess_images(x_batch)\n",
    "    print(greyBatch[0].shape)\n",
    "    imgplot = plt.imshow(greyBatch[0])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Label:{}\".format(y_batch[0]))\n",
    "    n = n + 1\n",
    "    if n > 8:\n",
    "        break\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capsule layer attributes\n",
    "TODO:\n",
    "- build a simple conv layer: DONE (first conv layer with relu)\n",
    "- build capsule's functions (squash, routing)\n",
    "- build primary and secondary capsnet\n",
    "- fully connected layer for reconstruction\n",
    "- loss functions\n",
    "- add visualizations to track the changes in prior logits, training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torchvision.transforms as T\n",
    "\n",
    "a = torch.Tensor(train_images[0]) # RGB images\n",
    "b = torch.Tensor(train_images[1])\n",
    "c = torch.stack((a,b), dim=0).permute(0, 3, 1, 2) # create a 2 batchs with 1 image in each batch\n",
    "\n",
    "class ImgAug:\n",
    "    def __init__(self, batch_img, value):\n",
    "        \"\"\"\n",
    "        Augment images by batch\n",
    "        :param batch_img: [batch, C, H, W]\n",
    "        :param value: the degree at which the imgs are transformed \n",
    "                      (recommended 0.3 -> 9 pixels)\n",
    "\n",
    "        :return: transformed batch [batch, C, H, W]\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_img = batch_img\n",
    "        self.value = value\n",
    "        aug_method = random.randint(0, 2)\n",
    "        aug_dict = {\n",
    "            '0': self._horizontal_shift(),\n",
    "            '1': self._vertical_shift(),\n",
    "            '2': self._rotate()\n",
    "        }\n",
    "        \n",
    "        augmented_batch = aug_dict[str(aug_method)]\n",
    "        self.resized_img = self._fill(augmented_batch)\n",
    " \n",
    "    def apply_trans(self):\n",
    "        return self.resized_img\n",
    "       \n",
    "    @staticmethod\n",
    "    def _fill(img):\n",
    "        return T.Resize(size=(32,32), antialias=True)(img)\n",
    "\n",
    "    def _horizontal_shift(self):\n",
    "        ratio = random.uniform(-self.value, self.value)\n",
    "        shift_by = int(ratio*(self.batch_img.size()[-1]))\n",
    "        if shift_by < 0: # shift to the right\n",
    "            shifted_batch_img = self.batch_img[:, :, :, :shift_by]\n",
    "        elif shift_by > 0: # shift to the left\n",
    "         \n",
    "            shifted_batch_img = self.batch_img[:, :, :, shift_by:]\n",
    "        else:\n",
    "            shifted_batch_img = self.batch_img\n",
    "        \n",
    "        return shifted_batch_img\n",
    "\n",
    "    def _vertical_shift(self):\n",
    "        ratio = random.uniform(-self.value, self.value)\n",
    "        shift_by = int(ratio*(self.batch_img.size()[-2]))\n",
    "        if shift_by < 0: # shift to the upward\n",
    "            shifted_batch_img = self.batch_img[:, :, :shift_by, :]\n",
    "        elif shift_by > 0: # shift to the downward\n",
    "            shifted_batch_img = self.batch_img[:, :, shift_by:, :]\n",
    "          \n",
    "        else:\n",
    "            shifted_batch_img = self.batch_img\n",
    "\n",
    "        return shifted_batch_img\n",
    "    \n",
    "    def _rotate(self):\n",
    "  \n",
    "        return T.RandomRotation(degrees=45)(self.batch_img)\n",
    "\n",
    "# shifted_batch = ImgAug(c, 0.3)\n",
    "# print(shifted_batch.apply_trans().size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu will be used\n",
      "check point\n",
      "check point\n",
      "check point\n",
      "check point\n",
      "check point\n",
      "check point\n",
      "check point\n"
     ]
    }
   ],
   "source": [
    "# Currently this network only supports grey scale imgage due to fully connected reconstruction layer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "logging.basicConfig(filename='caps_net.log', filemode='w', format='%(asctime)s: %(message)s', level=logging.DEBUG)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\n",
    "    r'D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\src\\capsnet_config.ini'\n",
    "    )\n",
    "\n",
    "# General info\n",
    "num_class = int(config['network']['num_class'])\n",
    "img_channels = int(config['network']['image_channels'])\n",
    "batch_size = int(config['network']['batch_size'])\n",
    "learning_rate = float(config['network']['lr'])\n",
    "epochs = int(config['network']['epochs'])\n",
    "\n",
    "train_num = torch.tensor(train_images).size()[0]\n",
    "valid_num = torch.tensor(val_images).size()[0]\n",
    "test_num = torch.tensor(test_images).size()[0]\n",
    "\n",
    "# Primary caps\n",
    "primary_num_caps = int(config['primary_caps']['num_caps'])\n",
    "primary_channels = int(config['primary_caps']['channels'])\n",
    "\n",
    "# Digit caps\n",
    "digit_num_caps = num_class\n",
    "digit_channels = int(config['digit_caps']['channels'])\n",
    "num_iterations = int(config['network']['num_routing_iter'])\n",
    "\n",
    "# Loss hyper params\n",
    "m_plus = float(config['loss']['m_plus'])\n",
    "m_minus = float(config['loss']['m_minus'])\n",
    "lmbd = float(config['loss']['lambda'])\n",
    "regularization_factor = float(config['loss']['regularization_factor'])\n",
    "\n",
    "\n",
    "def squash(vector, axis=-1 ,epsilon=1e-7, squash=True):\n",
    "        \"\"\"\n",
    "        normalize the length of the vector by 1\n",
    "        `vector`: the muliplication of coupling coefs and prediction vectors sum [ c(ij)u^(j|i) ]\n",
    "        `axis`: the axis that would not be reduced\n",
    "        'epsilon`: a workaround to prevent devision by zero\n",
    "        \"\"\"\n",
    "        squared_norm = torch.sum(torch.square(vector), dim=axis, \n",
    "                                keepdim=True)\n",
    "        safe_norm = torch.sqrt(squared_norm + epsilon)\n",
    "\n",
    "        if squash:\n",
    "            squash_factor = squared_norm / (1. + squared_norm)\n",
    "            unit_vector = vector / safe_norm\n",
    "            return squash_factor * unit_vector\n",
    "        else:\n",
    "            return safe_norm\n",
    "\n",
    "\n",
    "class CapsLayers(nn.Module):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "    :param num_conv: number of filters/convolutional unit per capsule (dimension of a capsule)\n",
    "    :param num_capsules: number of primary/digit caps\n",
    "    :param num_routing_nodes: number of possible u(i), \n",
    "                            set to -1 if it's not a secondary capsule layer\n",
    "    :param in_channels: output convolutional layers of the prev layer\n",
    "    :param out_channels: output convolutional layers of the current layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsules: int, in_channels: int, out_channels: int, \n",
    "                 kernel_size=None, stride=None, num_routing_nodes=None ,num_iterations=None):\n",
    "        super(CapsLayers, self).__init__()\n",
    "        self.num_capsules = num_capsules\n",
    "        \n",
    "        self.num_iterations = num_iterations\n",
    "        self.num_routing_nodes = num_routing_nodes\n",
    "        if num_routing_nodes is not None:\n",
    "            self.weights = nn.Parameter(torch.randn(\n",
    "                                        self.num_routing_nodes, num_capsules, out_channels, in_channels))\n",
    "            self.b = nn.Parameter(torch.zeros(\n",
    "                                    self.num_routing_nodes, num_capsules, 1, 1))\n",
    "        else:\n",
    "            self.primary_caps = nn.ModuleList(nn.Conv2d(in_channels, out_channels, kernel_size, stride) \n",
    "                                                    for _ in range(num_capsules))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Feed foward function for non-reconstruction layer\n",
    "        :param inputs: \n",
    "            for the primary caps, the inputs are convolutional layer pixels\n",
    "            for digit caps, the inputs are n-D vectors from a primary cap\n",
    "                where n is the # of filters for one capsule  \n",
    "            Required Paramteters:\n",
    "            prior_logits(b) \n",
    "            primary layer prediction (requires u-layer 1 ouput, Weights)\n",
    "        \"\"\"\n",
    "        if self.num_routing_nodes is not None:\n",
    "            weights = self.weights[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "            b_ij = self.b[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "            inputs = inputs.tile(1, 1, self.num_capsules, 1, 1)\n",
    "            \n",
    "            # u_hat = [batch, num_routing_nodes, # digit_caps, digit_caps_dims, 1]\n",
    "            u_hat = weights @ inputs \n",
    "\n",
    "            for i in range(self.num_iterations):\n",
    "                c_ij = F.softmax(b_ij, dim=2)\n",
    "                outputs = squash((c_ij*u_hat).sum(dim=1, keepdim=True))\n",
    "\n",
    "                if i < self.num_iterations - 1 :\n",
    "                    # v_j OR outputs = [batch, 1 -> num_routing_nodes, num_digit_caps, digit_caps_dims, 1 )]\n",
    "                    b_ij +=  (b_ij + torch.transpose(u_hat, 3, 4) @ outputs.tile(1, self.num_routing_nodes, 1, 1, 1))\n",
    "\n",
    "        else:\n",
    "            outputs = [\n",
    "                capsule(inputs)[:, None, :, :, :].permute(0, 1, 3, 4, 2) for capsule in self.primary_caps]\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "            # u(i) = [batch, num_prim_caps*prim_caps_2D_size, prim_caps_output_dimension]\n",
    "            outputs = outputs.view(outputs.size(0), -1, outputs.size(4)) \n",
    "            outputs = squash(outputs)[:, :, None, :, None]\n",
    "        \n",
    "        # outputs = [batch, 1, num_digit_caps/num_class, digit_caps_dims, 1 )]\n",
    "        return outputs            \n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    \"\"\"\n",
    "        This class contains the full CapsNet architecture:\n",
    "        Convolutional -> primary capsules -> digit capsules -> (3) fully connected\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Params: \n",
    "        `inputs`: a 4D tensor (grey scale or RGB)\n",
    "        \"\"\"\n",
    "        super(CapsNet, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(img_channels, 256, \n",
    "                                kernel_size=9, stride=1)\n",
    "        self.primary_caps = CapsLayers(primary_num_caps, 256, primary_channels, \n",
    "                                        kernel_size=5, stride=2)\n",
    "        self.digit_caps = CapsLayers(digit_num_caps, primary_channels, digit_channels, \n",
    "                                     num_routing_nodes=10*10*primary_num_caps, num_iterations=num_iterations)\n",
    "        self.grey_scale_decoder = nn.Sequential(\n",
    "                nn.Linear(digit_channels*digit_num_caps, 576),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(576, 1600),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1600, 1024),\n",
    "                nn.Sigmoid(),\n",
    "        )\n",
    "        self.linear_trans = nn.Linear(digit_channels*digit_num_caps, 400)\n",
    "        self.RGB_decoder = nn.Sequential(\n",
    "                nn.Upsample(size=(8, 8)),\n",
    "                nn.Conv2d(16, 4, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(4, 8, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(8, 16, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv2d(16, 3, 3, padding='same'),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, images, labels=None): # labels should be applied `one_hot` function\n",
    "        conv_1_ouputs = F.relu(self.conv_1(images))\n",
    "        primary_caps_outputs =  self.primary_caps(conv_1_ouputs) \n",
    "        digit_caps_outputs = self.digit_caps(primary_caps_outputs).squeeze(1)\n",
    "        \n",
    "        assert list(digit_caps_outputs.size()) == [images.size()[0], num_class, digit_channels, 1]\n",
    "               \n",
    "        v_norm = squash(digit_caps_outputs, axis=-2, squash=False)\n",
    "        v_prob = F.softmax(v_norm, dim=1)\n",
    "\n",
    "        self.img = images\n",
    "        self.v_norm = v_norm\n",
    "\n",
    "        idx = torch.zeros(images.size()[0], 1, 1)\n",
    "        # Masking\n",
    "        if labels is None: # Testing mode\n",
    "            _, idx = torch.max(v_prob, dim=1)\n",
    "            labels = torch.eye(num_class).index_select(dim=0, index = idx.squeeze())\n",
    "\n",
    "        #masked_v = [batch_size, digit_channels*classes])\n",
    "        masked_v = (labels[:, :, None, None] * digit_caps_outputs).view(images.size(0), -1)\n",
    "        \n",
    "        # Reconstruction\n",
    "        if images.size()[1] == 1:\n",
    "            reconstructed_img = self.grey_scale_decoder(masked_v).view(batch_size, 32, 32) # [batch, 32x32]\n",
    "        else:\n",
    "  \n",
    "            fc_out = self.linear_trans(masked_v).view(batch_size, 16, 5 ,5) #masked_v\n",
    "            reconstructed_img = self.RGB_decoder(fc_out) # [batch, channel, height, width]\n",
    "    \n",
    "            logging.debug(f'Capsule layer pred / batch: {idx}')\n",
    "\n",
    "        return idx, reconstructed_img \n",
    "        \n",
    "    def loss_fn(self, reconstructed_img, labels):\n",
    "        # Margin loss\n",
    "        max_1 = F.relu(m_plus - self.v_norm)\n",
    "        max_2 = F.relu(self.v_norm - m_minus)\n",
    "        T_k = labels[:, :, None, None]\n",
    "        \n",
    "        L_k = T_k * torch.square(max_1) + lmbd * (1 - T_k) * torch.square(max_2)\n",
    "\n",
    "        assert L_k.size() == self.v_norm.size()\n",
    "        margin_loss = L_k.sum(dim=1).mean()\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss_obj = nn.MSELoss()\n",
    "\n",
    "        # original_img = [batch size, flatten image (pixels are flatten into arrays)]\n",
    "        original_img = self.img.contiguous().view(batch_size, -1)\n",
    "        reconstructed_img = reconstructed_img.view(batch_size, -1)\n",
    "\n",
    "        reconstruction_loss = reconstruction_loss_obj(reconstructed_img, original_img)\n",
    "        total_loss = margin_loss + regularization_factor * reconstruction_loss\n",
    "\n",
    "        return margin_loss, reconstruction_loss, total_loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    # plot_batch = plt.figure()\n",
    "    \n",
    "    model.train() # set the model to training mode\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch = x_batch.permute(0, 3, 1, 2)\n",
    "        y_batch = torch.nn.functional.one_hot(y_batch.to(torch.int64), num_classes=43)\n",
    "\n",
    "        aug_bool = random.randint(0,1)\n",
    "        if aug_bool:\n",
    "            augmentor = ImgAug(x_batch, 0.3)\n",
    "            x_batch = augmentor.apply_trans()\n",
    "        if device == 'cuda':\n",
    "            x_batch, y_batch = x_batch.to('cuda'), y_batch.to('cuda')\n",
    "\n",
    "        _, recons_img = model(x_batch, y_batch) # recons_img = [batch, C, H, W]\n",
    "        margin_loss, recons_loss, total_loss = model.loss_fn(recons_img, y_batch)\n",
    "        total_loss.backward()\n",
    "        optimizer.step() # update the params to be optimized (weights, biases, routing weights)\n",
    "\n",
    "        # if batch_idx % (batch_size * 2) == 0:\n",
    "        #     print(f'Batch {batch_idx+1}/{train_num//batch_size}')\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Can be used for both test and validation\n",
    "    \"\"\"\n",
    "    # set the model to testing mode\n",
    "    model.eval() \n",
    "    for idx, (x_batch, y_batch) in enumerate(test_loader):\n",
    "        if device == 'cuda':\n",
    "            x_batch, y_batch = x_batch.to('cuda'), y_batch.to('cuda')\n",
    "        \n",
    "        image = torch.Tensor(\n",
    "            preprocess_images(image[None, :, :, :]).permute(0, 3, 1, 2),\n",
    "            dtype=torch.float32)\n",
    "        \n",
    "        pred_idx, recons_img = model(image)\n",
    "        \n",
    "def data_loader(images: np.array, \n",
    "                labels: np.array, \n",
    "                batch_size: int,\n",
    "                shuffle: bool) -> object:\n",
    "    \"\"\"\n",
    "    :param images: 3D array [H, W, Channels]\n",
    "    :param labels: 1D array\n",
    "    :param batch_size: number of images per batch\n",
    "\n",
    "    :return: an iterable object\n",
    "    \"\"\"\n",
    "    images_tensor = torch.Tensor(images)\n",
    "    labels_tensor = torch.Tensor(labels)\n",
    "\n",
    "    dataset = TensorDataset(images_tensor, labels_tensor)\n",
    "    data_loader = DataLoader(dataset, batch_size, shuffle=shuffle)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = CapsNet()\n",
    "    if torch.cuda.is_available():\n",
    "        device_as_str = 'cuda'\n",
    "        model = CapsNet().to('cuda') \n",
    "    else:\n",
    "        device_as_str = 'cpu'\n",
    "    print(f'{device_as_str} will be used')\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, min_lr=1e-5)\n",
    "\n",
    "    # Prepare dataset\n",
    "    val_loader = data_loader(val_images, val_labels, batch_size, shuffle=False)\n",
    "    train_loader = data_loader(train_images, train_labels, batch_size, shuffle=True)\n",
    "    \n",
    "    # for idx, (img, label) in enumerate(train_loader):\n",
    "    #     print(img.size())\n",
    "    #     if idx == 5:\n",
    "    #         break\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        train(model, train_loader, optimizer, device_as_str) \n",
    "        # val_loss = test(model, val_loader, device_as_str)\n",
    "        # scheduler.step(val_loss)\n",
    "\n",
    "    # should put next_batch into train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (i,l) in enumerate(train_loader):\n",
    "    a = T.Grayscale()(i.permute(0, 2, 3, 1))\n",
    "    print(a.size())\n",
    "    if idx > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from skimage import exposure\n",
    "\n",
    "def preprocess_images(images):\n",
    "    \"\"\"\n",
    "        - Convert RGB images to grey scale \n",
    "        - Normalize pixels to 0-1,\n",
    "        - Improve the contrast with adaptive histogram localization\n",
    "    \"\"\"\n",
    "     \n",
    "    # Conver RGB -> grey scale\n",
    "    images = 0.299 * images[:, :, :, 0] + 0.587 * images[:, :, :, 1] + 0.114 * images[:, :, :, 2]\n",
    "    # Improve the contrast\n",
    "    images = exposure.equalize_adapthist(images)\n",
    "\n",
    "    # Add ONE 3-D channel for grey scale\n",
    "    images = images.reshape(images.shape + (1,)) \n",
    "\n",
    "    return images\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\n",
    "    r'C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\src\\capsnet_config.ini'\n",
    "    )\n",
    "\n",
    "# General info\n",
    "num_class = int(config['network']['num_class'])\n",
    "img_channels = int(config['network']['image_channels'])\n",
    "batch_size = int(config['network']['batch_size'])\n",
    "learning_rate = float(config['network']['lr'])\n",
    "epochs = int(config['network']['epochs'])\n",
    "\n",
    "train_num = torch.tensor(train_images).size()[0]\n",
    "valid_num = torch.tensor(val_images).size()[0]\n",
    "test_num = torch.tensor(test_images).size()[0]\n",
    "\n",
    "# Primary caps\n",
    "primary_num_caps = int(config['primary_caps']['num_caps'])\n",
    "primary_channels = int(config['primary_caps']['channels'])\n",
    "\n",
    "# Digit caps\n",
    "digit_num_caps = num_class\n",
    "digit_channels = int(config['digit_caps']['channels'])\n",
    "num_iterations = int(config['network']['num_routing_iter'])\n",
    "\n",
    "# Loss hyper params\n",
    "m_plus = float(config['loss']['m_plus'])\n",
    "m_minus = float(config['loss']['m_minus'])\n",
    "lmbd = float(config['loss']['lambda'])\n",
    "regularization_factor = float(config['loss']['regularization_factor'])\n",
    "\n",
    "\n",
    "shuffeled_train_images, shuffeled_train_labels = shuffle(train_images, train_labels, random_state=0)\n",
    "demo_train_images = shuffeled_train_images[0:100]\n",
    "demo_train_labels = shuffeled_train_labels[0:100]\n",
    "X_train = demo_train_images\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "inference_datagen = ImageDataGenerator()\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "def squash(vector, axis=-1 ,epsilon=1e-7, squash=True):\n",
    "        \"\"\"\n",
    "        normalize the length of the vector by 1\n",
    "        `vector`: the muliplication of coupling coefs and prediction vectors sum [ c(ij)u^(j|i) ]\n",
    "        `axis`: the axis that would not be reduced\n",
    "        'epsilon`: a workaround to prevent devision by zero\n",
    "        \"\"\"\n",
    "        squared_norm = torch.sum(torch.square(vector), dim=axis, \n",
    "                                keepdim=True)\n",
    "        safe_norm = torch.sqrt(squared_norm + epsilon)\n",
    "\n",
    "        if squash:\n",
    "            squash_factor = squared_norm / (1. + squared_norm)\n",
    "            unit_vector = vector / safe_norm\n",
    "            return squash_factor * unit_vector\n",
    "        else:\n",
    "            return safe_norm\n",
    "\n",
    "\n",
    "class CapsLayers(nn.Module):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "    :param num_conv: number of filters/convolutional unit per capsule (dimension of a capsule)\n",
    "    :param num_capsules: number of primary/digit caps\n",
    "    :param num_routing_nodes: number of possible u(i), \n",
    "                            set to -1 if it's not a secondary capsule layer\n",
    "    :param in_channels: output convolutional layers of the prev layer\n",
    "    :param out_channels: output convolutional layers of the current layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsules: int, in_channels: int, out_channels: int, \n",
    "                 kernel_size=None, stride=None, num_routing_nodes=None ,num_iterations=None):\n",
    "        super(CapsLayers, self).__init__()\n",
    "        self.num_capsules = num_capsules\n",
    "        \n",
    "        self.num_iterations = num_iterations\n",
    "        self.num_routing_nodes = num_routing_nodes\n",
    "        if num_routing_nodes is not None:\n",
    "            self.weights = nn.Parameter(torch.randn(\n",
    "                                        self.num_routing_nodes, num_capsules, out_channels, in_channels))\n",
    "            self.b = nn.Parameter(torch.zeros(\n",
    "                                    self.num_routing_nodes, num_capsules, 1, 1))\n",
    "        else:\n",
    "            self.primary_caps = nn.ModuleList(nn.Conv2d(in_channels, out_channels, kernel_size, stride) \n",
    "                                                    for _ in range(num_capsules))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Feed foward function for non-reconstruction layer\n",
    "        :param inputs: \n",
    "            for the primary caps, the inputs are convolutional layer pixels\n",
    "            for digit caps, the inputs are n-D vectors from a primary cap\n",
    "                where n is the # of filters for one capsule  \n",
    "            Required Paramteters:\n",
    "            prior_logits(b) \n",
    "            primary layer prediction (requires u-layer 1 ouput, Weights)\n",
    "        \"\"\"\n",
    "        if self.num_routing_nodes is not None:\n",
    "            weights = self.weights[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "            b_ij = self.b[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "            inputs = inputs.tile(1, 1, self.num_capsules, 1, 1)\n",
    "            \n",
    "            # u_hat = [batch, num_routing_nodes, # digit_caps, digit_caps_dims, 1]\n",
    "            u_hat = weights @ inputs \n",
    "\n",
    "            for i in range(self.num_iterations):\n",
    "                c_ij = F.softmax(b_ij, dim=2)\n",
    "                outputs = squash((c_ij*u_hat).sum(dim=1, keepdim=True))\n",
    "\n",
    "                if i < self.num_iterations - 1 :\n",
    "                    # v_j OR outputs = [batch, 1 -> num_routing_nodes, num_digit_caps, digit_caps_dims, 1 )]\n",
    "                    b_ij +=  (b_ij + torch.transpose(u_hat, 3, 4) @ outputs.tile(1, self.num_routing_nodes, 1, 1, 1))\n",
    "\n",
    "        else:\n",
    "            outputs = [\n",
    "                capsule(inputs)[:, None, :, :, :].permute(0, 1, 3, 4, 2) for capsule in self.primary_caps]\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "            # u(i) = [batch, num_prim_caps*prim_caps_2D_size, prim_caps_output_dimension]\n",
    "            outputs = outputs.view(outputs.size(0), -1, outputs.size(4)) \n",
    "            outputs = squash(outputs)[:, :, None, :, None]\n",
    "        \n",
    "        # outputs = [batch, 1, num_digit_caps/num_class, digit_caps_dims, 1 )]\n",
    "        return outputs            \n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    \"\"\"\n",
    "        This class contains the full CapsNet architecture:\n",
    "        Convolutional -> primary capsules -> digit capsules -> (3) fully connected\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Params: \n",
    "        `inputs`: a 4D tensor (grey scale or RGB)\n",
    "        \"\"\"\n",
    "        super(CapsNet, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(img_channels, 256, \n",
    "                                kernel_size=9, stride=1)\n",
    "        self.primary_caps = CapsLayers(primary_num_caps, 256, primary_channels, \n",
    "                                        kernel_size=5, stride=2)\n",
    "        self.digit_caps = CapsLayers(digit_num_caps, primary_channels, digit_channels, \n",
    "                                     num_routing_nodes=10*10*primary_num_caps, num_iterations=num_iterations)\n",
    "        self.grey_scale_decoder = nn.Sequential(\n",
    "                nn.Linear(digit_channels*digit_num_caps, 576),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(576, 1600),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1600, 1024),\n",
    "                nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.RGB_decoder = nn.Sequential(\n",
    "                nn.Upsample(size=(8, 8)),\n",
    "                nn.Conv2d(16, 4, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(4, 8, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(8, 16, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv2d(16, 3, 3, padding='same'),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, images, labels=None): # labels should be applied `one_hot` function\n",
    "        conv_1_ouputs = F.relu(self.conv_1(images))\n",
    "        primary_caps_outputs =  self.primary_caps(conv_1_ouputs) #TODO: check again the size of conv1ouputs to \n",
    "        # verify capsules takes the correct size (since conv outputs has size [b, v, h, w])\n",
    "        digit_caps_outputs = self.digit_caps(primary_caps_outputs).squeeze(1)\n",
    "        \n",
    "        assert list(digit_caps_outputs.size()) == [images.size()[0], num_class, digit_channels, 1]\n",
    "               \n",
    "        v_norm = squash(digit_caps_outputs, axis=-2, squash=False)\n",
    "        v_prob = F.softmax(v_norm, dim=1)\n",
    "\n",
    "        self.img = images\n",
    "        self.v_norm = v_norm\n",
    "\n",
    "        idx = torch.zeros(images.size()[0], 1, 1)\n",
    "        # Masking\n",
    "        if labels is None: # Testing mode\n",
    "            _, idx = torch.max(v_prob, dim=1)\n",
    "            labels = torch.eye(num_class).index_select(dim=0, index = idx.squeeze())\n",
    "\n",
    "        masked_v = (labels[:, :, None, None] * digit_caps_outputs).view(images.size(0), -1)\n",
    "\n",
    "        # Reconstruction\n",
    "        if images.size()[1] == 1:\n",
    "            reconstructed_img = self.grey_scale_decoder(masked_v) # [batch, 32x32]\n",
    "\n",
    "        else:\n",
    "            linear_trans = nn.Linear(digit_channels*digit_num_caps, 400)\n",
    "            fc_out = linear_trans(masked_v)\n",
    "            reconstructed_img = self.RGB_decoder(fc_out) # [batch, channel, height, width]\n",
    "\n",
    "        return idx, reconstructed_img \n",
    "        \n",
    "\n",
    "    def loss_fn(self, reconstructed_img, labels):\n",
    "        # Margin loss\n",
    "        max_1 = F.relu(m_plus - self.v_norm)\n",
    "        max_2 = F.relu(self.v_norm - m_minus)\n",
    "        T_k = labels[:, :, None, None]\n",
    "        \n",
    "        L_k = T_k * torch.square(max_1) + lmbd * (1 - T_k) * torch.square(max_2)\n",
    "\n",
    "        assert L_k.size() == self.v_norm.size()\n",
    "        margin_loss = L_k.sum(dim=1).mean()\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss_obj = nn.MSELoss()\n",
    "\n",
    "        # original_img = [batch size, flatten image (pixels are flatten into arrays)]\n",
    "        original_img = self.img.view(batch_size, -1)\n",
    "        reconstruction_loss = reconstruction_loss_obj(reconstructed_img, original_img)\n",
    "        \n",
    "        total_loss = margin_loss + regularization_factor * reconstruction_loss\n",
    "\n",
    "        return margin_loss, reconstruction_loss, total_loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    # plot_batch = plt.figure()\n",
    "    n = 0\n",
    "    model.train() # set the model to training mode\n",
    "    for batch_idx in range(train_num//batch_size):\n",
    "        # plot_batch.add_subplot(10, 10, n+1)\n",
    "        batch = next(train_loader)\n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "        # plt.imshow(x_batch[0])\n",
    "        y_batch = torch.tensor(y_batch)\n",
    "        y_batch = torch.nn.functional.one_hot(y_batch.to(torch.int64), num_classes=43)\n",
    "\n",
    "        x_batch = torch.tensor(preprocess_images(x_batch))\n",
    "        x_batch = x_batch.permute(0, 3, 1, 2)# [batch_size, channels, height, width]\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            x_batch, y_batch = x_batch.cuda(), y_batch.cuda() \n",
    "\n",
    "        _, recons_img = model(x_batch, y_batch)\n",
    "        margin_loss, recons_loss, total_loss = model.loss_fn(recons_img, y_batch)\n",
    "        total_loss.backward()\n",
    "        optimizer.step() # update the params to be optimized (weights, biases, routing weights)\n",
    "     \n",
    "        # if batch_idx % (batch_size * 2) == 0:\n",
    "        #     print(f'Batch {batch_idx+1}/{len(train_loader)*batch_size}')\n",
    "        print(batch_idx)\n",
    "        if n > 100:\n",
    "            break\n",
    "        n += 1 \n",
    "\n",
    "def test(model, test_imgs, test_labels , device):\n",
    "    \"\"\"\n",
    "    Can be used for both test and validation\n",
    "    \"\"\"\n",
    "    test_imgs = torch.tensor(test_imgs)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "\n",
    "    if device == 'cuda':\n",
    "        test_imgs = test_imgs.cuda()\n",
    "        test_labels = test_labels.cuda()\n",
    "\n",
    "    model.eval()\n",
    "    for idx, image in enumerate(test_imgs):\n",
    "        image = image[None, :, :, :]\n",
    "        pred_idx, recons_img = model(image)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device_as_str = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device_as_str, \"will be used\")\n",
    "    device_as_str = 'cpu'\n",
    "    model = CapsNet()#.cuda()\n",
    "  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, min_lr=1e-5)\n",
    "\n",
    "    train_loader = train_datagen_augmented.flow(X_train, demo_train_labels, batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        train(model, train_loader, optimizer, device_as_str) \n",
    "        val_loss = test(model, val_images, val_labels)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # should put next_batch into train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_channels = 16\n",
    "digit_num_caps = 43\n",
    "\n",
    "grey_scale_decoder = nn.Sequential(\n",
    "                nn.Linear(digit_channels*digit_num_caps, 576, device='cuda'),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(576, 1600, device='cuda'),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1600, 1024, device='cuda'),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "RGB_decoder = nn.Sequential(\n",
    "            nn.Upsample(size=(8, 8)),\n",
    "            nn.Conv2d(16, 4, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(4, 8, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(8, 16, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16, 3, 3, padding='same'),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "inputs = torch.ones(2, 43, 16, 1).view(2, -1)\n",
    "decoder = nn.Linear(digit_channels*digit_num_caps, 400)\n",
    "decoded_v = decoder(inputs).view(2, digit_channels, 5, 5) #torch.Size([2, 16, 8, 8])\n",
    "\n",
    "inputs2 = torch.ones(2, 43, 16, 1, device = 'cuda').view(2, -1)\n",
    "\n",
    "output1 = RGB_decoder(decoded_v)\n",
    "output2 = grey_scale_decoder(inputs2)\n",
    "print(output1.size())\n",
    "plt.imshow(output1[0].permute(1, 2, 0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "capsule_vector = tf.ones(shape=[2, 43, 16, 1])\n",
    "capsule_vector = tf.reshape(capsule_vector, shape=[2, -1])\n",
    "fc1 = tf.keras.layers.Dense(400)(capsule_vector)\n",
    "fc1 = tf.reshape(fc1, shape=(batch_size, 5, 5, 16))\n",
    "upsample1 = tf.image.resize(fc1, [8, 8])\n",
    "\n",
    "print('11111',tf.shape(upsample1))\n",
    "conv1 = tf.keras.layers.Conv2D(4, kernel_size=(3,3), activation=tf.nn.relu)(upsample1)\n",
    "print('22222',tf.shape(conv1))\n",
    "\n",
    "upsample2 = tf.image.resize(conv1, (16, 16))\n",
    "conv2 = tf.keras.layers.Conv2D(8, (3,3), padding='same', activation=tf.nn.relu)(upsample2)\n",
    "\n",
    "upsample3 = tf.image.resize(conv2, (32, 32))\n",
    "conv6 = tf.keras.layers.Conv2D(16, (3,3), padding='same', activation=tf.nn.relu)(upsample3)\n",
    "\n",
    "# 3 channel for RGG\n",
    "logits = tf.keras.layers.Conv2D( 3, (3,3), padding='same', activation=None)(conv6)\n",
    "decoded = tf.nn.sigmoid(logits, name='decoded')\n",
    "print(tf.shape(decoded))\n",
    "\n",
    "plt.imshow(decoded[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo (based on German dataset but matrix sizes are chosen on purpose to match MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_conv = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=13, stride=1) #kernel = 9, stride = 1 for 10x10\n",
    "primary_caps =  nn.ModuleList(nn.Conv2d(256, 8, 9, 2) for _ in range(32))#kernel = 5 or 6, stride = 2 for 10x10\n",
    "\n",
    "def squash(vector, axis=-1, epsilon=1e-7, squash=True):\n",
    "        \"\"\"\n",
    "        normalize the length of the vector by 1\n",
    "        `vector`: the muliplication of coupling coefs and prediction vectors sum [ c(ij)u^(j|i) ]\n",
    "        `axis`: the axis that would not be reduced\n",
    "        'epsilon`: a workaround to prevent devision by zero\n",
    "        \"\"\"\n",
    "        squaredNorm = torch.sum(torch.square(vector), dim=axis, \n",
    "                                keepdim=True)\n",
    "        safeNorm = torch.sqrt(squaredNorm + epsilon)\n",
    "        \n",
    "        if squash:\n",
    "                squashFactor = squaredNorm / (1. + squaredNorm)\n",
    "                unitVector = vector / safeNorm\n",
    "                return squashFactor * unitVector\n",
    "        else:\n",
    "                return squaredNorm\n",
    "\n",
    "def test_routing(inputs, num_capsules, num_routing_nodes, num_iterations ):\n",
    "        b = nn.Parameter(torch.zeros(num_routing_nodes, num_capsules, 1, 1))\n",
    "        weights = nn.Parameter(torch.rand(num_routing_nodes, num_capsules, 16, 8))\n",
    "\n",
    "        weights = weights[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "        b_ij = b[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "        inputs = inputs.tile(1, 1, num_capsules, 1, 1)\n",
    "  \n",
    "        # u_hat = [batch, num_routing_nodes, # digit_caps, digit_caps_dims, 1]\n",
    "        u_hat = torch.matmul(weights, inputs)\n",
    "    \n",
    "        for i in range(num_iterations):\n",
    "                c_ij = F.softmax(b_ij, dim=2)\n",
    "                v_j = squash((c_ij*u_hat).sum(dim=1, keepdim=True))\n",
    "                \n",
    "                if i < num_iterations - 1 :\n",
    "                        # v_j = [batch, 1 -> num_routing_nodes, num_digit_caps, digit_caps_dims, 1 )]\n",
    "                        b_ij +=  (b_ij + torch.transpose(u_hat, 3, 4) @ v_j.tile(1, num_routing_nodes, 1, 1, 1))\n",
    "\n",
    "        return v_j \n",
    "\n",
    "def main():\n",
    "        a = torch.Tensor(train_images[0]) # RGB images\n",
    "        b = torch.Tensor(train_images[1])\n",
    "        c = torch.stack((a,b), dim=0) # create a 2 batchs with 1 image in each batch\n",
    "\n",
    "        num_categories = 10\n",
    "        conv_output = demo_conv(c.permute(0, 3, 1, 2)) # Notice: input is in form [batch, channel, height, width]\n",
    "        # ouputs -> list: len(list) = num_caps, outputs elements -> tensor: size (1,8,10,10)\n",
    "        caps_output = [\n",
    "                (cap(conv_output))[:, None, :, :, :].permute(0, 1, 3, 4, 2) for cap in primary_caps]\n",
    "        output = torch.cat(caps_output, dim=1)\n",
    "        output = output.view(output.size(0), -1, output.size(4))\n",
    "        output = squash(output)[:, :, None, :, None]\n",
    "        # print(conv_output.size(), output.size())\n",
    "\n",
    "        #routing\n",
    "        v_j = test_routing(output, 10, 1152, 3).squeeze(1)\n",
    "        v_j_norm = squash(v_j, axis=-2, squash=False)\n",
    "        v_softmax = F.softmax(v_j_norm, dim=1)\n",
    "        # if y is None:\n",
    "        v_active, idx = torch.max(v_softmax, dim=1)\n",
    "        y = torch.eye(10).index_select(dim=0, index = idx.squeeze())\n",
    "        \n",
    "        masked_v = (y[:, :, None, None] * v_j).view(batch_size, -1)\n",
    "\n",
    "        # print(masked_v.size())\n",
    "        print(idx.size())\n",
    "        assert list(v_j.size()) == [c.size()[0], 10, 16, 1]\n",
    "   \n",
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Miscellaneous</strong>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super() examples\n",
    "use super to access the characteristics of other classes\n",
    "Ex:\n",
    "super().__init__(mammalName) is equivalent to Class1.__init_(self, mammalName)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animal(object):\n",
    "  def __init__(self, Animal):\n",
    "    print(Animal, 'is an animal.')\n",
    "\n",
    "class Mammal(Animal):\n",
    "  def __init__(self, mammalName):\n",
    "    print(mammalName, 'is a warm-blooded animal.')\n",
    "    super().__init__(mammalName)\n",
    "\n",
    "class NonMarineMammal(Mammal):\n",
    "  def __init__(self, NonMarineMammal):\n",
    "    print(NonMarineMammal, \"can't swim.\")\n",
    "    super().__init__(NonMarineMammal)\n",
    "   \n",
    "class NonWingedMammal(Mammal):\n",
    "  def __init__(self, NonWingedMammal):\n",
    "    print(NonWingedMammal, \"can't fly.\")\n",
    "    super().__init__(NonWingedMammal)\n",
    "\n",
    "class Dog(NonMarineMammal, NonWingedMammal):\n",
    "  def __init__(self):\n",
    "    print('Dog has 4 legs.')\n",
    "    super().__init__('Dog')\n",
    "    \n",
    "d = Dog()\n",
    "print(d)\n",
    "# bat = NonMarineMammal('Bat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parent:\n",
    "  def __init__(self, *txt):\n",
    "    # *args is not a must have input\n",
    "    self.message = txt\n",
    "\n",
    "  def printmessage(self):\n",
    "    print(self.message)\n",
    "\n",
    "  @staticmethod\n",
    "  def printmessage2(text):\n",
    "    print(text)\n",
    "\n",
    "\n",
    "class Child(Parent):\n",
    "  def __init__(self, txt: str, num):\n",
    "    self.num = num\n",
    "    super(Child, self).__init__()\n",
    "    self.printmessage()\n",
    "  \n",
    "  def call_static(self, msg):\n",
    "    self.printmessage2(msg)\n",
    "\n",
    "x = Child(\"Hello, and welcome!\", 2)\n",
    "\n",
    "x.printmessage2(\"hi static\") # another way to call Parent's method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rectangle(object):\n",
    "    def __init__(self, length, width):\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "\n",
    "    def area(self):\n",
    "        return self.length * self.length\n",
    "\n",
    "    def perimeter(self):\n",
    "        return 2 * self.length + 2 * self.width\n",
    "\n",
    "# Here we declare that the Square class inherits from the Rectangle class\n",
    "class Square(Rectangle):\n",
    "    def __init__(self, length_sqr):\n",
    "        super().__init__(length_sqr, length_sqr)   # length_sqr = length and width of class Rectangle\n",
    "Square(5).area()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIL module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = np.matrix('250 60 143; 90 100 40; 120 150 200')\n",
    "im = Image.fromarray(a) # create a n image object as arrays\n",
    "plt.imshow(im)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Employee:\n",
    "    def __init__(self, name, base_pay):\n",
    "        self.name = name\n",
    "        self.base_pay = base_pay\n",
    "\n",
    "    def get_pay(self):\n",
    "        return self.base_pay\n",
    "\n",
    "\n",
    "class SalesEmployee(Employee):\n",
    "    def __init__(self, name, base_pay, sales_incentive):\n",
    "        self.name = name\n",
    "        self.base_pay = base_pay\n",
    "        self.sales_incentive = sales_incentive\n",
    "\n",
    "    def get_pay(self):\n",
    "        return self.base_pay + self.sales_incentive\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    john = SalesEmployee('John', 5000, 1500)\n",
    "    print(john.get_pay())\n",
    "\n",
    "    jane = Employee('Jane', 5000)\n",
    "    print(jane.get_pay())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_caps = 3\n",
    "input_dims = 3\n",
    "output_caps = 2\n",
    "output_dims = 2\n",
    "a = torch.Tensor(input_caps, input_dims , output_caps * output_dims)\n",
    "b = torch.Tensor([ [[10], [30]], [[50], [70]] ])\n",
    "print(b.shape)\n",
    "plt.imshow(b, cmap =plt.cm.binary)\n",
    "print(a, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! 4D requires diff inputs compared to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2,2,3)\n",
    "b = torch.randn(2,2,2,3)\n",
    "print(a)\n",
    "print('---------------')\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to perform 2D convolution operation on an image\n",
    "# Import the required libraries\n",
    "\n",
    "'''input of size [N,C,H, W]\n",
    "N==>batch size,\n",
    "C==> number of channels,\n",
    "H==> height of input planes in pixels,\n",
    "W==> width in pixels.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Read input image\n",
    "img = Image.open('dogncat.jpg')\n",
    "\n",
    "# convert the input image to torch tensor\n",
    "img = T.ToTensor()(img)\n",
    "print(\"Input image size:\", img.size()) # size = [3, 466, 700]\n",
    "\n",
    "# unsqueeze the image to make it 4D tensor\n",
    "img = img.unsqueeze(0) # image size = [1, 3, 466, 700]\n",
    "# define convolution layer\n",
    "# conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "conv = torch.nn.Conv2d(3, 3, 2)\n",
    "\n",
    "# apply convolution operation on image\n",
    "img = conv(img)\n",
    "print(img.size())\n",
    "plt.imshow(img[0,:,:,:].detach().numpy())\n",
    "\n",
    "\n",
    "# squeeze image to make it 3D\n",
    "img = img.squeeze(0) #now size is again [3, 466, 700]\n",
    "# convert image to PIL image\n",
    "img = T.ToPILImage()(img)\n",
    "\n",
    "# display the image after convolution\n",
    "img.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary layer unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_Tensor(size):\n",
    "    return torch.rand(size, size, size)\n",
    "a = [c for c in random_Tensor(3) ]\n",
    "print(a)\n",
    "a = torch.cat(a)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
