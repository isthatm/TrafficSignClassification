{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shutil, pickle, os, random, matplotlib\n",
    "from PIL import Image, ImageEnhance\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import configparser\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Main</strong>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "The data is converted into RGB tensors with skimage.io.imread()\n",
    "\n",
    "<strong>Notice:</strong> pip install scikit-image "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Pickling data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data(file, writeColumns=None):\n",
    "    \"\"\"\n",
    "    Read/Write pickle training/testing data, models to avoid\n",
    "    loading data again (time consuming)\n",
    "    \n",
    "    ---Params---\n",
    "\n",
    "    file: path to pickle file\n",
    "\n",
    "    writeColumns (array): variables to be saved to pickle file\n",
    "\n",
    "    \"\"\"\n",
    "    if writeColumns is None:\n",
    "        with open(file, mode=\"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "            return tuple(map(lambda col: dataset[col], ['images', 'labels'])) # lambda(col) where columns are the inputs\n",
    "    else:\n",
    "        with open(file, mode=\"wb\") as f:\n",
    "            dataset = pickle.dump({\"images\": writeColumns[0], \"labels\": writeColumns[1]}, f)\n",
    "            print(\"Data is saved in\", file)\n",
    "# lambda function: https://www.youtube.com/watch?v=BcbVe1r2CYc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Label the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_test(src, csv_file, labeled_test_dir, NoOfCategories):\n",
    "    \"\"\"\n",
    "    This function creates named folders corresponding to 43 categories\n",
    "    and move the test images to these folders\n",
    "\n",
    "    `csv_file` and `labeled_test_dir` should have already been in src directly \n",
    "    (create a blank folder to store labeled images)\n",
    "\n",
    "    \"\"\"\n",
    "    # Remove the existing folders in the labeled test directory if there is any\n",
    "    for filename in os.listdir(labeled_test_dir):\n",
    "        file_path = os.path.join(labeled_test_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "    csv_dir = os.path.join(src, csv_file)\n",
    "    SubTestDir = [os.path.join(labeled_test_dir, str(d)) for d in range(NoOfCategories)]\n",
    "    \n",
    "    # Create label folders\n",
    "    [os.mkdir(test_d) for test_d in SubTestDir]\n",
    "\n",
    "    testImageDir = pd.read_csv(csv_dir)['Path']\n",
    "    testImageLabel = pd.read_csv(csv_dir)['ClassId']\n",
    "    for idx in range(len(testImageLabel)):\n",
    "        label = testImageLabel[idx]\n",
    "        shutil.copy(os.path.join(src, testImageDir[idx]), SubTestDir[label])\n",
    "\n",
    "# labeled_test_dir = \"D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\German\\LabeledTest\"\n",
    "labeled_test_dir = r\"C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\German\\Labeled_test\" # BH   \n",
    "# src = \"D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\German\" # BH\n",
    "src = r\"C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\German\"\n",
    "csv_file = \"Test.csv\"\n",
    "NoOfCats = 43\n",
    "\n",
    "label_test(src, csv_file, labeled_test_dir, NoOfCats)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\"Loads a data set and returns two lists:\n",
    "    \n",
    "    images: a list of Numpy arrays, each representing an image.\n",
    "    labels: a list of numbers that represent the images labels.\n",
    "    \"\"\"\n",
    "    # Get all subdirectories of data_dir. Each represents a label.\n",
    "    directories = [d for d in os.listdir(data_dir) \n",
    "                   if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    # Loop through the label directories and collect the data in\n",
    "    # two lists, labels and images.\n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in directories:\n",
    "        # label_dir contains 61 catefories paths\n",
    "        label_dir = os.path.join(data_dir, d)\n",
    "\n",
    "        # list subdirectories within each of the 61 categories\n",
    "        file_names = [os.path.join(label_dir, f) \n",
    "                      for f in os.listdir(label_dir) if f.endswith(\".png\")]\n",
    "        # For each label, load it's images and add them to the images list.\n",
    "        # And add the label number (i.e. directory name) to the labels list.\n",
    "        for f in file_names:\n",
    "            images.append(skimage.io.imread(f))\n",
    "            labels.append(int(d))\n",
    "    return images, labels\n",
    "\n",
    "# ROOT_PATH = \"D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\German\"\n",
    "ROOT_PATH = r\"C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\German\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last execution 10:01PM 20/5/23\n",
    "!!!: resized images are already normalized to [0,1] format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset.\n",
    "train_data_dir = os.path.join(ROOT_PATH, \"Train\")\n",
    "first_train_images, first_train_labels = load_data(train_data_dir)\n",
    "pickle_data(file = \"primary_train_dataset\", writeColumns = [first_train_images, first_train_labels] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply constant resolution among train images and pickle them \n",
    "train_images, train_labels  = pickle_data(file = 'primary_train_dataset')\n",
    "\n",
    "train_images = [ skimage.transform.resize(train_image, (32, 32), mode = \"constant\") \n",
    "                            for train_image in train_images ]\n",
    "\n",
    "train_images = np.stack(train_images, axis = 0)\n",
    "\n",
    "pickle_data(file = \"primary32_train_dataset\", writeColumns = [train_images, train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_data_dir = os.path.join(ROOT_PATH, \"Labeled_test\")\n",
    "first_test_images, first_test_labels = load_data(test_data_dir)\n",
    "test_images, val_images, test_labels, val_labels = train_test_split(first_test_images, first_test_labels, \n",
    "                                                                        test_size=0.36, random_state=0)\n",
    "pickle_data(file = \"primary_test_dataset\", writeColumns = [test_images, test_labels])\n",
    "pickle_data(file = \"primary_val_dataset\", writeColumns = [val_images, val_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply constant resolution among test,val images and pickle them \n",
    "test_images, test_labels  = pickle_data(file = 'primary_test_dataset')\n",
    "val_images, val_labels  = pickle_data(file = 'primary_val_dataset')\n",
    "\n",
    "test_images = [ skimage.transform.resize(test_image, (32, 32), mode = \"constant\") \n",
    "                            for test_image in test_images ]\n",
    "val_images = [ skimage.transform.resize(val_image, (32, 32), mode = \"constant\") \n",
    "                            for val_image in val_images ]\n",
    "\n",
    "test_images = np.stack(test_images, axis = 0)\n",
    "val_images = np.stack(val_images, axis = 0)\n",
    "\n",
    "pickle_data(file = \"primary32_test_dataset\", writeColumns = [test_images, test_labels])\n",
    "pickle_data(file = \"primary32_val_dataset\", writeColumns = [val_images, val_labels])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program starts here if pickle folders are not updated\n",
    "train_images, train_labels  = pickle_data(file = './Data/primary32_train_dataset')\n",
    "test_images, test_labels  = pickle_data(file = './Data/primary32_test_dataset')\n",
    "val_images, val_labels  = pickle_data(file = './Data/primary32_val_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Display images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images, labels, category=False, greyScale=False):\n",
    "    \"\"\"\n",
    "    Display the first image of each label.\n",
    "    \n",
    "    category: set to True when only images within a category are displayed\n",
    "    greyScale: set to True to display images in grey scale\n",
    "    \"\"\"\n",
    "\n",
    "    if category:\n",
    "        i = 1\n",
    "        startIndex = labels.index(category)\n",
    "        catImages = images[startIndex:(startIndex + labels.count(category))] #catImage = categoryImage\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        for catImage in catImages[:24]:\n",
    "            plt.subplot(8, 8, i)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.imshow(catImage)\n",
    "            i += 1 \n",
    "    else:\n",
    "        unique_labels = set(labels) # Create a list contains only the labels (non-iterative)\n",
    "    \n",
    "        #Example: a = [1, 1, 1, 2, 2, 3]\n",
    "        #set(a) >> {1,2,3}\n",
    "\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        i = 1\n",
    "        for label in unique_labels:\n",
    "            image = images[labels.index(label)] # Pick the first image for each label.\n",
    "\n",
    "        # object.index(element) returns the index of the element specified when it's first encountered\n",
    "        # Example: a = [1, 1, 1, 3, 2, 2, 3]\n",
    "        # a.index(3) = 3\n",
    "\n",
    "            plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.title(\"Label {0} ({1})\".format(label, labels.count(label))) # sign category and the # of its samples\n",
    "            if greyScale is False:\n",
    "                plt.imshow(image)\n",
    "            else:\n",
    "                plt.imshow(image, cmap=plt.cm.binary)\n",
    "            i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Convert to grey scale, improve contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "\n",
    "def preprocess_images(images):\n",
    "    \"\"\"\n",
    "        - Convert RGB images to grey scale \n",
    "        - Normalize pixels to 0-1,\n",
    "        - Improve the contrast with adaptive histogram localization\n",
    "    \"\"\"\n",
    "     \n",
    "    # Conver RGB -> grey scale\n",
    "    images = 0.299 * images[:, :, :, 0] + 0.587 * images[:, :, :, 1] + 0.114 * images[:, :, :, 2]\n",
    "    # Improve the contrast\n",
    "    images = exposure.equalize_adapthist(images)\n",
    "\n",
    "    # Add ONE 3-D channel for grey scale\n",
    "    images = images.reshape(images.shape + (1,)) \n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_images[0]\n",
    "# print(a.shape)\n",
    "a = 0.299 * a[:, :, 0] + 0.587 * a[:, :, 1] + 0.114 * a[:, :, 2]\n",
    "print(a.shape)\n",
    "a = exposure.equalize_adapthist(a)\n",
    "a = a.reshape(a.shape + (1,)) \n",
    "plt.imshow(a,cmap=plt.cm.binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all images \n",
    "display_images(preprocess_images(train_images, improveCon=True), train_labels, greyScale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all images \n",
    "display_images(preprocess_images(train_images), train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "n_classes = len(set(train_labels))\n",
    "values, bins, patches = ax.hist(train_labels, n_classes)\n",
    "ax.set_xlabel(\"Classes\")\n",
    "ax.set_ylabel(\"Number of images\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "X_train = train_images\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "inference_datagen = ImageDataGenerator()\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "train_datagen.fit(X_train)\n",
    "train_datagen_augmented.fit(X_train)\n",
    "fig = plt.figure()\n",
    "n = 0\n",
    "graph_size = 3\n",
    "\n",
    "for x_batch, y_batch in train_datagen_augmented.flow(X_train, train_labels, batch_size=4):\n",
    "    a=fig.add_subplot(graph_size, graph_size, n+1)\n",
    "    greyBatch = preprocess_images(x_batch)\n",
    "    print(greyBatch[0].shape)\n",
    "    imgplot = plt.imshow(greyBatch[0])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Label:{}\".format(y_batch[0]))\n",
    "    n = n + 1\n",
    "    if n > 8:\n",
    "        break\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capsule layer attributes\n",
    "TODO:\n",
    "- build a simple conv layer: DONE (first conv layer with relu)\n",
    "- build capsule's functions (squash, routing)\n",
    "- build primary and secondary capsnet\n",
    "- fully connected layer for reconstruction\n",
    "- loss functions\n",
    "- add visualizations to track the changes in prior logits, training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently this network only supports grey scale imgage due to fully connected reconstruction layer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\n",
    "    'D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\src\\capsnet_config.ini'\n",
    "    )\n",
    "\n",
    "# General info\n",
    "num_class = config['network']['num_class']\n",
    "img_channels = config['network']['image_channels']\n",
    "batch_size = config['network']['batch_size']\n",
    "\n",
    "# Primary caps\n",
    "primary_num_caps = config['primary_caps']['num_caps']\n",
    "primary_channels = config['primary_caps']['channels']\n",
    "\n",
    "# Digit caps\n",
    "digit_num_caps = num_class\n",
    "digit_channels = config['digit_caps']['channels']\n",
    "num_iterations = config['network']['num_routing_iter']\n",
    "\n",
    "# Loss hyper params\n",
    "m_plus = config['loss']['m_plus']\n",
    "m_minus = config['loss']['m_minus']\n",
    "lmbd = config['loss']['lambda']\n",
    "regularization_factor = config['loss']['regularization_factor']\n",
    "\n",
    "shuffeled_train_images, shuffeled_train_labels = shuffle(train_images, train_labels, random_state=0)\n",
    "demo_train_images = shuffeled_train_images[0:1]\n",
    "demo_train_labels = shuffeled_train_labels[0:1]\n",
    "X_train = demo_train_images\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "inference_datagen = ImageDataGenerator()\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "train_datagen.fit(X_train)\n",
    "train_datagen_augmented.fit(X_train)\n",
    "\n",
    "def squash(vector, axis=-1 ,epsilon=1e-7, squash=True):\n",
    "        \"\"\"\n",
    "        normalize the length of the vector by 1\n",
    "        `vector`: the muliplication of coupling coefs and prediction vectors sum [ c(ij)u^(j|i) ]\n",
    "        `axis`: the axis that would not be reduced\n",
    "        'epsilon`: a workaround to prevent devision by zero\n",
    "        \"\"\"\n",
    "        squared_norm = torch.sum(torch.square(vector), dim=axis, \n",
    "                                keepdim=True)\n",
    "        safe_norm = torch.sqrt(squared_norm + epsilon)\n",
    "\n",
    "        if squash:\n",
    "            squash_factor = squared_norm / (1. + squared_norm)\n",
    "            unit_vector = vector / safe_norm\n",
    "            return squash_factor * unit_vector\n",
    "        else:\n",
    "            return safe_norm\n",
    "\n",
    "\n",
    "class capsLayers(nn.Module):\n",
    "\n",
    "    \"\"\" \n",
    "    Args:\n",
    "    :param num_conv: number of filters/convolutional unit per capsule (dimension of a capsule)\n",
    "    :param num_capsules: number of primary/digit caps\n",
    "    :param num_routing_nodes: number of possible u(i), \n",
    "                            set to -1 if it's not a secondary capsule layer\n",
    "    :param in_channels: output convolutional layers of the prev layer\n",
    "    :param out_channels: output convolutional layers of the current layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsules: int, in_channels: int, out_channels: int, \n",
    "                 kernel_size=None, stride=None, num_routing_nodes=None ,num_iterations=None):\n",
    "        super(capsLayers, self).__init__()\n",
    "        self.num_capsules = num_capsules\n",
    "        \n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "        if num_routing_nodes is not None:\n",
    "            self.num_routing_nodes = num_routing_nodes\n",
    "            self.weights = nn.Parameter(torch.randn(\n",
    "                                        num_routing_nodes, num_capsules, out_channels, in_channels))\n",
    "            self.b = nn.Parameter(torch.zeros(\n",
    "                                    num_routing_nodes, num_capsules, 1, 1))\n",
    "\n",
    "        else:\n",
    "            self.primary_caps = nn.ModuleList(nn.Conv2d(in_channels, out_channels, kernel_size, stride) \n",
    "                                                    for _ in range(num_capsules))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Feed foward function for non-reconstruction layer\n",
    "        :param inputs: \n",
    "            for the primary caps, the inputs are convolutional layer pixels\n",
    "            for digit caps, the inputs are n-D vectors from a primary cap\n",
    "                where n is the # of filters for one capsule  \n",
    "            Required Paramteters:\n",
    "            prior_logits(b) \n",
    "            primary layer prediction (requires u-layer 1 ouput, Weights)\n",
    "        \"\"\"\n",
    "        if self.num_routing_nodes is not None:\n",
    "            weights = self.weights[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "            b_ij = self.b[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "            inputs = inputs.tile(1, 1, self.num_capsules, 1, 1)\n",
    "            \n",
    "            # u_hat = [batch, num_routing_nodes, # digit_caps, digit_caps_dims, 1]\n",
    "            u_hat = weights @ inputs \n",
    "\n",
    "            for i in range(self.num_iterations):\n",
    "                c_ij = F.softmax(b_ij, dim=2)\n",
    "                outputs = self.squash((c_ij*u_hat).sum(dim=1, keepdim=True))\n",
    "\n",
    "                if i < self.num_iterations - 1 :\n",
    "                    # v_j OR outputs = [batch, 1 -> num_routing_nodes, num_digit_caps, digit_caps_dims, 1 )]\n",
    "                    b_ij +=  (b_ij + torch.transpose(u_hat, 3, 4) @ outputs.tile(1, self.num_routing_nodes, 1, 1, 1))\n",
    "\n",
    "        else:\n",
    "            outputs = [\n",
    "                capsule(inputs)[:, None, :, :, :].permute(0, 1, 3, 4, 2) for capsule in self.primary_caps]\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "            # u(i) = [batch, num_prim_caps*prim_caps_2D_size, prim_caps_output_dimension]\n",
    "            outputs = outputs.view(outputs.size(0), -1, outputs.size(4)) \n",
    "            outputs = self.squash(outputs)[:, :, None, :, None]\n",
    "        \n",
    "        # outputs = [batch, 1, num_digit_caps/num_class, digit_caps_dims, 1 )]\n",
    "        return outputs            \n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    \"\"\"\n",
    "        This class contains the full CapsNet architecture:\n",
    "        \n",
    "        Convolutional -> primary capsules -> digit capsules -> (3) fully connected\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Params: \n",
    "        `inputs`: a 4D tensor (grey scale or RGB)\n",
    "        \"\"\"\n",
    "        super(CapsNet, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(in_channels=img_channels, out_channels=256, #TODO: !!! `in_channels` depends on grey scale/RGB image\n",
    "                                kernel_size=9, stride=1)\n",
    "        self.primary_caps = capsLayers(primary_num_caps, 256, primary_channels, \n",
    "                                        kernel_size=5, stride=2)\n",
    "        self.digit_caps = capsLayers(digit_num_caps, primary_channels, digit_channels, \n",
    "                                     num_routing_nodes=10*10*primary_num_caps, num_iterations=num_iterations)\n",
    "        self.grey_scale_decoder = nn.Sequential(\n",
    "                    nn.Linear(digit_channels*digit_num_caps, 576, device='cuda'),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(576, 1600, device='cuda'),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(1600, 1024, device='cuda'),\n",
    "                    nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, images, labels=None): # labels should be applied `one_hot` function\n",
    "                                            # before being used in this forward method\n",
    "        conv_1_ouputs = F.relu(self.conv_1(images))\n",
    "        primary_caps_outputs =  self.primary_caps(conv_1_ouputs)\n",
    "        digit_caps_outputs = self.digit_caps(primary_caps_outputs).squeeze(1)\n",
    "        \n",
    "        assert list(digit_caps_outputs.size()) == [images.size()[0], num_class, digit_channels, 1]\n",
    "               \n",
    "        v_norm = squash(digit_caps_outputs, axis=-2, squash=False)\n",
    "        v_prob = F.softmax(v_norm, dim=1)\n",
    "\n",
    "        self.img = images\n",
    "        self.v_norm = v_norm\n",
    "\n",
    "        # Masking\n",
    "        if labels is None:\n",
    "            _, idx = torch.max(v_prob, dim=1)\n",
    "            labels = torch.eye(num_class).index_select(dim=0, index = idx.squeeze())\n",
    "        \n",
    "        masked_v = labels[:, :, None, None] * digit_caps_outputs\n",
    "        \n",
    "        # Reconstruction\n",
    "        if images.size()[-1] == 1:\n",
    "            reconstructed_img = self.grey_scale_decoder(masked_v)\n",
    "\n",
    "        else:\n",
    "            # RGB decoder\n",
    "            pass\n",
    "    \n",
    "        return reconstructed_img \n",
    "        \n",
    "\n",
    "    def total_loss(self, reconstructed_img, labels): # TODO: convert these inputs to self. in def forward()\n",
    "        # Margin loss\n",
    "        max_1 = F.relu(m_plus - self.v_norm)\n",
    "        max_2 = F.relu(self.v_norm - m_minus)\n",
    "        T_k = labels[:, :, None, None]\n",
    "        \n",
    "        L_k = T_k * torch.square(max_1) + lmbd * (1 - T_k) * torch.square(max_2)\n",
    "\n",
    "        assert L_k.size() == self.v_norm.size()\n",
    "        margin_loss = L_k.sum(dim=1).mean()\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss_obj = nn.MSELoss()\n",
    "\n",
    "        # original_img = [batch size, flatten image (pixels are flatten into arrays)]\n",
    "        original_img = self.img.view(batch_size, -1)\n",
    "        reconstruction_loss = reconstruction_loss_obj(reconstructed_img, original_img)\n",
    "        \n",
    "        total_loss = margin_loss + regularization_factor * reconstruction_loss\n",
    "\n",
    "        return total_loss\n",
    "   \n",
    "def main():\n",
    "    fig = plt.figure()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device, \"will be used\")\n",
    "    # model = CapsNet()\n",
    "    n = 0\n",
    "    for x_batch, y_batch in train_datagen_augmented.flow(X_train, demo_train_labels, batch_size=1):\n",
    "        greyBatch = torch.tensor(preprocess_images(x_batch))\n",
    "        # greyBatch = torch.tensor(x_batch)\n",
    "        greyBatch = torch.tensor(greyBatch.permute(0, 3, 1, 2)) # [batch_size, channels, height, width]\n",
    "        # convolved_image = model.forward(greyBatch)\n",
    "        print(greyBatch.size())\n",
    "        # # plt.imshow(convolved_image.data.numpy())\n",
    "        n += 1\n",
    "        if n >2:\n",
    "            break\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(512)\n",
    "b = torch.tensor(40)\n",
    "print(torch.sqrt(a))\n",
    "print(b.pow(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.constant([1.8, 2.2], dtype=tf.float32)\n",
    "print(tf.cast(x, tf.int32))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo (based on German dataset but matrix sizes are chosen on purpose to match MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_conv = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=13, stride=1) #kernel = 9, stride = 1 for 10x10\n",
    "primary_caps =  nn.ModuleList(nn.Conv2d(256, 8, 9, 2) for _ in range(32))#kernel = 5 or 6, stride = 2 for 10x10\n",
    "\n",
    "def squash(vector, axis=-1, epsilon=1e-7, squash=True):\n",
    "        \"\"\"\n",
    "        normalize the length of the vector by 1\n",
    "        `vector`: the muliplication of coupling coefs and prediction vectors sum [ c(ij)u^(j|i) ]\n",
    "        `axis`: the axis that would not be reduced\n",
    "        'epsilon`: a workaround to prevent devision by zero\n",
    "        \"\"\"\n",
    "        squaredNorm = torch.sum(torch.square(vector), dim=axis, \n",
    "                                keepdim=True)\n",
    "        safeNorm = torch.sqrt(squaredNorm + epsilon)\n",
    "        \n",
    "        if squash:\n",
    "                squashFactor = squaredNorm / (1. + squaredNorm)\n",
    "                unitVector = vector / safeNorm\n",
    "                return squashFactor * unitVector\n",
    "        else:\n",
    "                return squaredNorm\n",
    "\n",
    "def test_routing(inputs, num_capsules, num_routing_nodes, num_iterations ):\n",
    "        b = nn.Parameter(torch.zeros(num_routing_nodes, num_capsules, 1, 1))\n",
    "        weights = nn.Parameter(torch.rand(num_routing_nodes, num_capsules, 16, 8))\n",
    "\n",
    "        weights = weights[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "        b_ij = b[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "        inputs = inputs.tile(1, 1, num_capsules, 1, 1)\n",
    "  \n",
    "        # u_hat = [batch, num_routing_nodes, # digit_caps, digit_caps_dims, 1]\n",
    "        u_hat = torch.matmul(weights, inputs)\n",
    "    \n",
    "        for i in range(num_iterations):\n",
    "                c_ij = F.softmax(b_ij, dim=2)\n",
    "                v_j = squash((c_ij*u_hat).sum(dim=1, keepdim=True))\n",
    "                \n",
    "                if i < num_iterations - 1 :\n",
    "                        # v_j = [batch, 1 -> num_routing_nodes, num_digit_caps, digit_caps_dims, 1 )]\n",
    "                        b_ij +=  (b_ij + torch.transpose(u_hat, 3, 4) @ v_j.tile(1, num_routing_nodes, 1, 1, 1))\n",
    "\n",
    "        return v_j \n",
    "\n",
    "def main():\n",
    "        a = torch.Tensor(train_images[0]) # RGB images\n",
    "        b = torch.Tensor(train_images[1])\n",
    "        c = torch.stack((a,b), dim=0) # create a batch of 2 images\n",
    "        d = c.view(c.size(0), -1)\n",
    "        e = torch.ones(d.size())\n",
    "\n",
    "        err = d - e\n",
    "        err_square = torch.square(err)\n",
    "        # print(err_square.size())\n",
    "        print(err_square.sum(dim=1)) #.mean())\n",
    "        \n",
    "        loss1 = nn.MSELoss(reduction='sum')\n",
    "        loss2 = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        print(loss1(d, e), loss2(d, e))\n",
    "\n",
    "        num_categories = 10\n",
    "        conv_output = demo_conv(c.permute(0, 3, 1, 2))\n",
    "        # ouputs -> list: len(list) = num_caps, outputs elements -> tensor: size (1,8,10,10)\n",
    "        caps_output = [\n",
    "                (cap(conv_output))[:, None, :, :, :].permute(0, 1, 3, 4, 2) for cap in primary_caps]\n",
    "        output = torch.cat(caps_output, dim=1)\n",
    "        output = output.view(output.size(0), -1, output.size(4))\n",
    "        output = squash(output)[:, :, None, :, None]\n",
    "        # print(conv_output.size(), output.size())\n",
    "\n",
    "        #routing\n",
    "        v_j = test_routing(output, 10, 1152, 3).squeeze(1)\n",
    "        v_j_norm = squash(v_j, axis=-2, squash=False)\n",
    "        v_softmax = F.softmax(v_j_norm, dim=1)\n",
    "        # if y is None:\n",
    "        v_active, idx = torch.max(v_softmax, dim=1)\n",
    "        y = torch.eye(10).index_select(dim=0, index = idx.squeeze())\n",
    "        \n",
    "        masked_v = y[:, :, None, None] * v_j\n",
    "        # print(masked_v.size())\n",
    "        print(v_j_norm.size())\n",
    "        assert list(v_j.size()) == [c.size()[0], 10, 16, 1]\n",
    "   \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = F.one_hot(torch.tensor(test_labels), 43)\n",
    "print(set(test_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Miscellaneous</strong>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super() examples\n",
    "use super to access the characteristics of other classes\n",
    "Ex:\n",
    "super().__init__(mammalName) is equivalent to Class1.__init_(self, mammalName)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animal(object):\n",
    "  def __init__(self, Animal):\n",
    "    print(Animal, 'is an animal.')\n",
    "\n",
    "class Mammal(Animal):\n",
    "  def __init__(self, mammalName):\n",
    "    print(mammalName, 'is a warm-blooded animal.')\n",
    "    super().__init__(mammalName)\n",
    "\n",
    "class NonMarineMammal(Mammal):\n",
    "  def __init__(self, NonMarineMammal):\n",
    "    print(NonMarineMammal, \"can't swim.\")\n",
    "    super().__init__(NonMarineMammal)\n",
    "   \n",
    "class NonWingedMammal(Mammal):\n",
    "  def __init__(self, NonWingedMammal):\n",
    "    print(NonWingedMammal, \"can't fly.\")\n",
    "    super().__init__(NonWingedMammal)\n",
    "\n",
    "class Dog(NonMarineMammal, NonWingedMammal):\n",
    "  def __init__(self):\n",
    "    print('Dog has 4 legs.')\n",
    "    super().__init__('Dog')\n",
    "    \n",
    "d = Dog()\n",
    "print(d)\n",
    "# bat = NonMarineMammal('Bat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parent:\n",
    "  def __init__(self, *txt):\n",
    "    # *args is not a must have input\n",
    "    self.message = txt\n",
    "\n",
    "  def printmessage(self):\n",
    "    print(self.message)\n",
    "\n",
    "  @staticmethod\n",
    "  def printmessage2(text):\n",
    "    print(text)\n",
    "\n",
    "\n",
    "class Child(Parent):\n",
    "  def __init__(self, txt: str, num):\n",
    "    self.num = num\n",
    "    super(Child, self).__init__()\n",
    "    self.printmessage()\n",
    "  \n",
    "  def call_static(self, msg):\n",
    "    self.printmessage2(msg)\n",
    "\n",
    "x = Child(\"Hello, and welcome!\", 2)\n",
    "\n",
    "x.printmessage2(\"hi static\") # another way to call Parent's method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rectangle(object):\n",
    "    def __init__(self, length, width):\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "\n",
    "    def area(self):\n",
    "        return self.length * self.length\n",
    "\n",
    "    def perimeter(self):\n",
    "        return 2 * self.length + 2 * self.width\n",
    "\n",
    "# Here we declare that the Square class inherits from the Rectangle class\n",
    "class Square(Rectangle):\n",
    "    def __init__(self, length_sqr):\n",
    "        super().__init__(length_sqr, length_sqr)   # length_sqr = length and width of class Rectangle\n",
    "Square(5).area()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIL module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = np.matrix('250 60 143; 90 100 40; 120 150 200')\n",
    "im = Image.fromarray(a) # create a n image object as arrays\n",
    "plt.imshow(im)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Employee:\n",
    "    def __init__(self, name, base_pay):\n",
    "        self.name = name\n",
    "        self.base_pay = base_pay\n",
    "\n",
    "    def get_pay(self):\n",
    "        return self.base_pay\n",
    "\n",
    "\n",
    "class SalesEmployee(Employee):\n",
    "    def __init__(self, name, base_pay, sales_incentive):\n",
    "        self.name = name\n",
    "        self.base_pay = base_pay\n",
    "        self.sales_incentive = sales_incentive\n",
    "\n",
    "    def get_pay(self):\n",
    "        return self.base_pay + self.sales_incentive\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    john = SalesEmployee('John', 5000, 1500)\n",
    "    print(john.get_pay())\n",
    "\n",
    "    jane = Employee('Jane', 5000)\n",
    "    print(jane.get_pay())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_caps = 3\n",
    "input_dims = 3\n",
    "output_caps = 2\n",
    "output_dims = 2\n",
    "a = torch.Tensor(input_caps, input_dims , output_caps * output_dims)\n",
    "b = torch.Tensor([ [[10], [30]], [[50], [70]] ])\n",
    "print(b.shape)\n",
    "plt.imshow(b, cmap =plt.cm.binary)\n",
    "print(a, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! 4D requires diff inputs compared to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2,2,3)\n",
    "b = torch.randn(2,2,2,3)\n",
    "print(a)\n",
    "print('---------------')\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to perform 2D convolution operation on an image\n",
    "# Import the required libraries\n",
    "\n",
    "'''input of size [N,C,H, W]\n",
    "N==>batch size,\n",
    "C==> number of channels,\n",
    "H==> height of input planes in pixels,\n",
    "W==> width in pixels.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Read input image\n",
    "img = Image.open('dogncat.jpg')\n",
    "\n",
    "# convert the input image to torch tensor\n",
    "img = T.ToTensor()(img)\n",
    "print(\"Input image size:\", img.size()) # size = [3, 466, 700]\n",
    "\n",
    "# unsqueeze the image to make it 4D tensor\n",
    "img = img.unsqueeze(0) # image size = [1, 3, 466, 700]\n",
    "# define convolution layer\n",
    "# conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "conv = torch.nn.Conv2d(3, 3, 2)\n",
    "\n",
    "# apply convolution operation on image\n",
    "img = conv(img)\n",
    "print(img.size())\n",
    "plt.imshow(img[0,:,:,:].detach().numpy())\n",
    "\n",
    "\n",
    "# squeeze image to make it 3D\n",
    "img = img.squeeze(0) #now size is again [3, 466, 700]\n",
    "# convert image to PIL image\n",
    "img = T.ToPILImage()(img)\n",
    "\n",
    "# display the image after convolution\n",
    "img.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary layer unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_Tensor(size):\n",
    "    return torch.rand(size, size, size)\n",
    "a = [c for c in random_Tensor(3) ]\n",
    "print(a)\n",
    "a = torch.cat(a)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
