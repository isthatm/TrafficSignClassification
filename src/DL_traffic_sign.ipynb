{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil, pickle, os, random, matplotlib\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "\n",
    "# data logging\n",
    "import configparser\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Main</strong>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "The data is converted into RGB tensors with skimage.io.imread()\n",
    "\n",
    "<strong>Notice:</strong> pip install scikit-image "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingReporter:\n",
    "    def __init__(self, file_name) -> None:\n",
    "        self.file_name = file_name\n",
    "        self.cols = [\"Batch ID\",  \"Accuracy\", \"Margin Loss\", \"Reconstruction Loss\", \"Total Loss\"]\n",
    "        self.epoch, self.accuracy, self.marg_loss, self.recons_loss, self.total_loss  = [], [], [], [], []\n",
    "        \n",
    "        self.__write_rp()\n",
    "\n",
    "    def __write_rp(self):\n",
    "        df = pd.DataFrame(list(\n",
    "            zip(self.epoch, self.accuracy, self.marg_loss, self.recons_loss,self.total_loss)), \n",
    "            columns=self.cols) \n",
    "\n",
    "    def record(self, data: dict) -> None: \n",
    "        self.epoch.append(data['Batch ID'])\n",
    "        self.accuracy.append(data['Accuracy'])\n",
    "        self.marg_loss.append(data['Loss']['Margin'])\n",
    "        self.recons_loss.append(data['Loss']['Recon'])\n",
    "        self.total_loss.append(data['Loss']['Total'])\n",
    "\n",
    "    def test_dict(self):\n",
    "        train_dict = {}\n",
    "        a = [1,2,3,4,5]\n",
    "        for entry in self.cols:\n",
    "            print(entry)\n",
    "            train_dict.update({entry: a})\n",
    "        print(train_dict)\n",
    "a = TrainingReporter(\"hello.csv\")\n",
    "a.test_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],\n",
    "                   index=['row 1', 'row 2'],\n",
    "                   columns=['col 1', 'col 2'])\n",
    "df1.to_excel(\"output.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Pickling data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data(file, writeColumns=None):\n",
    "    \"\"\"\n",
    "    Read/Write pickle training/testing data, models to avoid\n",
    "    loading data again (time consuming)\n",
    "    \n",
    "    ---Params---\n",
    "\n",
    "    file: path to pickle file\n",
    "\n",
    "    writeColumns (array): variables to be saved to pickle file\n",
    "\n",
    "    \"\"\"\n",
    "    if writeColumns is None:\n",
    "        with open(file, mode=\"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "            return tuple(map(lambda col: dataset[col], ['images', 'labels'])) # lambda(col) where columns are the inputs\n",
    "    else:\n",
    "        with open(file, mode=\"wb\") as f:\n",
    "            dataset = pickle.dump({\"images\": writeColumns[0], \"labels\": writeColumns[1]}, f)\n",
    "            print(\"Data is saved in\", file)\n",
    "# lambda function: https://www.youtube.com/watch?v=BcbVe1r2CYc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Label the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_test(src, csv_file, labeled_test_dir, NoOfCategories):\n",
    "    \"\"\"\n",
    "    This function creates named folders corresponding to 43 categories\n",
    "    and move the test images to these folders\n",
    "\n",
    "    `csv_file` and `labeled_test_dir` should have already been in src directly \n",
    "    (create a blank folder to store labeled images)\n",
    "\n",
    "    \"\"\"\n",
    "    # Remove the existing folders in the labeled test directory if there is any\n",
    "    for filename in os.listdir(labeled_test_dir):\n",
    "        file_path = os.path.join(labeled_test_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "    csv_dir = os.path.join(src, csv_file)\n",
    "    SubTestDir = [os.path.join(labeled_test_dir, str(d)) for d in range(NoOfCategories)]\n",
    "    \n",
    "    # Create label folders\n",
    "    [os.mkdir(test_d) for test_d in SubTestDir]\n",
    "\n",
    "    testImageDir = pd.read_csv(csv_dir)['Path']\n",
    "    testImageLabel = pd.read_csv(csv_dir)['ClassId']\n",
    "    for idx in range(len(testImageLabel)):\n",
    "        label = testImageLabel[idx]\n",
    "        shutil.copy(os.path.join(src, testImageDir[idx]), SubTestDir[label])\n",
    "\n",
    "labeled_test_dir = \"D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\German\\LabeledTest\"\n",
    "# labeled_test_dir = r\"C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\German\\LabeleTest\" # BH   \n",
    "src = \"D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\German\" \n",
    "# src = r\"C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\German\" # BH\n",
    "csv_file = \"Test.csv\"\n",
    "NoOfCats = 43\n",
    "\n",
    "label_test(src, csv_file, labeled_test_dir, NoOfCats)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\"Loads a data set and returns two lists:\n",
    "    \n",
    "    images: a list of Numpy arrays, each representing an image.\n",
    "    labels: a list of numbers that represent the images labels.\n",
    "    \"\"\"\n",
    "    # Get all subdirectories of data_dir. Each represents a label.\n",
    "    directories = [d for d in os.listdir(data_dir) \n",
    "                   if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    # Loop through the label directories and collect the data in\n",
    "    # two lists, labels and images.\n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in directories:\n",
    "        # label_dir contains 61 catefories paths\n",
    "        label_dir = os.path.join(data_dir, d)\n",
    "\n",
    "        # list subdirectories within each of the 61 categories\n",
    "        file_names = [os.path.join(label_dir, f) \n",
    "                      for f in os.listdir(label_dir) if f.endswith(\".png\")]\n",
    "        # For each label, load it's images and add them to the images list.\n",
    "        # And add the label number (i.e. directory name) to the labels list.\n",
    "        for f in file_names:\n",
    "            images.append(skimage.io.imread(f))\n",
    "            labels.append(int(d))\n",
    "    return images, labels\n",
    "\n",
    "# ROOT_PATH = \"D:\\Programming Files\\Python Files\\Deep learning - traffic signs\\German\"\n",
    "ROOT_PATH = r\"C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\German\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last execution 10:01PM 20/5/23\n",
    "!!!: resized images are already normalized to [0,1] format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset.\n",
    "train_data_dir = os.path.join(ROOT_PATH, \"Train\")\n",
    "first_train_images, first_train_labels = load_data(train_data_dir)\n",
    "pickle_data(file = \"primary_train_dataset\", writeColumns = [first_train_images, first_train_labels] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply constant resolution among train images and pickle them \n",
    "train_images, train_labels  = pickle_data(file = 'primary_train_dataset')\n",
    "\n",
    "train_images = [ skimage.transform.resize(train_image, (32, 32), mode = \"constant\") \n",
    "                            for train_image in train_images ]\n",
    "\n",
    "train_images = np.stack(train_images, axis = 0)\n",
    "\n",
    "pickle_data(file = \"primary32_train_dataset\", writeColumns = [train_images, train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_data_dir = os.path.join(ROOT_PATH, \"LabeledTest\")\n",
    "first_test_images, first_test_labels = load_data(test_data_dir)\n",
    "test_images, val_images, test_labels, val_labels = train_test_split(first_test_images, first_test_labels, \n",
    "                                                                        test_size=0.36, random_state=0)\n",
    "pickle_data(file = \"primary_test_dataset\", writeColumns = [test_images, test_labels])\n",
    "pickle_data(file = \"primary_val_dataset\", writeColumns = [val_images, val_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply constant resolution among test,val images and pickle them \n",
    "test_images, test_labels  = pickle_data(file = 'primary_test_dataset')\n",
    "val_images, val_labels  = pickle_data(file = 'primary_val_dataset')\n",
    "\n",
    "test_images = [ skimage.transform.resize(test_image, (32, 32), mode = \"constant\") \n",
    "                            for test_image in test_images ]\n",
    "val_images = [ skimage.transform.resize(val_image, (32, 32), mode = \"constant\") \n",
    "                            for val_image in val_images ]\n",
    "\n",
    "test_images = np.stack(test_images, axis = 0)\n",
    "val_images = np.stack(val_images, axis = 0)\n",
    "\n",
    "pickle_data(file = \"primary32_test_dataset\", writeColumns = [test_images, test_labels])\n",
    "pickle_data(file = \"primary32_val_dataset\", writeColumns = [val_images, val_labels])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program starts here if pickle folders are not updated\n",
    "train_images, train_labels  = pickle_data(file = './Data/primary32_train_dataset')\n",
    "test_images, test_labels  = pickle_data(file = './Data/primary32_test_dataset')\n",
    "val_images, val_labels  = pickle_data(file = './Data/primary32_val_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Display images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images, labels, category=False, greyScale=False):\n",
    "    \"\"\"\n",
    "    Display the first image of each label.\n",
    "    \n",
    "    category: set to True when only images within a category are displayed\n",
    "    greyScale: set to True to display images in grey scale\n",
    "    \"\"\"\n",
    "    if category:\n",
    "        i = 1\n",
    "        startIndex = labels.index(category)\n",
    "        catImages = images[startIndex:(startIndex + labels.count(category))] #catImage = categoryImage\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        for catImage in catImages[:24]:\n",
    "            plt.subplot(8, 8, i)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.imshow(catImage)\n",
    "            i += 1 \n",
    "    else:\n",
    "        unique_labels = set(labels) # Create a list contains only the labels (non-iterative)\n",
    "    \n",
    "        #Example: a = [1, 1, 1, 2, 2, 3]\n",
    "        #set(a) >> {1,2,3}\n",
    "\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        i = 1\n",
    "        for label in unique_labels:\n",
    "            image = images[labels.index(label)] # Pick the first image for each label.\n",
    "\n",
    "        # object.index(element) returns the index of the element specified when it's first encountered\n",
    "        # Example: a = [1, 1, 1, 3, 2, 2, 3]\n",
    "        # a.index(3) = 3\n",
    "\n",
    "            plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.title(\"Label {0} ({1})\".format(label, labels.count(label))) # sign category and the # of its samples\n",
    "            if greyScale is False:\n",
    "                plt.imshow(image)\n",
    "            else:\n",
    "                plt.imshow(image, cmap=plt.cm.binary)\n",
    "            i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (function) Data Augmentation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "n_classes = len(set(train_labels))\n",
    "values, bins, patches = ax.hist(train_labels, n_classes)\n",
    "ax.set_xlabel(\"Classes\")\n",
    "ax.set_ylabel(\"Number of images\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "X_train = train_images\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "inference_datagen = ImageDataGenerator()\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "train_datagen.fit(X_train)\n",
    "train_datagen_augmented.fit(X_train)\n",
    "fig = plt.figure()\n",
    "n = 0\n",
    "graph_size = 3\n",
    "\n",
    "for x_batch, y_batch in train_datagen_augmented.flow(X_train, train_labels, batch_size=4):\n",
    "    a=fig.add_subplot(graph_size, graph_size, n+1)\n",
    "    greyBatch = preprocess_images(x_batch)\n",
    "    print(greyBatch[0].shape)\n",
    "    imgplot = plt.imshow(greyBatch[0])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Label:{}\".format(y_batch[0]))\n",
    "    n = n + 1\n",
    "    if n > 8:\n",
    "        break\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capsule layer attributes\n",
    "TODO:\n",
    "- build a simple conv layer: DONE (first conv layer with relu)\n",
    "- build capsule's functions (squash, routing)\n",
    "- build primary and secondary capsnet\n",
    "- fully connected layer for reconstruction\n",
    "- loss functions\n",
    "- add visualizations to track the changes in prior logits, training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torchvision.transforms as T\n",
    "\n",
    "a = torch.Tensor(train_images[0]) # RGB images\n",
    "b = torch.Tensor(train_images[1])\n",
    "c = torch.stack((a,b), dim=0).permute(0, 3, 1, 2) # create a 2 batchs with 1 image in each batch\n",
    "\n",
    "class ImgAug:\n",
    "    def __init__(self, batch_img, value):\n",
    "        \"\"\"\n",
    "        Augment images by batch\n",
    "        :param batch_img: [batch, C, H, W]\n",
    "        :param value: the degree at which the imgs are transformed \n",
    "                      (recommended 0.3 -> 9 pixels)\n",
    "\n",
    "        :return: transformed batch [batch, C, H, W]\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_img = batch_img\n",
    "        self.value = value\n",
    "        aug_method = random.randint(0, 2)\n",
    "        aug_dict = {\n",
    "            '0': self._horizontal_shift(),\n",
    "            '1': self._vertical_shift(),\n",
    "            '2': self._rotate()\n",
    "        }\n",
    "        \n",
    "        augmented_batch = aug_dict[str(aug_method)]\n",
    "        self.resized_img = self._fill(augmented_batch)\n",
    " \n",
    "    def apply_trans(self):\n",
    "        return self.resized_img\n",
    "       \n",
    "    @staticmethod\n",
    "    def _fill(img):\n",
    "        return T.Resize(size=(32,32), antialias=True)(img)\n",
    "\n",
    "    def _horizontal_shift(self):\n",
    "        ratio = random.uniform(-self.value, self.value)\n",
    "        shift_by = int(ratio*(self.batch_img.size()[-1]))\n",
    "        if shift_by < 0: # shift to the right\n",
    "            shifted_batch_img = self.batch_img[:, :, :, :shift_by]\n",
    "        elif shift_by > 0: # shift to the left\n",
    "         \n",
    "            shifted_batch_img = self.batch_img[:, :, :, shift_by:]\n",
    "        else:\n",
    "            shifted_batch_img = self.batch_img\n",
    "        \n",
    "        return shifted_batch_img\n",
    "\n",
    "    def _vertical_shift(self):\n",
    "        ratio = random.uniform(-self.value, self.value)\n",
    "        shift_by = int(ratio*(self.batch_img.size()[-2]))\n",
    "        if shift_by < 0: # shift to the upward\n",
    "            shifted_batch_img = self.batch_img[:, :, :shift_by, :]\n",
    "        elif shift_by > 0: # shift to the downward\n",
    "            shifted_batch_img = self.batch_img[:, :, shift_by:, :]\n",
    "          \n",
    "        else:\n",
    "            shifted_batch_img = self.batch_img\n",
    "\n",
    "        return shifted_batch_img\n",
    "    \n",
    "    def _rotate(self):\n",
    "  \n",
    "        return T.RandomRotation(degrees=45)(self.batch_img)\n",
    "\n",
    "# shifted_batch = ImgAug(c, 0.3)\n",
    "# print(shifted_batch.apply_trans().size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently this network only supports grey scale imgage due to fully connected reconstruction layer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "logging.basicConfig(filename='caps_net.log', filemode='w', format='%(asctime)s: %(message)s', level=logging.DEBUG)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\n",
    "    r'C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\src\\capsnet_config.ini'\n",
    "    )\n",
    "\n",
    "# General info\n",
    "num_class = int(config['network']['num_class'])\n",
    "img_channels = int(config['network']['image_channels'])\n",
    "batch_size = int(config['network']['batch_size'])\n",
    "learning_rate = float(config['network']['lr'])\n",
    "epochs = int(config['network']['epochs'])\n",
    "\n",
    "train_num = torch.tensor(train_images).size()[0]\n",
    "valid_num = torch.tensor(val_images).size()[0]\n",
    "test_num = torch.tensor(test_images).size()[0]\n",
    "\n",
    "# Primary caps\n",
    "primary_num_caps = int(config['primary_caps']['num_caps'])\n",
    "primary_channels = int(config['primary_caps']['channels'])\n",
    "\n",
    "# Digit caps\n",
    "digit_num_caps = num_class\n",
    "digit_channels = int(config['digit_caps']['channels'])\n",
    "num_iterations = int(config['network']['num_routing_iter'])\n",
    "\n",
    "# Loss hyper params\n",
    "m_plus = float(config['loss']['m_plus'])\n",
    "m_minus = float(config['loss']['m_minus'])\n",
    "lmbd = float(config['loss']['lambda'])\n",
    "regularization_factor = float(config['loss']['regularization_factor'])\n",
    "\n",
    "\n",
    "def squash(vector, axis=-1 ,epsilon=1e-7, squash=True):\n",
    "        \"\"\"\n",
    "        normalize the length of the vector by 1\n",
    "        `vector`: the muliplication of coupling coefs and prediction vectors sum [ c(ij)u^(j|i) ]\n",
    "        `axis`: the axis that would not be reduced\n",
    "        'epsilon`: a workaround to prevent devision by zero\n",
    "        \"\"\"\n",
    "        squared_norm = torch.sum(torch.square(vector), dim=axis, \n",
    "                                keepdim=True)\n",
    "        safe_norm = torch.sqrt(squared_norm + epsilon)\n",
    "\n",
    "        if squash:\n",
    "            squash_factor = squared_norm / (1. + squared_norm)\n",
    "            unit_vector = vector / safe_norm\n",
    "            return squash_factor * unit_vector\n",
    "        else:\n",
    "            return safe_norm\n",
    "\n",
    "\n",
    "class CapsLayers(nn.Module):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "    :param num_conv: number of filters/convolutional unit per capsule (dimension of a capsule)\n",
    "    :param num_capsules: number of primary/digit caps\n",
    "    :param num_routing_nodes: number of possible u(i), \n",
    "                            set to -1 if it's not a secondary capsule layer\n",
    "    :param in_channels: output convolutional layers of the prev layer\n",
    "    :param out_channels: output convolutional layers of the current layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsules: int, in_channels: int, out_channels: int, \n",
    "                 kernel_size=None, stride=None, num_routing_nodes=None ,num_iterations=None):\n",
    "        super(CapsLayers, self).__init__()\n",
    "        self.num_capsules = num_capsules\n",
    "        \n",
    "        self.num_iterations = num_iterations\n",
    "        self.num_routing_nodes = num_routing_nodes\n",
    "        if num_routing_nodes is not None:\n",
    "            self.weights = nn.Parameter(torch.randn(\n",
    "                                        self.num_routing_nodes, num_capsules, out_channels, in_channels))\n",
    "            self.b = nn.Parameter(torch.zeros(\n",
    "                                    self.num_routing_nodes, num_capsules, 1, 1))\n",
    "        else:\n",
    "            self.primary_caps = nn.ModuleList(nn.Conv2d(in_channels, out_channels, kernel_size, stride) \n",
    "                                                    for _ in range(num_capsules))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Feed foward function for non-reconstruction layer\n",
    "        :param inputs: \n",
    "            for the primary caps, the inputs are convolutional layer pixels\n",
    "            for digit caps, the inputs are n-D vectors from a primary cap\n",
    "                where n is the # of filters for one capsule  \n",
    "            Required Paramteters:\n",
    "            prior_logits(b) \n",
    "            primary layer prediction (requires u-layer 1 ouput, Weights)\n",
    "        \"\"\"\n",
    "        if self.num_routing_nodes is not None:\n",
    "            weights = self.weights[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "            b_ij = self.b[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "            inputs = inputs.tile(1, 1, self.num_capsules, 1, 1)\n",
    "            \n",
    "            # u_hat = [batch, num_routing_nodes, # digit_caps, digit_caps_dims, 1]\n",
    "            u_hat = weights @ inputs \n",
    "\n",
    "            for i in range(self.num_iterations):\n",
    "                c_ij = F.softmax(b_ij, dim=2)\n",
    "                outputs = squash((c_ij*u_hat).sum(dim=1, keepdim=True))\n",
    "\n",
    "                if i < self.num_iterations - 1 :\n",
    "                    # v_j OR outputs = [batch, 1 -> num_routing_nodes, num_digit_caps, digit_caps_dims, 1 )]\n",
    "                    b_ij +=  (b_ij + torch.transpose(u_hat, 3, 4) @ outputs.tile(1, self.num_routing_nodes, 1, 1, 1))\n",
    "\n",
    "        else:\n",
    "            outputs = [\n",
    "                capsule(inputs)[:, None, :, :, :].permute(0, 1, 3, 4, 2) for capsule in self.primary_caps]\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "            # u(i) = [batch, num_prim_caps*prim_caps_2D_size, prim_caps_output_dimension]\n",
    "            outputs = outputs.view(outputs.size(0), -1, outputs.size(4)) \n",
    "            outputs = squash(outputs)[:, :, None, :, None]\n",
    "        \n",
    "        # outputs = [batch, 1, num_digit_caps/num_class, digit_caps_dims, 1 )]\n",
    "        return outputs            \n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    \"\"\"\n",
    "        This class contains the full CapsNet architecture:\n",
    "        Convolutional -> primary capsules -> digit capsules -> (3) fully connected\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Params: \n",
    "        `inputs`: a 4D tensor (grey scale or RGB)\n",
    "        \"\"\"\n",
    "        super(CapsNet, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(img_channels, 256, \n",
    "                                kernel_size=9, stride=1)\n",
    "        self.primary_caps = CapsLayers(primary_num_caps, 256, primary_channels, \n",
    "                                        kernel_size=5, stride=2)\n",
    "        self.digit_caps = CapsLayers(digit_num_caps, primary_channels, digit_channels, \n",
    "                                     num_routing_nodes=10*10*primary_num_caps, num_iterations=num_iterations)\n",
    "        self.grey_scale_decoder = nn.Sequential(\n",
    "                nn.Linear(digit_channels*digit_num_caps, 576),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(576, 1600),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1600, 1024),\n",
    "                nn.Sigmoid(),\n",
    "        )\n",
    "        self.linear_trans = nn.Linear(digit_channels*digit_num_caps, 400)\n",
    "        self.RGB_decoder = nn.Sequential(\n",
    "                nn.Upsample(size=(8, 8)),\n",
    "                nn.Conv2d(16, 4, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(4, 8, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(8, 16, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv2d(16, 3, 3, padding='same'),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, images, labels=None): # labels should be applied `one_hot` function\n",
    "        conv_1_ouputs = F.relu(self.conv_1(images))\n",
    "        primary_caps_outputs =  self.primary_caps(conv_1_ouputs) \n",
    "        digit_caps_outputs = self.digit_caps(primary_caps_outputs).squeeze(1)\n",
    "        \n",
    "        assert list(digit_caps_outputs.size()) == [images.size()[0], num_class, digit_channels, 1]\n",
    "               \n",
    "        v_norm = squash(digit_caps_outputs, axis=-2, squash=False)\n",
    "        v_prob = F.softmax(v_norm, dim=1)\n",
    "\n",
    "        self.img = images\n",
    "        self.v_norm = v_norm\n",
    "\n",
    "        idx = torch.zeros(images.size()[0], 1, 1)\n",
    "        # Masking\n",
    "        if labels is None: # Testing mode\n",
    "            _, idx = torch.max(v_prob, dim=1)\n",
    "            labels = torch.eye(num_class).index_select(dim=0, index = idx.squeeze())\n",
    "\n",
    "        #masked_v = [batch_size, digit_channels*classes])\n",
    "        masked_v = (labels[:, :, None, None] * digit_caps_outputs).view(images.size(0), -1)\n",
    "        \n",
    "        # Reconstruction\n",
    "        if images.size()[1] == 1:\n",
    "            reconstructed_img = self.grey_scale_decoder(masked_v) # [batch, 32x32]\n",
    "        else:\n",
    "            # try:\n",
    "            fc_out = self.linear_trans(masked_v).view(batch_size, 16 ,5, 5) #masked_v\n",
    "            reconstructed_img = self.RGB_decoder(fc_out) # [batch, channel, height, width]\n",
    "            # except:\n",
    "            #     print(fc_out.size())\n",
    "           \n",
    "        logging.debug(f'Capsule layer pred / batch: {idx}')\n",
    "\n",
    "        return idx, reconstructed_img \n",
    "        \n",
    "\n",
    "    def loss_fn(self, reconstructed_img, labels):\n",
    "        # Margin loss\n",
    "        max_1 = F.relu(m_plus - self.v_norm)\n",
    "        max_2 = F.relu(self.v_norm - m_minus)\n",
    "        T_k = labels[:, :, None, None]\n",
    "        \n",
    "        L_k = T_k * torch.square(max_1) + lmbd * (1 - T_k) * torch.square(max_2)\n",
    "\n",
    "        assert L_k.size() == self.v_norm.size()\n",
    "        margin_loss = L_k.sum(dim=1).mean()\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss_obj = nn.MSELoss()\n",
    "\n",
    "        # original_img = [batch size, flatten image (pixels are flatten into arrays)]\n",
    "        original_img = self.img.view(batch_size, -1)\n",
    "        reconstruction_loss = reconstruction_loss_obj(reconstructed_img, original_img)\n",
    "        \n",
    "        total_loss = margin_loss + regularization_factor * reconstruction_loss\n",
    "\n",
    "        return margin_loss, reconstruction_loss, total_loss\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    # plot_batch = plt.figure()\n",
    "    \n",
    "    model.train() # set the model to training mode\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        img_aug = ImgAug(x_batch.permute(0, 3, 1, 2), 0.3)\n",
    "\n",
    "        x_batch = img_aug.apply_trans()\n",
    "        y_batch = torch.nn.functional.one_hot(y_batch.to(torch.int64), num_classes=43)\n",
    "               \n",
    "        if device == 'cuda':\n",
    "            x_batch, y_batch = x_batch.to('cuda'), y_batch.to('cuda')\n",
    "        # print(x_batch.device, y_batch.device)\n",
    "        # print(x_batch.size(), y_batch.size()) \n",
    "\n",
    "        _, recons_img = model(x_batch, y_batch) # recons_img = [batch, C, H, W]\n",
    "        print('recons', recons_img.size())\n",
    "        # margin_loss, recons_loss, total_loss = model.loss_fn(recons_img, y_batch)\n",
    "        # total_loss.backward()\n",
    "        # optimizer.step() # update the params to be optimized (weights, biases, routing weights)\n",
    "     \n",
    "        # if batch_idx % (batch_size * 2) == 0:\n",
    "        #     print(f'Batch {batch_idx+1}/{train_num//batch_size}')\n",
    "        \n",
    "        if batch_idx > 100:\n",
    "            break\n",
    "\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Can be used for both test and validation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    for idx, (batch_images, batch_labels) in enumerate(test_loader):\n",
    "        if device == 'cuda':\n",
    "            batch_images = batch_images.cuda()\n",
    "            batch_labels = batch_labels.cuda()\n",
    "        \n",
    "        image = torch.Tensor(\n",
    "            preprocess_images(image[None, :, :, :]).permute(0, 3, 1, 2),\n",
    "            dtype=torch.float32)\n",
    "        \n",
    "        pred_idx, recons_img = model(image)\n",
    "        \n",
    "def data_loader(images: np.array, \n",
    "                labels: np.array, \n",
    "                batch_size: int,\n",
    "                shuffle: bool) -> object:\n",
    "    \"\"\"\n",
    "    :param images: 3D array [H, W, Channels]\n",
    "    :param labels: 1D array\n",
    "    :param batch_size: number of images per batch\n",
    "\n",
    "    :return: an iterable object\n",
    "    \"\"\"\n",
    "    images_tensor = torch.Tensor(images)\n",
    "    labels_tensor = torch.Tensor(labels)\n",
    "\n",
    "    dataset = TensorDataset(images_tensor, labels_tensor)\n",
    "    data_loader = DataLoader(dataset, batch_size, shuffle=shuffle)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device_as_str = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device_as_str, \"will be used\")\n",
    "    # device_as_str = 'cpu'\n",
    "    model = CapsNet().to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, min_lr=1e-5)\n",
    "\n",
    "    # Prepare dataset\n",
    "    val_loader = data_loader(val_images, val_labels, batch_size, shuffle=False)\n",
    "    train_loader = data_loader(train_images, train_labels, batch_size, shuffle=True)\n",
    "    \n",
    "    # for idx, (img, label) in enumerate(train_loader):\n",
    "    #     print(img.size())\n",
    "    #     if idx == 5:\n",
    "    #         break\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        train(model, train_loader, optimizer, device_as_str) \n",
    "        # val_loss = test(model, val_loader, device_as_str)\n",
    "        # scheduler.step(val_loss)\n",
    "\n",
    "    # should put next_batch into train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input image tensor permitted channel values are [1, 3], but found 32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m idx, (i,l) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m----> 2\u001b[0m     a \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39;49mGrayscale()(i\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(a\u001b[39m.\u001b[39msize())\n\u001b[0;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m idx \u001b[39m>\u001b[39m \u001b[39m20\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\DLvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\DLvenv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:1580\u001b[0m, in \u001b[0;36mGrayscale.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1572\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m   1573\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be converted to grayscale.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1578\u001b[0m \u001b[39m        PIL Image or Tensor: Grayscaled image.\u001b[39;00m\n\u001b[0;32m   1579\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1580\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrgb_to_grayscale(img, num_output_channels\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_output_channels)\n",
      "File \u001b[1;32mc:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\DLvenv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1299\u001b[0m, in \u001b[0;36mrgb_to_grayscale\u001b[1;34m(img, num_output_channels)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m   1297\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39mto_grayscale(img, num_output_channels)\n\u001b[1;32m-> 1299\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mrgb_to_grayscale(img, num_output_channels)\n",
      "File \u001b[1;32mc:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\DLvenv\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:149\u001b[0m, in \u001b[0;36mrgb_to_grayscale\u001b[1;34m(img, num_output_channels)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m img\u001b[39m.\u001b[39mndim \u001b[39m<\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput image tensor should have at least 3 dimensions, but found \u001b[39m\u001b[39m{\u001b[39;00mimg\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 149\u001b[0m _assert_channels(img, [\u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m])\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m num_output_channels \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m    152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnum_output_channels should be either 1 or 3\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\DLvenv\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:61\u001b[0m, in \u001b[0;36m_assert_channels\u001b[1;34m(img, permitted)\u001b[0m\n\u001b[0;32m     59\u001b[0m c \u001b[39m=\u001b[39m get_dimensions(img)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     60\u001b[0m \u001b[39mif\u001b[39;00m c \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m permitted:\n\u001b[1;32m---> 61\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput image tensor permitted channel values are \u001b[39m\u001b[39m{\u001b[39;00mpermitted\u001b[39m}\u001b[39;00m\u001b[39m, but found \u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Input image tensor permitted channel values are [1, 3], but found 32"
     ]
    }
   ],
   "source": [
    "for idx, (i,l) in enumerate(train_loader):\n",
    "    a = T.Grayscale()(i.permute(0, 2, 3, 1))\n",
    "    print(a.size())\n",
    "    if idx > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda will be used\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 315\u001b[0m\n\u001b[0;32m    312\u001b[0m train_loader \u001b[39m=\u001b[39m train_datagen_augmented\u001b[39m.\u001b[39mflow(X_train, demo_train_labels, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[0;32m    314\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):  \n\u001b[1;32m--> 315\u001b[0m     train(model, train_loader, optimizer, device_as_str) \n\u001b[0;32m    316\u001b[0m     val_loss \u001b[39m=\u001b[39m test(model, val_images, val_labels)\n\u001b[0;32m    317\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(val_loss)\n",
      "Cell \u001b[1;32mIn[54], line 276\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, device)\u001b[0m\n\u001b[0;32m    274\u001b[0m _, recons_img \u001b[39m=\u001b[39m model(x_batch, y_batch)\n\u001b[0;32m    275\u001b[0m margin_loss, recons_loss, total_loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss_fn(recons_img, y_batch)\n\u001b[1;32m--> 276\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    277\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \u001b[39m# update the params to be optimized (weights, biases, routing weights)\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[39m# if batch_idx % (batch_size * 2) == 0:\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[39m#     print(f'Batch {batch_idx+1}/{len(train_loader)*batch_size}')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\DLvenv\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\DLvenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from skimage import exposure\n",
    "\n",
    "def preprocess_images(images):\n",
    "    \"\"\"\n",
    "        - Convert RGB images to grey scale \n",
    "        - Normalize pixels to 0-1,\n",
    "        - Improve the contrast with adaptive histogram localization\n",
    "    \"\"\"\n",
    "     \n",
    "    # Conver RGB -> grey scale\n",
    "    images = 0.299 * images[:, :, :, 0] + 0.587 * images[:, :, :, 1] + 0.114 * images[:, :, :, 2]\n",
    "    # Improve the contrast\n",
    "    images = exposure.equalize_adapthist(images)\n",
    "\n",
    "    # Add ONE 3-D channel for grey scale\n",
    "    images = images.reshape(images.shape + (1,)) \n",
    "\n",
    "    return images\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\n",
    "    r'C:\\Users\\lemin03\\Documents\\traffic_class\\TrafficSignRec\\src\\capsnet_config.ini'\n",
    "    )\n",
    "\n",
    "# General info\n",
    "num_class = int(config['network']['num_class'])\n",
    "img_channels = int(config['network']['image_channels'])\n",
    "batch_size = int(config['network']['batch_size'])\n",
    "learning_rate = float(config['network']['lr'])\n",
    "epochs = int(config['network']['epochs'])\n",
    "\n",
    "train_num = torch.tensor(train_images).size()[0]\n",
    "valid_num = torch.tensor(val_images).size()[0]\n",
    "test_num = torch.tensor(test_images).size()[0]\n",
    "\n",
    "# Primary caps\n",
    "primary_num_caps = int(config['primary_caps']['num_caps'])\n",
    "primary_channels = int(config['primary_caps']['channels'])\n",
    "\n",
    "# Digit caps\n",
    "digit_num_caps = num_class\n",
    "digit_channels = int(config['digit_caps']['channels'])\n",
    "num_iterations = int(config['network']['num_routing_iter'])\n",
    "\n",
    "# Loss hyper params\n",
    "m_plus = float(config['loss']['m_plus'])\n",
    "m_minus = float(config['loss']['m_minus'])\n",
    "lmbd = float(config['loss']['lambda'])\n",
    "regularization_factor = float(config['loss']['regularization_factor'])\n",
    "\n",
    "\n",
    "shuffeled_train_images, shuffeled_train_labels = shuffle(train_images, train_labels, random_state=0)\n",
    "demo_train_images = shuffeled_train_images[0:100]\n",
    "demo_train_labels = shuffeled_train_labels[0:100]\n",
    "X_train = demo_train_images\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "inference_datagen = ImageDataGenerator()\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "def squash(vector, axis=-1 ,epsilon=1e-7, squash=True):\n",
    "        \"\"\"\n",
    "        normalize the length of the vector by 1\n",
    "        `vector`: the muliplication of coupling coefs and prediction vectors sum [ c(ij)u^(j|i) ]\n",
    "        `axis`: the axis that would not be reduced\n",
    "        'epsilon`: a workaround to prevent devision by zero\n",
    "        \"\"\"\n",
    "        squared_norm = torch.sum(torch.square(vector), dim=axis, \n",
    "                                keepdim=True)\n",
    "        safe_norm = torch.sqrt(squared_norm + epsilon)\n",
    "\n",
    "        if squash:\n",
    "            squash_factor = squared_norm / (1. + squared_norm)\n",
    "            unit_vector = vector / safe_norm\n",
    "            return squash_factor * unit_vector\n",
    "        else:\n",
    "            return safe_norm\n",
    "\n",
    "\n",
    "class CapsLayers(nn.Module):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "    :param num_conv: number of filters/convolutional unit per capsule (dimension of a capsule)\n",
    "    :param num_capsules: number of primary/digit caps\n",
    "    :param num_routing_nodes: number of possible u(i), \n",
    "                            set to -1 if it's not a secondary capsule layer\n",
    "    :param in_channels: output convolutional layers of the prev layer\n",
    "    :param out_channels: output convolutional layers of the current layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsules: int, in_channels: int, out_channels: int, \n",
    "                 kernel_size=None, stride=None, num_routing_nodes=None ,num_iterations=None):\n",
    "        super(CapsLayers, self).__init__()\n",
    "        self.num_capsules = num_capsules\n",
    "        \n",
    "        self.num_iterations = num_iterations\n",
    "        self.num_routing_nodes = num_routing_nodes\n",
    "        if num_routing_nodes is not None:\n",
    "            self.weights = nn.Parameter(torch.randn(\n",
    "                                        self.num_routing_nodes, num_capsules, out_channels, in_channels))\n",
    "            self.b = nn.Parameter(torch.zeros(\n",
    "                                    self.num_routing_nodes, num_capsules, 1, 1))\n",
    "        else:\n",
    "            self.primary_caps = nn.ModuleList(nn.Conv2d(in_channels, out_channels, kernel_size, stride) \n",
    "                                                    for _ in range(num_capsules))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Feed foward function for non-reconstruction layer\n",
    "        :param inputs: \n",
    "            for the primary caps, the inputs are convolutional layer pixels\n",
    "            for digit caps, the inputs are n-D vectors from a primary cap\n",
    "                where n is the # of filters for one capsule  \n",
    "            Required Paramteters:\n",
    "            prior_logits(b) \n",
    "            primary layer prediction (requires u-layer 1 ouput, Weights)\n",
    "        \"\"\"\n",
    "        if self.num_routing_nodes is not None:\n",
    "            weights = self.weights[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "            b_ij = self.b[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "            inputs = inputs.tile(1, 1, self.num_capsules, 1, 1)\n",
    "            \n",
    "            # u_hat = [batch, num_routing_nodes, # digit_caps, digit_caps_dims, 1]\n",
    "            u_hat = weights @ inputs \n",
    "\n",
    "            for i in range(self.num_iterations):\n",
    "                c_ij = F.softmax(b_ij, dim=2)\n",
    "                outputs = squash((c_ij*u_hat).sum(dim=1, keepdim=True))\n",
    "\n",
    "                if i < self.num_iterations - 1 :\n",
    "                    # v_j OR outputs = [batch, 1 -> num_routing_nodes, num_digit_caps, digit_caps_dims, 1 )]\n",
    "                    b_ij +=  (b_ij + torch.transpose(u_hat, 3, 4) @ outputs.tile(1, self.num_routing_nodes, 1, 1, 1))\n",
    "\n",
    "        else:\n",
    "            outputs = [\n",
    "                capsule(inputs)[:, None, :, :, :].permute(0, 1, 3, 4, 2) for capsule in self.primary_caps]\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "            # u(i) = [batch, num_prim_caps*prim_caps_2D_size, prim_caps_output_dimension]\n",
    "            outputs = outputs.view(outputs.size(0), -1, outputs.size(4)) \n",
    "            outputs = squash(outputs)[:, :, None, :, None]\n",
    "        \n",
    "        # outputs = [batch, 1, num_digit_caps/num_class, digit_caps_dims, 1 )]\n",
    "        return outputs            \n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    \"\"\"\n",
    "        This class contains the full CapsNet architecture:\n",
    "        Convolutional -> primary capsules -> digit capsules -> (3) fully connected\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Params: \n",
    "        `inputs`: a 4D tensor (grey scale or RGB)\n",
    "        \"\"\"\n",
    "        super(CapsNet, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(img_channels, 256, \n",
    "                                kernel_size=9, stride=1)\n",
    "        self.primary_caps = CapsLayers(primary_num_caps, 256, primary_channels, \n",
    "                                        kernel_size=5, stride=2)\n",
    "        self.digit_caps = CapsLayers(digit_num_caps, primary_channels, digit_channels, \n",
    "                                     num_routing_nodes=10*10*primary_num_caps, num_iterations=num_iterations)\n",
    "        self.grey_scale_decoder = nn.Sequential(\n",
    "                nn.Linear(digit_channels*digit_num_caps, 576),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(576, 1600),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1600, 1024),\n",
    "                nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.RGB_decoder = nn.Sequential(\n",
    "                nn.Upsample(size=(8, 8)),\n",
    "                nn.Conv2d(16, 4, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(4, 8, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(8, 16, 3, padding='same'),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv2d(16, 3, 3, padding='same'),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, images, labels=None): # labels should be applied `one_hot` function\n",
    "        conv_1_ouputs = F.relu(self.conv_1(images))\n",
    "        primary_caps_outputs =  self.primary_caps(conv_1_ouputs) #TODO: check again the size of conv1ouputs to \n",
    "        # verify capsules takes the correct size (since conv outputs has size [b, v, h, w])\n",
    "        digit_caps_outputs = self.digit_caps(primary_caps_outputs).squeeze(1)\n",
    "        \n",
    "        assert list(digit_caps_outputs.size()) == [images.size()[0], num_class, digit_channels, 1]\n",
    "               \n",
    "        v_norm = squash(digit_caps_outputs, axis=-2, squash=False)\n",
    "        v_prob = F.softmax(v_norm, dim=1)\n",
    "\n",
    "        self.img = images\n",
    "        self.v_norm = v_norm\n",
    "\n",
    "        idx = torch.zeros(images.size()[0], 1, 1)\n",
    "        # Masking\n",
    "        if labels is None: # Testing mode\n",
    "            _, idx = torch.max(v_prob, dim=1)\n",
    "            labels = torch.eye(num_class).index_select(dim=0, index = idx.squeeze())\n",
    "\n",
    "        masked_v = (labels[:, :, None, None] * digit_caps_outputs).view(images.size(0), -1)\n",
    "\n",
    "        # Reconstruction\n",
    "        if images.size()[1] == 1:\n",
    "            reconstructed_img = self.grey_scale_decoder(masked_v) # [batch, 32x32]\n",
    "\n",
    "        else:\n",
    "            linear_trans = nn.Linear(digit_channels*digit_num_caps, 400)\n",
    "            fc_out = linear_trans(masked_v)\n",
    "            reconstructed_img = self.RGB_decoder(fc_out) # [batch, channel, height, width]\n",
    "\n",
    "        return idx, reconstructed_img \n",
    "        \n",
    "\n",
    "    def loss_fn(self, reconstructed_img, labels):\n",
    "        # Margin loss\n",
    "        max_1 = F.relu(m_plus - self.v_norm)\n",
    "        max_2 = F.relu(self.v_norm - m_minus)\n",
    "        T_k = labels[:, :, None, None]\n",
    "        \n",
    "        L_k = T_k * torch.square(max_1) + lmbd * (1 - T_k) * torch.square(max_2)\n",
    "\n",
    "        assert L_k.size() == self.v_norm.size()\n",
    "        margin_loss = L_k.sum(dim=1).mean()\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss_obj = nn.MSELoss()\n",
    "\n",
    "        # original_img = [batch size, flatten image (pixels are flatten into arrays)]\n",
    "        original_img = self.img.view(batch_size, -1)\n",
    "        reconstruction_loss = reconstruction_loss_obj(reconstructed_img, original_img)\n",
    "        \n",
    "        total_loss = margin_loss + regularization_factor * reconstruction_loss\n",
    "\n",
    "        return margin_loss, reconstruction_loss, total_loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    # plot_batch = plt.figure()\n",
    "    n = 0\n",
    "    model.train() # set the model to training mode\n",
    "    for batch_idx in range(train_num//batch_size):\n",
    "        # plot_batch.add_subplot(10, 10, n+1)\n",
    "        batch = next(train_loader)\n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "        # plt.imshow(x_batch[0])\n",
    "        y_batch = torch.tensor(y_batch)\n",
    "        y_batch = torch.nn.functional.one_hot(y_batch.to(torch.int64), num_classes=43)\n",
    "\n",
    "        x_batch = torch.tensor(preprocess_images(x_batch))\n",
    "        x_batch = x_batch.permute(0, 3, 1, 2)# [batch_size, channels, height, width]\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            x_batch, y_batch = x_batch.cuda(), y_batch.cuda() \n",
    "\n",
    "        _, recons_img = model(x_batch, y_batch)\n",
    "        margin_loss, recons_loss, total_loss = model.loss_fn(recons_img, y_batch)\n",
    "        total_loss.backward()\n",
    "        optimizer.step() # update the params to be optimized (weights, biases, routing weights)\n",
    "     \n",
    "        # if batch_idx % (batch_size * 2) == 0:\n",
    "        #     print(f'Batch {batch_idx+1}/{len(train_loader)*batch_size}')\n",
    "        print(batch_idx)\n",
    "        if n > 100:\n",
    "            break\n",
    "        n += 1 \n",
    "\n",
    "def test(model, test_imgs, test_labels , device):\n",
    "    \"\"\"\n",
    "    Can be used for both test and validation\n",
    "    \"\"\"\n",
    "    test_imgs = torch.tensor(test_imgs)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "\n",
    "    if device == 'cuda':\n",
    "        test_imgs = test_imgs.cuda()\n",
    "        test_labels = test_labels.cuda()\n",
    "\n",
    "    model.eval()\n",
    "    for idx, image in enumerate(test_imgs):\n",
    "        image = image[None, :, :, :]\n",
    "        pred_idx, recons_img = model(image)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device_as_str = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device_as_str, \"will be used\")\n",
    "    device_as_str = 'cpu'\n",
    "    model = CapsNet()#.cuda()\n",
    "  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, min_lr=1e-5)\n",
    "\n",
    "    train_loader = train_datagen_augmented.flow(X_train, demo_train_labels, batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        train(model, train_loader, optimizer, device_as_str) \n",
    "        val_loss = test(model, val_images, val_labels)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # should put next_batch into train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_channels = 16\n",
    "digit_num_caps = 43\n",
    "\n",
    "grey_scale_decoder = nn.Sequential(\n",
    "                nn.Linear(digit_channels*digit_num_caps, 576, device='cuda'),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(576, 1600, device='cuda'),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1600, 1024, device='cuda'),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "RGB_decoder = nn.Sequential(\n",
    "            nn.Upsample(size=(8, 8)),\n",
    "            nn.Conv2d(16, 4, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(4, 8, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(8, 16, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16, 3, 3, padding='same'),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "inputs = torch.ones(2, 43, 16, 1).view(2, -1)\n",
    "decoder = nn.Linear(digit_channels*digit_num_caps, 400)\n",
    "decoded_v = decoder(inputs).view(2, digit_channels, 5, 5) #torch.Size([2, 16, 8, 8])\n",
    "\n",
    "inputs2 = torch.ones(2, 43, 16, 1, device = 'cuda').view(2, -1)\n",
    "\n",
    "output1 = RGB_decoder(decoded_v)\n",
    "output2 = grey_scale_decoder(inputs2)\n",
    "print(output1.size())\n",
    "plt.imshow(output1[0].permute(1, 2, 0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111 tf.Tensor([ 2  8  8 16], shape=(4,), dtype=int32)\n",
      "22222 tf.Tensor([2 6 6 4], shape=(4,), dtype=int32)\n",
      "tf.Tensor([ 2 32 32  3], shape=(4,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16210e91290>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "capsule_vector = tf.ones(shape=[2, 43, 16, 1])\n",
    "capsule_vector = tf.reshape(capsule_vector, shape=[2, -1])\n",
    "fc1 = tf.keras.layers.Dense(400)(capsule_vector)\n",
    "fc1 = tf.reshape(fc1, shape=(batch_size, 5, 5, 16))\n",
    "upsample1 = tf.image.resize(fc1, [8, 8])\n",
    "\n",
    "print('11111',tf.shape(upsample1))\n",
    "conv1 = tf.keras.layers.Conv2D(4, kernel_size=(3,3), activation=tf.nn.relu)(upsample1)\n",
    "print('22222',tf.shape(conv1))\n",
    "\n",
    "upsample2 = tf.image.resize(conv1, (16, 16))\n",
    "conv2 = tf.keras.layers.Conv2D(8, (3,3), padding='same', activation=tf.nn.relu)(upsample2)\n",
    "\n",
    "upsample3 = tf.image.resize(conv2, (32, 32))\n",
    "conv6 = tf.keras.layers.Conv2D(16, (3,3), padding='same', activation=tf.nn.relu)(upsample3)\n",
    "\n",
    "# 3 channel for RGG\n",
    "logits = tf.keras.layers.Conv2D( 3, (3,3), padding='same', activation=None)(conv6)\n",
    "decoded = tf.nn.sigmoid(logits, name='decoded')\n",
    "print(tf.shape(decoded))\n",
    "\n",
    "plt.imshow(decoded[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo (based on German dataset but matrix sizes are chosen on purpose to match MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_conv = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=13, stride=1) #kernel = 9, stride = 1 for 10x10\n",
    "primary_caps =  nn.ModuleList(nn.Conv2d(256, 8, 9, 2) for _ in range(32))#kernel = 5 or 6, stride = 2 for 10x10\n",
    "\n",
    "def squash(vector, axis=-1, epsilon=1e-7, squash=True):\n",
    "        \"\"\"\n",
    "        normalize the length of the vector by 1\n",
    "        `vector`: the muliplication of coupling coefs and prediction vectors sum [ c(ij)u^(j|i) ]\n",
    "        `axis`: the axis that would not be reduced\n",
    "        'epsilon`: a workaround to prevent devision by zero\n",
    "        \"\"\"\n",
    "        squaredNorm = torch.sum(torch.square(vector), dim=axis, \n",
    "                                keepdim=True)\n",
    "        safeNorm = torch.sqrt(squaredNorm + epsilon)\n",
    "        \n",
    "        if squash:\n",
    "                squashFactor = squaredNorm / (1. + squaredNorm)\n",
    "                unitVector = vector / safeNorm\n",
    "                return squashFactor * unitVector\n",
    "        else:\n",
    "                return squaredNorm\n",
    "\n",
    "def test_routing(inputs, num_capsules, num_routing_nodes, num_iterations ):\n",
    "        b = nn.Parameter(torch.zeros(num_routing_nodes, num_capsules, 1, 1))\n",
    "        weights = nn.Parameter(torch.rand(num_routing_nodes, num_capsules, 16, 8))\n",
    "\n",
    "        weights = weights[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "        b_ij = b[None, :, :, :].tile(inputs.size(0), 1, 1, 1, 1)\n",
    "        inputs = inputs.tile(1, 1, num_capsules, 1, 1)\n",
    "  \n",
    "        # u_hat = [batch, num_routing_nodes, # digit_caps, digit_caps_dims, 1]\n",
    "        u_hat = torch.matmul(weights, inputs)\n",
    "    \n",
    "        for i in range(num_iterations):\n",
    "                c_ij = F.softmax(b_ij, dim=2)\n",
    "                v_j = squash((c_ij*u_hat).sum(dim=1, keepdim=True))\n",
    "                \n",
    "                if i < num_iterations - 1 :\n",
    "                        # v_j = [batch, 1 -> num_routing_nodes, num_digit_caps, digit_caps_dims, 1 )]\n",
    "                        b_ij +=  (b_ij + torch.transpose(u_hat, 3, 4) @ v_j.tile(1, num_routing_nodes, 1, 1, 1))\n",
    "\n",
    "        return v_j \n",
    "\n",
    "def main():\n",
    "        a = torch.Tensor(train_images[0]) # RGB images\n",
    "        b = torch.Tensor(train_images[1])\n",
    "        c = torch.stack((a,b), dim=0) # create a 2 batchs with 1 image in each batch\n",
    "\n",
    "        num_categories = 10\n",
    "        conv_output = demo_conv(c.permute(0, 3, 1, 2)) # Notice: input is in form [batch, channel, height, width]\n",
    "        # ouputs -> list: len(list) = num_caps, outputs elements -> tensor: size (1,8,10,10)\n",
    "        caps_output = [\n",
    "                (cap(conv_output))[:, None, :, :, :].permute(0, 1, 3, 4, 2) for cap in primary_caps]\n",
    "        output = torch.cat(caps_output, dim=1)\n",
    "        output = output.view(output.size(0), -1, output.size(4))\n",
    "        output = squash(output)[:, :, None, :, None]\n",
    "        # print(conv_output.size(), output.size())\n",
    "\n",
    "        #routing\n",
    "        v_j = test_routing(output, 10, 1152, 3).squeeze(1)\n",
    "        v_j_norm = squash(v_j, axis=-2, squash=False)\n",
    "        v_softmax = F.softmax(v_j_norm, dim=1)\n",
    "        # if y is None:\n",
    "        v_active, idx = torch.max(v_softmax, dim=1)\n",
    "        y = torch.eye(10).index_select(dim=0, index = idx.squeeze())\n",
    "        \n",
    "        masked_v = (y[:, :, None, None] * v_j).view(batch_size, -1)\n",
    "\n",
    "        # print(masked_v.size())\n",
    "        print(idx.size())\n",
    "        assert list(v_j.size()) == [c.size()[0], 10, 16, 1]\n",
    "   \n",
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Miscellaneous</strong>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super() examples\n",
    "use super to access the characteristics of other classes\n",
    "Ex:\n",
    "super().__init__(mammalName) is equivalent to Class1.__init_(self, mammalName)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animal(object):\n",
    "  def __init__(self, Animal):\n",
    "    print(Animal, 'is an animal.')\n",
    "\n",
    "class Mammal(Animal):\n",
    "  def __init__(self, mammalName):\n",
    "    print(mammalName, 'is a warm-blooded animal.')\n",
    "    super().__init__(mammalName)\n",
    "\n",
    "class NonMarineMammal(Mammal):\n",
    "  def __init__(self, NonMarineMammal):\n",
    "    print(NonMarineMammal, \"can't swim.\")\n",
    "    super().__init__(NonMarineMammal)\n",
    "   \n",
    "class NonWingedMammal(Mammal):\n",
    "  def __init__(self, NonWingedMammal):\n",
    "    print(NonWingedMammal, \"can't fly.\")\n",
    "    super().__init__(NonWingedMammal)\n",
    "\n",
    "class Dog(NonMarineMammal, NonWingedMammal):\n",
    "  def __init__(self):\n",
    "    print('Dog has 4 legs.')\n",
    "    super().__init__('Dog')\n",
    "    \n",
    "d = Dog()\n",
    "print(d)\n",
    "# bat = NonMarineMammal('Bat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parent:\n",
    "  def __init__(self, *txt):\n",
    "    # *args is not a must have input\n",
    "    self.message = txt\n",
    "\n",
    "  def printmessage(self):\n",
    "    print(self.message)\n",
    "\n",
    "  @staticmethod\n",
    "  def printmessage2(text):\n",
    "    print(text)\n",
    "\n",
    "\n",
    "class Child(Parent):\n",
    "  def __init__(self, txt: str, num):\n",
    "    self.num = num\n",
    "    super(Child, self).__init__()\n",
    "    self.printmessage()\n",
    "  \n",
    "  def call_static(self, msg):\n",
    "    self.printmessage2(msg)\n",
    "\n",
    "x = Child(\"Hello, and welcome!\", 2)\n",
    "\n",
    "x.printmessage2(\"hi static\") # another way to call Parent's method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rectangle(object):\n",
    "    def __init__(self, length, width):\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "\n",
    "    def area(self):\n",
    "        return self.length * self.length\n",
    "\n",
    "    def perimeter(self):\n",
    "        return 2 * self.length + 2 * self.width\n",
    "\n",
    "# Here we declare that the Square class inherits from the Rectangle class\n",
    "class Square(Rectangle):\n",
    "    def __init__(self, length_sqr):\n",
    "        super().__init__(length_sqr, length_sqr)   # length_sqr = length and width of class Rectangle\n",
    "Square(5).area()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIL module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = np.matrix('250 60 143; 90 100 40; 120 150 200')\n",
    "im = Image.fromarray(a) # create a n image object as arrays\n",
    "plt.imshow(im)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Employee:\n",
    "    def __init__(self, name, base_pay):\n",
    "        self.name = name\n",
    "        self.base_pay = base_pay\n",
    "\n",
    "    def get_pay(self):\n",
    "        return self.base_pay\n",
    "\n",
    "\n",
    "class SalesEmployee(Employee):\n",
    "    def __init__(self, name, base_pay, sales_incentive):\n",
    "        self.name = name\n",
    "        self.base_pay = base_pay\n",
    "        self.sales_incentive = sales_incentive\n",
    "\n",
    "    def get_pay(self):\n",
    "        return self.base_pay + self.sales_incentive\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    john = SalesEmployee('John', 5000, 1500)\n",
    "    print(john.get_pay())\n",
    "\n",
    "    jane = Employee('Jane', 5000)\n",
    "    print(jane.get_pay())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_caps = 3\n",
    "input_dims = 3\n",
    "output_caps = 2\n",
    "output_dims = 2\n",
    "a = torch.Tensor(input_caps, input_dims , output_caps * output_dims)\n",
    "b = torch.Tensor([ [[10], [30]], [[50], [70]] ])\n",
    "print(b.shape)\n",
    "plt.imshow(b, cmap =plt.cm.binary)\n",
    "print(a, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! 4D requires diff inputs compared to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2,2,3)\n",
    "b = torch.randn(2,2,2,3)\n",
    "print(a)\n",
    "print('---------------')\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to perform 2D convolution operation on an image\n",
    "# Import the required libraries\n",
    "\n",
    "'''input of size [N,C,H, W]\n",
    "N==>batch size,\n",
    "C==> number of channels,\n",
    "H==> height of input planes in pixels,\n",
    "W==> width in pixels.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Read input image\n",
    "img = Image.open('dogncat.jpg')\n",
    "\n",
    "# convert the input image to torch tensor\n",
    "img = T.ToTensor()(img)\n",
    "print(\"Input image size:\", img.size()) # size = [3, 466, 700]\n",
    "\n",
    "# unsqueeze the image to make it 4D tensor\n",
    "img = img.unsqueeze(0) # image size = [1, 3, 466, 700]\n",
    "# define convolution layer\n",
    "# conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "conv = torch.nn.Conv2d(3, 3, 2)\n",
    "\n",
    "# apply convolution operation on image\n",
    "img = conv(img)\n",
    "print(img.size())\n",
    "plt.imshow(img[0,:,:,:].detach().numpy())\n",
    "\n",
    "\n",
    "# squeeze image to make it 3D\n",
    "img = img.squeeze(0) #now size is again [3, 466, 700]\n",
    "# convert image to PIL image\n",
    "img = T.ToPILImage()(img)\n",
    "\n",
    "# display the image after convolution\n",
    "img.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary layer unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_Tensor(size):\n",
    "    return torch.rand(size, size, size)\n",
    "a = [c for c in random_Tensor(3) ]\n",
    "print(a)\n",
    "a = torch.cat(a)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
